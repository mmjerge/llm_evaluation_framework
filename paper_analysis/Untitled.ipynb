{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f37f9e6f-ed0e-4bd2-93d1-d01e20220cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total citations to fetch: 6318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching citations: 100%|██████████████████| 6318/6318 [00:25<00:00, 252.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total citations with arXiv IDs: 4895\n",
      "\n",
      "First 5 papers to be downloaded:\n",
      "1. Enhancing Table Recognition with Vision LLMs: A Benchmark and Neighbor-Guided Toolchain Reasoner\n",
      "2. Efficiently Serving LLM Reasoning Programs with Certaindex\n",
      "3. Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs\n",
      "4. Pushing the Envelope of Low-Bit LLM via Dynamic Error Compensation\n",
      "5. Toward Adaptive Reasoning in Large Language Models with Thought Rollback\n",
      "\n",
      "Starting downloads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing arXiv papers:   0%|                         | 0/4895 [00:00<?, ?it/s]/var/folders/0q/6k0gk6g5281gcsvh418ty0140000gn/T/ipykernel_2060/2173061590.py:10: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  paper = next(search.results())\n",
      "Processing arXiv papers:  10%|█▌             | 502/4895 [05:29<41:40,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error downloading http://arxiv.org/pdf/2410.07062v2: 404 Client Error: Not Found for url: http://arxiv.org/pdf/2410.07062v2\n",
      "\n",
      "Failed to download: TinyEmo: Scaling down Emotional Reasoning via Metric Projection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing arXiv papers:  37%|█████         | 1789/4895 [18:54<27:51,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error downloading http://arxiv.org/pdf/2405.02659v2: 404 Client Error: Not Found for url: http://arxiv.org/pdf/2405.02659v2\n",
      "\n",
      "Failed to download: R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing arXiv papers:  57%|███████▉      | 2777/4895 [31:35<34:25,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error downloading http://arxiv.org/pdf/2312.08926v2: 404 Client Error: Not Found for url: http://arxiv.org/pdf/2312.08926v2\n",
      "\n",
      "Failed to download: Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing arXiv papers:  63%|████████▊     | 3098/4895 [38:37<15:57,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error downloading http://arxiv.org/pdf/2310.18331v2: 404 Client Error: NOT FOUND for url: http://arxiv.org/pdf/2310.18331v2\n",
      "\n",
      "Failed to download: AllTogether: Investigating the Efficacy of Spliced Prompt for Web Navigation using Large Language Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing arXiv papers:  64%|█████████     | 3148/4895 [39:23<22:01,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error downloading http://arxiv.org/pdf/2310.10698v2: 404 Client Error: Not Found for url: http://arxiv.org/pdf/2310.10698v2\n",
      "\n",
      "Failed to download: Bridging Code Semantic and LLMs: Semantic Chain-of-Thought Prompting for Code Generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing arXiv papers:  70%|█████████▊    | 3438/4895 [44:14<24:42,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error downloading http://arxiv.org/pdf/2309.12481v2: 404 Client Error: NOT FOUND for url: http://arxiv.org/pdf/2309.12481v2\n",
      "\n",
      "Failed to download: HANS, are you clever? Clever Hans Effect Analysis of Neural Systems\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing arXiv papers:  71%|████████▌   | 3470/4895 [45:19<2:06:37,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error downloading http://arxiv.org/pdf/2309.09749v3: 404 Client Error: Not Found for url: http://arxiv.org/pdf/2309.09749v3\n",
      "\n",
      "Failed to download: Facilitating NSFW Text Detection in Open-Domain Dialogue Systems via Knowledge Distillation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing arXiv papers:  80%|███████████▏  | 3922/4895 [54:26<14:02,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error downloading http://arxiv.org/pdf/2306.08997v2: 404 Client Error: NOT FOUND for url: http://arxiv.org/pdf/2306.08997v2\n",
      "\n",
      "Failed to download: Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing arXiv papers:  95%|███████████▍| 4667/4895 [1:10:46<05:45,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error downloading http://arxiv.org/pdf/2306.07622v2: 404 Client Error: Not Found for url: http://arxiv.org/pdf/2306.07622v2\n",
      "\n",
      "Failed to download: Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing arXiv papers: 100%|████████████| 4895/4895 [1:14:17<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import arxiv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def get_arxiv_pdf(arxiv_id):\n",
    "    try:\n",
    "        search = arxiv.Search(id_list=[arxiv_id])\n",
    "        paper = next(search.results())\n",
    "        return paper.pdf_url\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError getting arXiv PDF URL for {arxiv_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def retrieve_url(url, filepath):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(filepath, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError downloading {url}: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_all_citations(paper_id):\n",
    "    citations = []\n",
    "    offset = 0\n",
    "    limit = 1000\n",
    "    total_citations = 0\n",
    "    \n",
    "    url = f\"http://api.semanticscholar.org/graph/v1/paper/{paper_id}\"\n",
    "    params = {\"fields\": \"citationCount\"}\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            total_citations = response.json()['citationCount']\n",
    "            print(f\"Total citations to fetch: {total_citations}\")\n",
    "        else:\n",
    "            print(f\"Failed to get citation count: {response.status_code}\")\n",
    "            return citations\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting citation count: {e}\")\n",
    "        return citations\n",
    "    \n",
    "    with tqdm(total=total_citations, desc=\"Fetching citations\") as pbar:\n",
    "        while True:\n",
    "            url = f\"http://api.semanticscholar.org/graph/v1/paper/{paper_id}/citations\"\n",
    "            params = {\n",
    "                \"fields\": \"citingPaper.title,citingPaper.externalIds,citingPaper.url\",  # Updated fields\n",
    "                \"limit\": limit,\n",
    "                \"offset\": offset\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(url, params=params)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    batch = data.get('data', [])\n",
    "                    if not batch:\n",
    "                        break\n",
    "                    \n",
    "                    citations.extend(batch)\n",
    "                    pbar.update(len(batch))\n",
    "                    \n",
    "                    offset += limit\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    print(f\"Error fetching citations: {response.status_code}\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Error in citation request: {e}\")\n",
    "                break\n",
    "                \n",
    "    return citations\n",
    "\n",
    "paperId = \"1b6e810ce0afd0dd093f789d2b2742d047e316d5\"\n",
    "citations = get_all_citations(paperId)\n",
    "\n",
    "# Filter citations with arXiv IDs (case-insensitive check)\n",
    "arxiv_citations = []\n",
    "for citation in citations:\n",
    "    if 'citingPaper' in citation:\n",
    "        paper = citation['citingPaper']\n",
    "        external_ids = paper.get('externalIds', {})\n",
    "        if any(key.lower() == 'arxiv' for key in external_ids.keys()):\n",
    "            arxiv_citations.append(paper)\n",
    "\n",
    "print(f\"\\nTotal citations with arXiv IDs: {len(arxiv_citations)}\")\n",
    "print(\"\\nFirst 5 papers to be downloaded:\")\n",
    "for i, paper in enumerate(arxiv_citations[:5]):\n",
    "    print(f\"{i+1}. {paper.get('title')}\")\n",
    "\n",
    "print(\"\\nStarting downloads...\")\n",
    "# Process citations with progress bar\n",
    "if not os.path.exists(\"pdfs\"):\n",
    "    os.makedirs(\"pdfs\")\n",
    "\n",
    "for paper in tqdm(arxiv_citations, desc=\"Processing arXiv papers\"):\n",
    "    title = paper['title']\n",
    "    arxiv_key = next(key for key in paper['externalIds'].keys() if key.lower() == 'arxiv')\n",
    "    arxiv_id = paper['externalIds'][arxiv_key]\n",
    "    \n",
    "    safe_title = \"\".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).strip()\n",
    "    pdf_path = f\"pdfs/{safe_title}.pdf\"\n",
    "    \n",
    "    if os.path.exists(pdf_path):\n",
    "        continue\n",
    "        \n",
    "    pdf_url = get_arxiv_pdf(arxiv_id)\n",
    "    if pdf_url:\n",
    "        if retrieve_url(pdf_url, pdf_path):\n",
    "            continue  \n",
    "        else:\n",
    "            print(f\"\\nFailed to download: {title}\")\n",
    "    \n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d26637aa-a5e2-47b6-b2b1-adbc75791594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|████████████████████| 4886/4886 [1:00:14<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved data to pdf_contents.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"PyPDF2\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "warnings.filterwarnings('ignore', message='.*FloatObject.*invalid.*')\n",
    "warnings.filterwarnings('ignore', message='.*unknown widths.*')\n",
    "warnings.filterwarnings('ignore', category=Warning, module='PyPDF2')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text of invalid Unicode characters\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Replace surrogate characters with a replacement character\n",
    "    return text.encode('utf-16', 'surrogatepass').decode('utf-16', 'replace')\n",
    "\n",
    "directory = \"pdfs\"\n",
    "pdf_files = list(Path(directory).glob('*.pdf'))\n",
    "pdf_text_dict = {}\n",
    "\n",
    "# Create progress bar\n",
    "for file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "    try:\n",
    "        reader = PdfReader(file)\n",
    "        meta = reader.metadata\n",
    "        \n",
    "        # Initialize dictionary entry with metadata, cleaning each field\n",
    "        pdf_text_dict[file.stem] = {\n",
    "            \"filename\": file.name,\n",
    "            \"author\": clean_text(meta.author) if meta.author else \"\",\n",
    "            \"creator\": clean_text(meta.creator) if meta.creator else \"\",\n",
    "            \"subject\": clean_text(meta.subject) if meta.subject else \"\",\n",
    "            \"title\": clean_text(meta.title) if meta.title else \"\",\n",
    "            \"text\": \"\"\n",
    "        }\n",
    "        \n",
    "        # Extract text from all pages\n",
    "        text_content = []\n",
    "        for page in reader.pages:\n",
    "            try:\n",
    "                extracted_text = page.extract_text()\n",
    "                if extracted_text:\n",
    "                    text_content.append(clean_text(extracted_text))\n",
    "            except Exception as page_error:\n",
    "                print(f\"\\nError extracting text from page in {file}: {str(page_error)}\")\n",
    "                continue\n",
    "            \n",
    "        # Join all pages' text with newlines\n",
    "        pdf_text_dict[file.stem][\"text\"] = \"\\n\".join(text_content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing {file}: {str(e)}\")\n",
    "        pdf_text_dict[file.stem] = {\n",
    "            \"filename\": file.name,\n",
    "            \"error\": str(e),\n",
    "            \"text\": \"\"\n",
    "        }\n",
    "\n",
    "# Save to JSON file\n",
    "output_path = \"pdf_contents.json\"\n",
    "try:\n",
    "    # First try with standard encoding\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(pdf_text_dict, f, ensure_ascii=False, indent=4)\n",
    "    except UnicodeEncodeError:\n",
    "        # If that fails, fall back to ensuring ASCII with escaping\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(pdf_text_dict, f, ensure_ascii=True, indent=4)\n",
    "    print(f\"\\nSuccessfully saved data to {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving JSON file: {str(e)}\")\n",
    "    # Last resort: try to save with problematic characters removed\n",
    "    try:\n",
    "        cleaned_dict = {k: {\n",
    "            key: str(value).encode('ascii', 'ignore').decode('ascii') \n",
    "            for key, value in v.items()\n",
    "        } for k, v in pdf_text_dict.items()}\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(cleaned_dict, f, ensure_ascii=True, indent=4)\n",
    "        print(f\"Saved file with ASCII-only characters to {output_path}\")\n",
    "    except Exception as final_e:\n",
    "        print(f\"Final attempt to save failed: {str(final_e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa11ee5a-3ffd-48e7-9ca3-aeb545689407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   3%|▎         | 48/1886 [04:20<2:44:35,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   4%|▍         | 72/1886 [07:24<3:53:16,  7.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - JSON parsing error: Invalid JSON format: line 1 column 1 (char 0)\n",
      "Raw response: {\n",
      "    \"benchmarks\": [\n",
      "        \"AQA-Bench\",\n",
      "        \"GuessNum\",\n",
      "        \"DFS\",\n",
      "        \"BFS\",\n",
      "        \"Coin\",\n",
      "        \"CaveDFS\",\n",
      "        \"CaveBFS\"\n",
      "    ],\n",
      "    \"models\": [\n",
      "        \"GPT-4\",\n",
      "        \"Gemini\",\n",
      "        \"GPT-3.5-Turbo\",\n",
      "        \"GPT-4-Turbo\",\n",
      "        \"Gemini-Pro\",\n",
      "        \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   9%|▊         | 161/1886 [20:59<2:14:33,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - JSON parsing error: Invalid JSON format: line 1 column 1 (char 0)\n",
      "Raw response: {\n",
      "    \"benchmarks\": [\n",
      "        \"ACE04\",\n",
      "        \"ACE05\",\n",
      "        \"CoNLL03\",\n",
      "        \"OntoNotes 5.0\",\n",
      "        \"GENIA\",\n",
      "        \"NYT\",\n",
      "        \"ADE\",\n",
      "        \"CoNLL04\",\n",
      "        \"SciERC\",\n",
      "        \"TACRED\",\n",
      "        \"Re-TACRED\",\n",
      "        \"TACREV\",\n",
      "        \"SemEval\"\n",
      "    ],\n",
      "    \"models\": [\n",
      "        \"GPT-4\",\n",
      "        \"ChatGPT\",\n",
      "        \"LLaMA\",\n",
      "        \"Flan-T5\",\n",
      "        \"CodeX\",\n",
      "        \"GPT-NER\",\n",
      "        \"Cp-NER\",\n",
      "        \"LLMaAA\",\n",
      "        \"PromptNER\",\n",
      "        \"UniNER\",\n",
      "        \"NAG-NER\",\n",
      "        \"GNER\",\n",
      "        \"NuNER\",\n",
      "        \"MetaNER\",\n",
      "        \"LinkNER\",\n",
      "        \"SLCoLM\",\n",
      "        \"ProgGen\",\n",
      "        \"C-ICL\",\n",
      "        \"VerifiNER\",\n",
      "        \"ConsistNER\",\n",
      "        \"GLiNER\",\n",
      "        \"LTNER\",\n",
      "        \"ToNER\",\n",
      "        \"RT\",\n",
      "        \"VANER\",\n",
      "        \"RiVEG\",\n",
      "        \"LLM-DA\",\n",
      "        \"REBEL\",\n",
      "        \"QA4RE\",\n",
      "        \"GPT-RE\",\n",
      "        \"STAR\",\n",
      "        \"AugURE\",\n",
      "        \"REPAL\",\n",
      "        \"RAG4RE\",\n",
      "        \"BART-Gen\",\n",
      "        \"Text2Event\",\n",
      "        \"ClarET\",\n",
      "        \"X-GEAR\",\n",
      "        \"PAIE\",\n",
      "        \"GTEE-DYNPREF\",\n",
      "        \"Code4Struct\",\n",
      "        \"PGAD\",\n",
      "        \"QGA-EE\",\n",
      "        \"SPEAE\",\n",
      "        \"AMPERE\",\n",
      "        \"KeyEE\",\n",
      "        \"ULTRA\",\n",
      "        \"DEEPSTRUCT\",\n",
      "        \"GenIE\",\n",
      "        \"UIE\",\n",
      "        \"LasUIE\",\n",
      "        \"ChatIE\",\n",
      "        \"InstructUIE\",\n",
      "        \"GIELLM\",\n",
      "        \"Set\",\n",
      "        \"CollabKG\",\n",
      "        \"TechGPT-2\",\n",
      "        \"YAYI-UIE\",\n",
      "        \"ChatUIE\",\n",
      "        \"IEPile\",\n",
      "        \"Guo et al.\",\n",
      "        \"CODEIE\",\n",
      "        \"CodeKGC\",\n",
      "        \"GoLLIE\",\n",
      "        \"Code4UIE\",\n",
      "        \"KnowCoder\",\n",
      "        \"TANL\",\n",
      "        \"TEMPGEN\",\n",
      "        \"Cui et al.\",\n",
      "        \"Yan et al.\",\n",
      "        \"Xia et al.\",\n",
      "        \"EnTDA\",\n",
      "        \"YAYI-UIE\",\n",
      "        \"KnowCoder\",\n",
      "        \"USM\",\n",
      "        \"RexUIE\",\n",
      "        \"Mirror\",\n",
      "        \"Li et al.\",\n",
      "        \"Xu et al.\",\n",
      "        \"Otto et al.\",\n",
      "        \"Shi et al.\",\n",
      "        \"Li et al.\",\n",
      "        \"Ni et al.\",\n",
      "        \"Zaratiana et al.\",\n",
      "        \"MetaIE\",\n",
      "        \"Atuhurra et al.\",\n",
      "        \"CHisIEC\",\n",
      "        \"Veyseh et al.\",\n",
      "        \"DAFS\",\n",
      "        \"Cai et al.\",\n",
      "        \"Ni et al.\",\n",
      "        \"BART-Gen\",\n",
      "        \"DEGREE\",\n",
      "        \"DemoSG\",\n",
      "        \"Kwak et al.\",\n",
      "        \"EventRL\",\n",
      "        \"Huang et al.\",\n",
      "        \"Ding et al.\",\n",
      "        \"Sun et al.\",\n",
      "        \"Zhou et al.\",\n",
      "        \"Hu et al.\",\n",
      "        \"Shao et al.\",\n",
      "        \"Evans et al.\",\n",
      "        \"Gonzalez et al.\",\n",
      "        \"Nune et al.\",\n",
      "        \"Oliveira et al.\",\n",
      "        \"Kwak et al.\",\n",
      "        \"Gutierrez et al.\",\n",
      "        \"GPT-3 +R\",\n",
      "        \"Labrak et al.\",\n",
      "        \"Xie et al.\",\n",
      "        \"Gao et al.\",\n",
      "        \"InstructIE\",\n",
      "        \"Han et al.\",\n",
      "        \"Katz et al.\",\n",
      "        \"XNLP\",\n",
      "        \"Foppiano et al.\",\n",
      "        \"PolyIE\",\n",
      "        \"Li et al.\",\n",
      "        \"Qi et al.\",\n",
      "        \"SynthIE\",\n",
      "        \"Chen et al.\",\n",
      "        \"Amalvy et al.\",\n",
      "        \"PGIM\",\n",
      "        \"VerifiNER\",\n",
      "        \"EnTDA\",\n",
      "        \"UniversalNER\",\n",
      "        \"GNER\",\n",
      "        \"Chen et al.\",\n",
      "        \"Wadhwa et al.\",\n",
      "        \"Yuan et al.\",\n",
      "        \"Bian et al.\",\n",
      "        \"Xie et al.\",\n",
      "        \"ProgGen\",\n",
      "        \"QA4RE\",\n",
      "        \"SUMASK\",\n",
      "        \"GPT-RE\",\n",
      "        \"Xu et al.\",\n",
      "        \"REBEL\",\n",
      "        \"Li et al.\",\n",
      "        \"Code4Struct\",\n",
      "        \"Code4UIE\",\n",
      "        \"TANL\",\n",
      "        \"Text2Event\",\n",
      "        \"BART-Gen\",\n",
      "        \"GTEE-DYNPREF\",\n",
      "        \"DEEPSTRUCT\",\n",
      "        \"PAIE\",\n",
      "        \"PGAD\",\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   9%|▉         | 176/1886 [34:42<5:37:17, 11.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 162\u001b[0m\n\u001b[1;32m    159\u001b[0m     save_progress(analysis_results, output_base, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 162\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[5], line 139\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     result \u001b[38;5;241m=\u001b[39m analyze_paper_with_gpt(paper_text)\n\u001b[1;32m    140\u001b[0m     analysis_results[paper_id] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m: paper_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalysis\u001b[39m\u001b[38;5;124m\"\u001b[39m: result\n\u001b[1;32m    143\u001b[0m     }\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Save progress every save_interval papers\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 66\u001b[0m, in \u001b[0;36manalyze_paper_with_gpt\u001b[0;34m(paper_text)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: Token count exceeds safe limit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenchmarks\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken limit exceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m     }\n\u001b[0;32m---> 66\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     67\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     68\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     69\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant that analyzes academic papers and returns responses in JSON format only.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     70\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}\n\u001b[1;32m     71\u001b[0m     ],\n\u001b[1;32m     72\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     73\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     74\u001b[0m     response_format\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson_object\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     77\u001b[0m result \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Add error checking for response content\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py:581\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    580\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    583\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    584\u001b[0m             {\n\u001b[1;32m    585\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    586\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    587\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    588\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    589\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    590\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    591\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    592\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    593\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    595\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    596\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    597\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    598\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    599\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    600\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    601\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    602\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    603\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    604\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    605\u001b[0m             },\n\u001b[1;32m    606\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    607\u001b[0m         ),\n\u001b[1;32m    608\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    609\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    610\u001b[0m         ),\n\u001b[1;32m    611\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    612\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    613\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    614\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1233\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1221\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1228\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1229\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1230\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1231\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1232\u001b[0m     )\n\u001b[0;32m-> 1233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:922\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    915\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    920\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    921\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 922\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    923\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    924\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    925\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    926\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    927\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    928\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:951\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    948\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_auth\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 951\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m    952\u001b[0m         request,\n\u001b[1;32m    953\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[1;32m    954\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    955\u001b[0m     )\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    957\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m    915\u001b[0m     request,\n\u001b[1;32m    916\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m    917\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    918\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    919\u001b[0m )\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m    943\u001b[0m         request,\n\u001b[1;32m    944\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    945\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(\n\u001b[1;32m    197\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv(max_bytes)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/ssl.py:1296\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(buflen)\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/ssl.py:1169\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import openai\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "from datetime import datetime\n",
    "\n",
    "client = OpenAI(api_key='') # Add your OpenAI API token here\n",
    "\n",
    "def count_tokens(text):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def truncate_text(text, max_tokens=14000):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    tokens = encoding.encode(text)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "        text = encoding.decode(tokens)\n",
    "    return text\n",
    "\n",
    "def create_prompt(paper_text):\n",
    "    return f\"\"\"Analyze this academic paper and extract all benchmarks/datasets and models mentioned in experiments, evaluations, or comparisons.\n",
    "\n",
    "Rules for extraction:\n",
    "1. For benchmarks/datasets:\n",
    "   - Include standard evaluation datasets (e.g., MNIST, ImageNet, SQuAD)\n",
    "   - Include custom datasets if they're used for evaluation\n",
    "   - Do NOT include training datasets unless they're also used for evaluation\n",
    "\n",
    "2. For models:\n",
    "   - Include baseline models used for comparison\n",
    "   - Include proposed/novel models being evaluated\n",
    "   - Include model variants tested in ablation studies\n",
    "   - Do NOT include referenced models that weren't actually tested\n",
    "\n",
    "Format your response as a JSON object with this exact structure:\n",
    "{{\n",
    "    \"benchmarks\": [\"benchmark1\", \"benchmark2\"],\n",
    "    \"models\": [\"model1\", \"model2\"]\n",
    "}}\n",
    "\n",
    "Paper text:\n",
    "{paper_text}\"\"\"\n",
    "\n",
    "def analyze_paper_with_gpt(paper_text):\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            truncated_text = truncate_text(paper_text)\n",
    "            was_truncated = len(truncated_text) < len(paper_text)\n",
    "            \n",
    "            prompt = create_prompt(truncated_text)\n",
    "            token_count = count_tokens(prompt)\n",
    "            \n",
    "            if token_count > 15000:\n",
    "                print(\"Warning: Token count exceeds safe limit\")\n",
    "                return {\n",
    "                    \"benchmarks\": [],\n",
    "                    \"models\": [],\n",
    "                    \"error\": \"Token limit exceeded\"\n",
    "                }\n",
    "                \n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that analyzes academic papers and returns responses in JSON format only.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=1000,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "\n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            # Add error checking for response content\n",
    "            if not result.strip().startswith('{') or not result.strip().endswith('}'):\n",
    "                raise json.JSONDecodeError(\"Invalid JSON format\", result, 0)\n",
    "                \n",
    "            parsed_result = json.loads(result)\n",
    "            \n",
    "            if was_truncated:\n",
    "                parsed_result[\"note\"] = \"Analysis based on truncated paper text\"\n",
    "                \n",
    "            return parsed_result\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"\\nAttempt {attempt + 1}/{max_retries} - JSON parsing error: {str(e)}\")\n",
    "            print(f\"Raw response: {result}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return {\n",
    "                    \"benchmarks\": [],\n",
    "                    \"models\": [],\n",
    "                    \"error\": f\"Failed to parse GPT response as JSON after {max_retries} attempts: {str(e)}\"\n",
    "                }\n",
    "            time.sleep(2)  # Wait before retry\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nAttempt {attempt + 1}/{max_retries} - Error: {str(e)}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return {\n",
    "                    \"benchmarks\": [],\n",
    "                    \"models\": [],\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "            time.sleep(2)  # Wait before retry\n",
    "\n",
    "def save_progress(analysis_results, base_filename, iteration):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{base_filename}_iter{iteration}_{timestamp}.json\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(analysis_results, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"\\nProgress saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    input_file = \"pdf_contents.json\"\n",
    "    output_base = \"paper_analysis\"\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        pdf_contents = json.load(f)\n",
    "\n",
    "    analysis_results = {}\n",
    "    \n",
    "    # Convert items to list for tqdm\n",
    "    items = list(pdf_contents.items())\n",
    "    save_interval = 500  # Save every 500 papers\n",
    "    \n",
    "    for i, (paper_id, paper_data) in enumerate(tqdm(items[3000:], desc=\"Analyzing papers\")):\n",
    "        paper_text = paper_data.get('text', '')\n",
    "        \n",
    "        if not paper_text:\n",
    "            print(f\"\\nSkipping {paper_id} - no text content\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            result = analyze_paper_with_gpt(paper_text)\n",
    "            analysis_results[paper_id] = {\n",
    "                \"filename\": paper_data['filename'],\n",
    "                \"analysis\": result\n",
    "            }\n",
    "            \n",
    "            # Save progress every save_interval papers\n",
    "            if (i + 1) % save_interval == 0:\n",
    "                save_progress(analysis_results, output_base, f\"checkpoint_{i+1}\")\n",
    "                \n",
    "            time.sleep(2)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError analyzing {paper_id}: {str(e)}\")\n",
    "            analysis_results[paper_id] = {\n",
    "                \"filename\": paper_data['filename'],\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    # Save final results\n",
    "    save_progress(analysis_results, output_base, \"final\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a33ce81-6ecf-4cdd-87a6-d9bdf1e3a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "json_files = glob.glob('paper_analysis_itercheckpoint_*.json')\n",
    "combined_data = {}\n",
    "\n",
    "for file in json_files:\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "        combined_data.update(data)\n",
    "\n",
    "with open('combined_analysis.json', 'w') as f:\n",
    "    json.dump(combined_data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61924570-ed8e-4229-ad3c-21a99f876304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Properties', 'Models Used', 'Models Used Recently', 'Models Used Paste', 'Benchmarks Used', 'Benchmarks by Use'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "excel_file = \"ensemble_works-new.xlsx\"\n",
    "sheet_names = pd.read_excel(excel_file, sheet_name=None).keys()\n",
    "print(sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8dd6b62-f120-4f61-8f7d-09154493e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.read_excel(excel_file, sheet_name='Models Used')\n",
    "model_df = model_df.iloc[:109, :-2]\n",
    "# model_df.head()\n",
    "# model_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c55b503e-13d4-430b-ae47-e8adf6b61d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_df = pd.read_excel(excel_file, sheet_name='Benchmarks Used')\n",
    "benchmark_df = benchmarks_df.iloc[:109, :-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f40145e2-010a-4384-a08f-758e48b66243",
   "metadata": {},
   "outputs": [],
   "source": [
    "annoted_papers = {}\n",
    "\n",
    "for (index_one, model_row), (index_two, benchmark_row) in zip(model_df.iterrows(), benchmark_df.iterrows()):\n",
    "    paper_title = model_row['Paper Titles']\n",
    "    annoted_papers[paper_title] = {\n",
    "        \"filename\": f\"{paper_title}.pdf\",\n",
    "        \"analysis\": {\n",
    "            \"models\": [\n",
    "                model for model, value in model_row[2:].items()\n",
    "                if value == 1.0\n",
    "            ],\n",
    "            \"benchmarks\": [\n",
    "                benchmark for benchmark, value in benchmark_row[2:].items()\n",
    "                if value == 1.0\n",
    "            ],\n",
    "            \"note\": \"Analysis based on human review.\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "with open('annotated_papers.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(annoted_papers, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f79232c0-8d4f-494a-9f27-de93afcd4cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Match found:\n",
      "File 1: ReAct: Synergizing Reasoning and Acting in Language Models\n",
      "File 2: ReAct Synergizing Reasoning and Acting in Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n",
      "File 2: Tree of Thoughts Deliberate Problem Solving with Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Boosted Prompt Ensembles for Large Language Models\n",
      "File 2: Boosted Prompt Ensembles for Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Ask Me Anything: A simple strategy for prompting language models\n",
      "File 2: Ask Me Anything A simple strategy for prompting language models\n",
      "\n",
      "Match found:\n",
      "File 1: Graph of Thoughts: Solving Elaborate Problems with Large Language Models\n",
      "File 2: Graph of Thoughts Solving Elaborate Problems with Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: More Agents Is All You Need\n",
      "File 2: More Agents Is All You Need\n",
      "\n",
      "Match found:\n",
      "File 1: ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs\n",
      "File 2: ReConcile Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs\n",
      "\n",
      "Match found:\n",
      "File 1: Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models\n",
      "File 2: Boosting of Thoughts Trial-and-Error Problem Solving with Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems\n",
      "File 2: Fill in the Blank Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems\n",
      "\n",
      "Match found:\n",
      "File 1: InferFix: End-to-End Program Repair with LLMs\n",
      "File 2: InferFix End-to-End Program Repair with LLMs\n",
      "\n",
      "Match found:\n",
      "File 1: Revisit Input Perturbation Problems for LLMs: A Unified Robustness Evaluation Framework for Noisy Slot Filling Task\n",
      "File 2: Revisit Input Perturbation Problems for LLMs A Unified Robustness Evaluation Framework for Noisy Slot Filling Task\n",
      "\n",
      "Match found:\n",
      "File 1: FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation\n",
      "File 2: FreshLLMs Refreshing Large Language Models with Search Engine Augmentation\n",
      "\n",
      "Match found:\n",
      "File 1: DiversiGATE: A Comprehensive Framework for Reliable Large Language Models\n",
      "File 2: DiversiGATE A Comprehensive Framework for Reliable Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Large Language Models are Zero-Shot Reasoners\n",
      "File 2: Large Language Models are Zero-Shot Reasoners\n",
      "\n",
      "Match found:\n",
      "File 1: ART: Automatic multi-step reasoning and tool-use for large language models\n",
      "File 2: ART Automatic multi-step reasoning and tool-use for large language models\n",
      "\n",
      "Match found:\n",
      "File 1: MathPrompter: Mathematical Reasoning using Large Language Models\n",
      "File 2: MathPrompter Mathematical Reasoning using Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Prompting GPT-3 To Be Reliable\n",
      "File 2: Prompting GPT-3 To Be Reliable\n",
      "\n",
      "Match found:\n",
      "File 1: Beyond Self-Consistency: Ensemble Reasoning Boosts Consistency and Accuracy of LLMs in Cancer Staging\n",
      "File 2: Beyond Self-Consistency Ensemble Reasoning Boosts Consistency and Accuracy of LLMs in Cancer Staging\n",
      "\n",
      "Match found:\n",
      "File 1: Chain-of-Verification Reduces Hallucination in Large Language Models\n",
      "File 2: Chain-of-Verification Reduces Hallucination in Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: LM vs LM: Detecting Factual Errors via Cross Examination\n",
      "File 2: LM vs LM Detecting Factual Errors via Cross Examination\n",
      "\n",
      "Match found:\n",
      "File 1: REFINER: Reasoning Feedback on Intermediate Representations\n",
      "File 2: REFINER Reasoning Feedback on Intermediate Representations\n",
      "\n",
      "Match found:\n",
      "File 1: Reliable Natural Language Understanding with Large Language Models and Answer Set Programming\n",
      "File 2: Reliable Natural Language Understanding with Large Language Models and Answer Set Programming\n",
      "\n",
      "Match found:\n",
      "File 1: Complexity-based Prompting for Multi-step Reasoning\n",
      "File 2: Complexity-Based Prompting for Multi-Step Reasoning\n",
      "\n",
      "Match found:\n",
      "File 1: Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling\n",
      "File 2: Reprompting Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling\n",
      "\n",
      "Match found:\n",
      "File 1: Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions\n",
      "File 2: Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions\n",
      "\n",
      "Match found:\n",
      "File 1: Rationale-Augmented Ensembles in Language Models\n",
      "File 2: Rationale-Augmented Ensembles in Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Automatic Chain of Thought Prompting in Large Language Models\n",
      "File 2: Automatic Chain of Thought Prompting in Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Inference-Time Intervention: Eliciting Truthful Answers from a Language Model\n",
      "File 2: Inference-Time Intervention Eliciting Truthful Answers from a Language Model\n",
      "\n",
      "Match found:\n",
      "File 1: Large Language Models Can Self-Improve\n",
      "File 2: Large Language Models Can Self-Improve\n",
      "\n",
      "Match found:\n",
      "File 1: Self-Refine: Iterative Refinement with Self-Feedback\n",
      "File 2: Self-Refine Iterative Refinement with Self-Feedback\n",
      "\n",
      "Match found:\n",
      "File 1: Reflexion: Language Agents with Verbal Reinforcement Learning\n",
      "File 2: Reflexion language agents with verbal reinforcement learning\n",
      "\n",
      "Match found:\n",
      "File 1: PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine\n",
      "File 2: PREFER Prompt Ensemble Learning via Feedback-Reflect-Refine\n",
      "\n",
      "Match found:\n",
      "File 1: N-CRITICS: Self-Refinement of Large Language Models with Ensemble of Critics\n",
      "File 2: N-Critics Self-Refinement of Large Language Models with Ensemble of Critics\n",
      "\n",
      "Match found:\n",
      "File 1: Improving Factuality and Reasoning in Language Models through Multiagent Debate\n",
      "File 2: Improving Factuality and Reasoning in Language Models through Multiagent Debate\n",
      "\n",
      "Match found:\n",
      "File 1: Self-Consistency Improves Chain of Thought Reasoning in Language Models\n",
      "File 2: Self-Consistency Improves Chain of Thought Reasoning in Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Universal Self-Consistency for Large Language Model Generation\n",
      "File 2: Universal Self-Consistency for Large Language Model Generation\n",
      "\n",
      "Match found:\n",
      "File 1: Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models\n",
      "File 2: Found in the Middle Permutation Self-Consistency Improves Listwise Ranking in Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate\n",
      "File 2: Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate\n",
      "\n",
      "Match found:\n",
      "File 1: Boosting Language Models Reasoning with Chain-of-Knowledge Prompting\n",
      "File 2: Boosting Language Models Reasoning with Chain-of-Knowledge Prompting\n",
      "\n",
      "Match found:\n",
      "File 1: Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models\n",
      "File 2: Chain-of-Note Enhancing Robustness in Retrieval-Augmented Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Active Prompting with Chain-of-Thought for Large Language Models\n",
      "File 2: Active Prompting with Chain-of-Thought for Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves\n",
      "File 2: Rephrase and Respond Let Large Language Models Ask Better Questions for Themselves\n",
      "\n",
      "Match found:\n",
      "File 1: Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models\n",
      "File 2: Take a Step Back Evoking Reasoning via Abstraction in Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models\n",
      "File 2: Synthetic Prompting Generating Chain-of-Thought Demonstrations for Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Towards Expert-Level Medical Question Answering with Large Language Models\n",
      "File 2: Towards Expert-Level Medical Question Answering with Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Embodied LLM Agents Learn to Cooperate in Organized Teams\n",
      "File 2: Embodied LLM Agents Learn to Cooperate in Organized Teams\n",
      "\n",
      "Match found:\n",
      "File 1: The Consensus Game: Language Model Generation via Equilibrium Search\n",
      "File 2: The Consensus Game Language Model Generation via Equilibrium Search\n",
      "\n",
      "Match found:\n",
      "File 1: RELIC: Investigating Large Language Model Responses using Self-Consistency\n",
      "File 2: RELIC Investigating Large Language Model Responses using Self-Consistency\n",
      "\n",
      "Match found:\n",
      "File 1: Better Zero-Shot Reasoning with Self-Adaptive Prompting\n",
      "File 2: Better Zero-Shot Reasoning with Self-Adaptive Prompting\n",
      "\n",
      "Match found:\n",
      "File 1: Universal Self-Adaptive Prompting\n",
      "File 2: Universal Self-adaptive Prompting\n",
      "\n",
      "Total papers in first file: 109\n",
      "Total papers in second file: 4886\n",
      "Number of matching papers: 50\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def clean_title(title):\n",
    "    # Remove common punctuation and convert to lowercase\n",
    "    return title.lower().replace(\":\", \"\").replace(\"-\", \"\").replace(\"  \", \" \").strip()\n",
    "\n",
    "with open('annotated_papers.json', 'r') as file1, open('pdf_contents.json', 'r') as file2:\n",
    "    data1 = json.load(file1)\n",
    "    data2 = json.load(file2)\n",
    "    \n",
    "    matches = 0\n",
    "    \n",
    "    # For each paper in first file\n",
    "    for title1, paper1 in data1.items():\n",
    "        title1_clean = clean_title(title1)\n",
    "        \n",
    "        # Check if this title exists in any title from second file\n",
    "        for title2 in data2.keys():\n",
    "            title2_clean = clean_title(title2)\n",
    "            \n",
    "            # Check if titles are effectively the same\n",
    "            if title1_clean == title2_clean or \\\n",
    "               title1_clean in title2_clean and len(title1_clean) > 20:  # Length check to avoid short title false matches\n",
    "                matches += 1\n",
    "                print(f\"\\nMatch found:\")\n",
    "                print(f\"File 1: {title1}\")\n",
    "                print(f\"File 2: {title2}\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\nTotal papers in first file: {len(data1)}\")\n",
    "    print(f\"Total papers in second file: {len(data2)}\")\n",
    "    print(f\"Number of matching papers: {matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e43616a0-dd55-441f-bd89-8108bce8e47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found: ReAct: Synergizing Reasoning and Acting in Language Models\n",
      "Match found: Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n",
      "Match found: Boosted Prompt Ensembles for Large Language Models\n",
      "Match found: Ask Me Anything: A simple strategy for prompting language models\n",
      "Match found: Graph of Thoughts: Solving Elaborate Problems with Large Language Models\n",
      "Match found: More Agents Is All You Need\n",
      "Match found: ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs\n",
      "Match found: Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models\n",
      "Match found: Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems\n",
      "Match found: InferFix: End-to-End Program Repair with LLMs\n",
      "Match found: Revisit Input Perturbation Problems for LLMs: A Unified Robustness Evaluation Framework for Noisy Slot Filling Task\n",
      "Match found: FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation\n",
      "Match found: DiversiGATE: A Comprehensive Framework for Reliable Large Language Models\n",
      "Match found: Large Language Models are Zero-Shot Reasoners\n",
      "Match found: ART: Automatic multi-step reasoning and tool-use for large language models\n",
      "Match found: MathPrompter: Mathematical Reasoning using Large Language Models\n",
      "Match found: Prompting GPT-3 To Be Reliable\n",
      "Match found: Beyond Self-Consistency: Ensemble Reasoning Boosts Consistency and Accuracy of LLMs in Cancer Staging\n",
      "Match found: Chain-of-Verification Reduces Hallucination in Large Language Models\n",
      "Match found: LM vs LM: Detecting Factual Errors via Cross Examination\n",
      "Match found: REFINER: Reasoning Feedback on Intermediate Representations\n",
      "Match found: Reliable Natural Language Understanding with Large Language Models and Answer Set Programming\n",
      "Match found: Complexity-based Prompting for Multi-step Reasoning\n",
      "Match found: Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling\n",
      "Match found: Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions\n",
      "Match found: Rationale-Augmented Ensembles in Language Models\n",
      "Match found: Automatic Chain of Thought Prompting in Large Language Models\n",
      "Match found: Inference-Time Intervention: Eliciting Truthful Answers from a Language Model\n",
      "Match found: Large Language Models Can Self-Improve\n",
      "Match found: Self-Refine: Iterative Refinement with Self-Feedback\n",
      "Match found: Reflexion: Language Agents with Verbal Reinforcement Learning\n",
      "Match found: PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine\n",
      "Match found: N-CRITICS: Self-Refinement of Large Language Models with Ensemble of Critics\n",
      "Match found: Improving Factuality and Reasoning in Language Models through Multiagent Debate\n",
      "Match found: Self-Consistency Improves Chain of Thought Reasoning in Language Models\n",
      "Match found: Universal Self-Consistency for Large Language Model Generation\n",
      "Match found: Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models\n",
      "Match found: Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate\n",
      "Match found: Boosting Language Models Reasoning with Chain-of-Knowledge Prompting\n",
      "Match found: Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models\n",
      "Match found: Active Prompting with Chain-of-Thought for Large Language Models\n",
      "Match found: Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves\n",
      "Match found: Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models\n",
      "Match found: Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models\n",
      "Match found: Towards Expert-Level Medical Question Answering with Large Language Models\n",
      "Match found: Embodied LLM Agents Learn to Cooperate in Organized Teams\n",
      "Match found: The Consensus Game: Language Model Generation via Equilibrium Search\n",
      "Match found: RELIC: Investigating Large Language Model Responses using Self-Consistency\n",
      "Match found: Better Zero-Shot Reasoning with Self-Adaptive Prompting\n",
      "Match found: Universal Self-Adaptive Prompting\n",
      "\n",
      "Found 50 matching papers to analyze\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing matched papers:  18%|███▏              | 9/50 [01:05<07:29, 10.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to matched_papers_analysis_itercheckpoint_10_20250103_132347.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing matched papers:  38%|██████▍          | 19/50 [01:47<02:18,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to matched_papers_analysis_itercheckpoint_20_20250103_132430.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing matched papers:  58%|█████████▊       | 29/50 [02:32<01:31,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to matched_papers_analysis_itercheckpoint_30_20250103_132516.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing matched papers:  78%|█████████████▎   | 39/50 [03:16<00:47,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to matched_papers_analysis_itercheckpoint_40_20250103_132559.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing matched papers:  98%|████████████████▋| 49/50 [04:13<00:05,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to matched_papers_analysis_itercheckpoint_50_20250103_132658.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing matched papers: 100%|█████████████████| 50/50 [04:20<00:00,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to matched_papers_analysis_iterfinal_20250103_132700.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import openai\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "from datetime import datetime\n",
    "\n",
    "client = OpenAI(api_key='')  # Replace with your API key\n",
    "\n",
    "def count_tokens(text):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def truncate_text(text, max_tokens=14000):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    tokens = encoding.encode(text)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "        text = encoding.decode(tokens)\n",
    "    return text\n",
    "\n",
    "def create_prompt(paper_text):\n",
    "    return f\"\"\"Analyze this academic paper and extract two specific types of information:\n",
    "\n",
    "1. Benchmarks/Datasets used for evaluation:\n",
    "   - Include standard evaluation datasets (e.g., MNIST, ImageNet, SQuAD)\n",
    "   - Include custom datasets if they're used for evaluation\n",
    "   - Do NOT include training datasets unless they're also used for evaluation\n",
    "\n",
    "2. Base Language Models used in experiments:\n",
    "   - Include specific model architectures and variants (e.g., GPT-4, LLaMA-70B, PaLM-540B), which includes parameter size\n",
    "   - Do NOT include methods or techniques (e.g., don't include Chain-of-Thought, Self-Consistency, etc.)\n",
    "   - For custom models, specify the base model they use (e.g., if a paper introduces \"CustomBERT\", note it uses BERT as base)\n",
    "\n",
    "Format your response as a JSON object with this exact structure:\n",
    "{{\n",
    "    \"benchmarks\": [\"benchmark1\", \"benchmark2\"],\n",
    "    \"base_models\": [\"model1 (with size if specified)\", \"model2 (with size if specified)\"]\n",
    "}}\n",
    "\n",
    "Paper text:\n",
    "{paper_text}\"\"\"\n",
    "\n",
    "def analyze_paper_with_gpt(paper_text):\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            truncated_text = truncate_text(paper_text)\n",
    "            was_truncated = len(truncated_text) < len(paper_text)\n",
    "            \n",
    "            prompt = create_prompt(truncated_text)\n",
    "            token_count = count_tokens(prompt)\n",
    "            \n",
    "            if token_count > 15000:\n",
    "                print(\"Warning: Token count exceeds safe limit\")\n",
    "                return {\n",
    "                    \"benchmarks\": [],\n",
    "                    \"models\": [],\n",
    "                    \"error\": \"Token limit exceeded\"\n",
    "                }\n",
    "                \n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful graduate research assistant that analyzes academic papers and returns responses in JSON format only.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=1000,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "\n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            # Add error checking for response content\n",
    "            if not result.strip().startswith('{') or not result.strip().endswith('}'):\n",
    "                raise json.JSONDecodeError(\"Invalid JSON format\", result, 0)\n",
    "                \n",
    "            parsed_result = json.loads(result)\n",
    "            \n",
    "            if was_truncated:\n",
    "                parsed_result[\"note\"] = \"Analysis based on truncated paper text\"\n",
    "                \n",
    "            return parsed_result\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"\\nAttempt {attempt + 1}/{max_retries} - JSON parsing error: {str(e)}\")\n",
    "            print(f\"Raw response: {result}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return {\n",
    "                    \"benchmarks\": [],\n",
    "                    \"models\": [],\n",
    "                    \"error\": f\"Failed to parse GPT response as JSON after {max_retries} attempts: {str(e)}\"\n",
    "                }\n",
    "            time.sleep(2)  # Wait before retry\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nAttempt {attempt + 1}/{max_retries} - Error: {str(e)}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return {\n",
    "                    \"benchmarks\": [],\n",
    "                    \"models\": [],\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "            time.sleep(2)  # Wait before retry\n",
    "\n",
    "def save_progress(analysis_results, base_filename, iteration):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{base_filename}_iter{iteration}_{timestamp}.json\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(analysis_results, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"\\nProgress saved to {filename}\")\n",
    "\n",
    "def clean_title(title):\n",
    "    return title.lower().replace(\":\", \"\").replace(\"-\", \"\").replace(\"  \", \" \").strip()\n",
    "\n",
    "def main():\n",
    "    # First, load and find matching papers\n",
    "    with open('annotated_papers.json', 'r') as file1, open('pdf_contents.json', 'r') as file2:\n",
    "        data1 = json.load(file1)\n",
    "        pdf_contents = json.load(file2)\n",
    "        \n",
    "        matched_papers = {}\n",
    "        \n",
    "        # Find matching papers\n",
    "        for title1, paper1 in data1.items():\n",
    "            title1_clean = clean_title(title1)\n",
    "            \n",
    "            for paper_id, paper_data in pdf_contents.items():\n",
    "                title2 = paper_data['filename']\n",
    "                title2_clean = clean_title(title2)\n",
    "                \n",
    "                if title1_clean == title2_clean or \\\n",
    "                   (title1_clean in title2_clean and len(title1_clean) > 20):\n",
    "                    matched_papers[paper_id] = paper_data\n",
    "                    print(f\"Match found: {title1}\")\n",
    "                    break\n",
    "\n",
    "    print(f\"\\nFound {len(matched_papers)} matching papers to analyze\")\n",
    "    \n",
    "    # Now process only the matched papers\n",
    "    analysis_results = {}\n",
    "    save_interval = 10  # Reduced save interval since we have fewer papers\n",
    "    \n",
    "    # Convert items to list for tqdm\n",
    "    items = list(matched_papers.items())\n",
    "    \n",
    "    for i, (paper_id, paper_data) in enumerate(tqdm(items, desc=\"Analyzing matched papers\")):\n",
    "        paper_text = paper_data.get('text', '')\n",
    "        \n",
    "        if not paper_text:\n",
    "            print(f\"\\nSkipping {paper_id} - no text content\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            result = analyze_paper_with_gpt(paper_text)\n",
    "            analysis_results[paper_id] = {\n",
    "                \"filename\": paper_data['filename'],\n",
    "                \"analysis\": result\n",
    "            }\n",
    "            \n",
    "            # Save progress more frequently since we're processing fewer papers\n",
    "            if (i + 1) % save_interval == 0:\n",
    "                save_progress(analysis_results, \"matched_papers_analysis\", f\"checkpoint_{i+1}\")\n",
    "                \n",
    "            time.sleep(2)  # Rate limiting\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError analyzing {paper_id}: {str(e)}\")\n",
    "            analysis_results[paper_id] = {\n",
    "                \"filename\": paper_data['filename'],\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    # Save final results\n",
    "    save_progress(analysis_results, \"matched_papers_analysis\", \"final\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a4e4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 4886 papers to analyze\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   0%|                     | 19/4886 [01:22<6:03:38,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_20_20250107_140550.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   1%|                     | 25/4886 [01:55<6:22:50,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   1%|▏                    | 39/4886 [03:06<7:45:07,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_40_20250107_140727.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   1%|▎                    | 59/4886 [04:42<6:00:59,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_60_20250107_140904.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   2%|▎                    | 79/4886 [06:48<9:12:03,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_80_20250107_141110.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   2%|▍                    | 99/4886 [08:16<5:38:43,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_100_20250107_141237.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   2%|▍                   | 119/4886 [10:10<5:45:55,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_120_20250107_141432.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   3%|▌                   | 139/4886 [11:40<5:31:01,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_140_20250107_141602.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   3%|▋                   | 159/4886 [13:22<5:53:46,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_160_20250107_141745.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   4%|▋                   | 179/4886 [15:27<8:34:39,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_180_20250107_141949.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   4%|▊                   | 199/4886 [17:10<6:00:34,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_200_20250107_142133.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   4%|▉                   | 219/4886 [19:05<8:33:10,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_220_20250107_142327.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   5%|▉                   | 239/4886 [20:40<6:26:20,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_240_20250107_142502.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   5%|█                   | 259/4886 [22:21<6:42:11,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_260_20250107_142643.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   6%|█▏                  | 279/4886 [23:54<7:07:05,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_280_20250107_142817.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   6%|█▏                  | 299/4886 [25:30<7:27:45,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_300_20250107_142952.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   7%|█▎                  | 319/4886 [27:11<6:49:04,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_320_20250107_143133.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   7%|█▍                  | 339/4886 [29:11<6:34:46,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_340_20250107_143336.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   7%|█▍                  | 359/4886 [30:48<5:15:36,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_360_20250107_143509.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   8%|█▌                  | 379/4886 [32:25<6:58:32,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_380_20250107_143647.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   8%|█▋                  | 399/4886 [33:47<5:31:18,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_400_20250107_143810.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   9%|█▋                  | 419/4886 [35:25<5:47:17,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_420_20250107_143948.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   9%|█▊                  | 439/4886 [37:17<9:08:40,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_440_20250107_144139.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   9%|█▉                  | 459/4886 [38:45<5:21:46,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_460_20250107_144309.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  10%|█▉                  | 479/4886 [40:41<7:09:51,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_480_20250107_144503.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  10%|██                  | 499/4886 [42:25<6:53:40,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_500_20250107_144648.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  11%|██                  | 519/4886 [43:59<5:28:40,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_520_20250107_144821.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  11%|██▏                 | 539/4886 [45:29<5:08:45,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_540_20250107_144951.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  11%|██▎                 | 559/4886 [47:04<5:54:32,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_560_20250107_145126.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  12%|██▎                 | 579/4886 [48:49<4:49:02,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_580_20250107_145314.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  12%|██▍                 | 599/4886 [50:25<4:43:43,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_600_20250107_145447.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  13%|██▌                 | 619/4886 [52:07<6:38:30,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_620_20250107_145629.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  13%|██▌                 | 639/4886 [53:42<5:55:24,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_640_20250107_145803.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  13%|██▋                 | 659/4886 [55:16<5:17:01,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_660_20250107_145938.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  14%|██▊                 | 679/4886 [56:55<4:50:18,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_680_20250107_150117.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  14%|██▊                 | 693/4886 [57:55<5:09:01,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  14%|██▊                 | 699/4886 [58:30<7:42:10,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_700_20250107_150302.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  15%|██▋               | 719/4886 [1:00:22<6:23:34,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_720_20250107_150447.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Analyzing papers:  15%|██▋               | 720/4886 [1:00:29<7:09:10,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  15%|██▋               | 739/4886 [1:02:06<5:52:56,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_740_20250107_150627.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  16%|██▊               | 759/4886 [1:03:45<5:26:31,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_760_20250107_150808.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  16%|██▊               | 779/4886 [1:05:32<5:46:35,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_780_20250107_150953.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  16%|██▉               | 799/4886 [1:07:19<5:50:48,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_800_20250107_151141.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  17%|███               | 819/4886 [1:09:16<5:39:12,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_820_20250107_151338.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  17%|███               | 827/4886 [1:09:53<5:39:50,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endofprompt|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endofprompt|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endofprompt|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endofprompt|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endofprompt|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endofprompt|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endofprompt|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endofprompt|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endofprompt|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  17%|███               | 839/4886 [1:11:20<7:19:12,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_840_20250107_151541.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  18%|███▏              | 859/4886 [1:13:02<4:43:06,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_860_20250107_151725.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  18%|███▏              | 879/4886 [1:14:57<5:25:30,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_880_20250107_151920.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  18%|███▎              | 899/4886 [1:16:40<6:02:19,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_900_20250107_152103.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  19%|███▍              | 919/4886 [1:18:20<4:18:20,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_920_20250107_152242.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  19%|███▍              | 939/4886 [1:19:46<4:34:12,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_940_20250107_152408.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  20%|███▌              | 959/4886 [1:21:17<5:31:24,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_960_20250107_152539.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  20%|███▌              | 979/4886 [1:22:50<4:20:58,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_980_20250107_152711.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  20%|███▋              | 999/4886 [1:24:22<4:31:51,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1000_20250107_152844.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  21%|███▌             | 1019/4886 [1:26:20<8:15:56,  7.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1020_20250107_153043.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  21%|███▌             | 1039/4886 [1:28:01<5:29:46,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1040_20250107_153227.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  22%|███▋             | 1059/4886 [1:29:36<4:49:08,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1060_20250107_153357.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  22%|███▊             | 1079/4886 [1:31:20<6:45:03,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1080_20250107_153542.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  22%|███▊             | 1099/4886 [1:33:10<5:07:55,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1100_20250107_153733.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  23%|███▉             | 1119/4886 [1:34:49<5:37:13,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1120_20250107_153916.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  23%|███▉             | 1139/4886 [1:36:25<4:17:48,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1140_20250107_154047.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  24%|████             | 1159/4886 [1:37:53<4:57:22,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1160_20250107_154215.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  24%|████             | 1179/4886 [1:39:28<4:45:59,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1180_20250107_154350.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  25%|████▏            | 1199/4886 [1:40:56<4:20:20,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1200_20250107_154518.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  25%|████▏            | 1219/4886 [1:42:28<4:39:55,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1220_20250107_154652.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  25%|████▎            | 1239/4886 [1:44:21<6:02:16,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1240_20250107_154843.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  26%|████▍            | 1259/4886 [1:45:48<4:15:06,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1260_20250107_155009.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  26%|████▍            | 1279/4886 [1:47:12<4:30:26,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1280_20250107_155140.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  27%|████▌            | 1299/4886 [1:48:49<4:13:25,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1300_20250107_155311.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  27%|████▌            | 1319/4886 [1:50:30<5:37:57,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1320_20250107_155451.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  27%|████▋            | 1335/4886 [1:51:40<4:40:03,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: 'NoneType' object has no attribute 'strip'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  27%|████▋            | 1339/4886 [1:51:59<4:22:09,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1340_20250107_155621.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  28%|████▋            | 1359/4886 [1:53:24<4:22:50,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1360_20250107_155746.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  28%|████▊            | 1379/4886 [1:54:57<5:22:31,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1380_20250107_155919.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  29%|████▊            | 1399/4886 [1:56:30<5:00:15,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1400_20250107_160052.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  29%|████▉            | 1419/4886 [1:58:05<4:39:47,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1420_20250107_160226.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  29%|█████            | 1439/4886 [1:59:43<3:53:48,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1440_20250107_160405.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  30%|█████            | 1459/4886 [2:01:06<3:36:42,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1460_20250107_160530.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  30%|█████▏           | 1479/4886 [2:02:28<3:56:47,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1480_20250107_160652.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  31%|█████▏           | 1499/4886 [2:03:57<4:06:37,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1500_20250107_160819.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  31%|█████▎           | 1519/4886 [2:05:20<3:44:39,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1520_20250107_160943.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  31%|█████▎           | 1539/4886 [2:06:48<3:56:02,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1540_20250107_161113.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  32%|█████▍           | 1559/4886 [2:08:23<3:50:13,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1560_20250107_161244.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  32%|█████▍           | 1579/4886 [2:09:47<3:31:48,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1580_20250107_161409.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  33%|█████▌           | 1599/4886 [2:11:13<3:34:24,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1600_20250107_161535.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  33%|█████▋           | 1619/4886 [2:12:58<4:57:25,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1620_20250107_161719.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  34%|█████▋           | 1639/4886 [2:14:33<4:57:05,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1640_20250107_161855.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  34%|█████▊           | 1659/4886 [2:16:14<6:47:34,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1660_20250107_162036.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  34%|█████▊           | 1679/4886 [2:17:44<3:58:26,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1680_20250107_162206.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  35%|█████▉           | 1699/4886 [2:19:11<3:56:04,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1700_20250107_162332.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  35%|█████▉           | 1719/4886 [2:20:36<3:24:39,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1720_20250107_162459.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  36%|██████           | 1739/4886 [2:22:05<3:49:57,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1740_20250107_162627.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  36%|██████           | 1759/4886 [2:23:25<3:43:32,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1760_20250107_162746.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  36%|██████▏          | 1779/4886 [2:24:53<3:49:18,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1780_20250107_162914.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  37%|██████▎          | 1799/4886 [2:26:14<3:21:07,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1800_20250107_163036.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  37%|██████▎          | 1819/4886 [2:27:45<3:09:53,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1820_20250107_163208.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  38%|██████▍          | 1839/4886 [2:29:16<3:46:47,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1840_20250107_163339.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  38%|██████▍          | 1859/4886 [2:30:36<3:09:24,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1860_20250107_163457.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  38%|██████▌          | 1879/4886 [2:32:05<4:10:41,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1880_20250107_163626.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  39%|██████▌          | 1899/4886 [2:33:38<4:35:16,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1900_20250107_163759.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  39%|██████▋          | 1919/4886 [2:35:10<3:20:43,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1920_20250107_163933.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  40%|██████▋          | 1939/4886 [2:36:40<3:35:37,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1940_20250107_164106.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  40%|██████▊          | 1959/4886 [2:38:20<4:19:11,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1960_20250107_164248.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  41%|██████▉          | 1979/4886 [2:39:54<4:08:42,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1980_20250107_164416.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  41%|██████▉          | 1999/4886 [2:41:20<3:06:10,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2000_20250107_164542.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  41%|███████          | 2019/4886 [2:42:47<4:00:53,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2020_20250107_164708.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  42%|███████          | 2039/4886 [2:44:18<3:54:11,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2040_20250107_164839.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  42%|███████▏         | 2059/4886 [2:45:54<3:33:36,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2060_20250107_165015.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  43%|███████▏         | 2079/4886 [2:47:31<3:18:12,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2080_20250107_165153.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  43%|███████▎         | 2099/4886 [2:49:05<3:24:42,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2100_20250107_165326.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  43%|███████▎         | 2119/4886 [2:50:29<3:00:08,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2120_20250107_165453.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  44%|███████▍         | 2139/4886 [2:54:12<3:48:32,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2140_20250107_165834.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  44%|███████▌         | 2159/4886 [2:55:36<3:13:52,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2160_20250107_165958.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  45%|███████▌         | 2179/4886 [2:57:11<3:06:10,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2180_20250107_170134.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  45%|███████▋         | 2199/4886 [2:58:49<3:42:19,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2200_20250107_170314.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  45%|███████▋         | 2219/4886 [3:00:15<2:50:28,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2220_20250107_170437.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  46%|███████▊         | 2239/4886 [3:01:39<3:24:10,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2240_20250107_170601.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  46%|███████▊         | 2259/4886 [3:03:04<3:20:19,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2260_20250107_170728.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  47%|███████▉         | 2279/4886 [3:04:34<2:51:20,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2280_20250107_170857.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  47%|███████▉         | 2299/4886 [3:05:50<2:33:11,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2300_20250107_171012.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  47%|████████         | 2319/4886 [3:07:16<3:17:05,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2320_20250107_171139.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  48%|████████▏        | 2339/4886 [3:08:38<3:06:33,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2340_20250107_171259.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  48%|████████▏        | 2356/4886 [3:09:49<2:41:29,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  48%|████████▏        | 2359/4886 [3:10:04<3:07:17,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2360_20250107_171426.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  49%|████████▎        | 2379/4886 [3:11:32<2:55:24,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2380_20250107_171553.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  49%|████████▎        | 2399/4886 [3:12:59<3:05:40,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2400_20250107_171720.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  50%|████████▍        | 2419/4886 [3:14:25<2:48:54,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2420_20250107_171847.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  50%|████████▍        | 2439/4886 [3:15:49<2:50:03,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2440_20250107_172010.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  50%|████████▌        | 2459/4886 [3:17:21<3:37:32,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2460_20250107_172143.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  51%|████████▋        | 2479/4886 [3:18:48<2:41:12,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2480_20250107_172310.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  51%|████████▋        | 2499/4886 [3:20:20<2:57:32,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2500_20250107_172442.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  52%|████████▊        | 2519/4886 [3:21:52<2:40:01,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2520_20250107_172613.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  52%|████████▊        | 2539/4886 [3:23:27<2:53:47,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2540_20250107_172750.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  52%|████████▉        | 2559/4886 [3:24:54<2:58:11,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2560_20250107_172915.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  53%|████████▉        | 2579/4886 [3:26:24<2:49:48,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2580_20250107_173045.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  53%|█████████        | 2599/4886 [3:27:46<2:25:44,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2600_20250107_173209.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  54%|█████████        | 2619/4886 [3:29:20<2:57:10,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2620_20250107_173342.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  54%|█████████▏       | 2639/4886 [3:30:41<2:35:35,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2640_20250107_173504.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  54%|█████████▎       | 2659/4886 [3:32:34<6:10:49,  9.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2660_20250107_173656.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  55%|█████████▎       | 2679/4886 [3:34:07<2:48:27,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2680_20250107_173830.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  55%|█████████▍       | 2699/4886 [3:35:32<2:26:04,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2700_20250107_173953.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  56%|█████████▍       | 2719/4886 [3:37:07<4:58:53,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2720_20250107_174134.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  56%|█████████▌       | 2739/4886 [3:38:49<2:40:50,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2740_20250107_174310.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  56%|█████████▌       | 2759/4886 [3:40:35<3:37:31,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2760_20250107_174458.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  57%|█████████▋       | 2779/4886 [3:41:50<2:04:30,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2780_20250107_174612.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  57%|█████████▋       | 2799/4886 [3:43:03<2:10:40,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2800_20250107_174725.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  58%|█████████▊       | 2819/4886 [3:44:13<1:54:42,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2820_20250107_174835.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  58%|█████████▉       | 2839/4886 [3:45:30<2:03:01,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2840_20250107_174951.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  59%|█████████▉       | 2859/4886 [3:46:47<2:27:24,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2860_20250107_175109.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  59%|██████████       | 2879/4886 [3:47:59<2:10:19,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2880_20250107_175220.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  59%|██████████       | 2899/4886 [3:49:12<1:50:05,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2900_20250107_175333.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  60%|██████████▏      | 2919/4886 [3:50:27<2:15:00,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2920_20250107_175448.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  60%|██████████▏      | 2925/4886 [3:50:50<2:05:10,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  60%|██████████▏      | 2939/4886 [3:51:42<1:49:05,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2940_20250107_175603.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  61%|██████████▎      | 2959/4886 [3:52:56<2:00:06,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2960_20250107_175717.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  61%|██████████▎      | 2979/4886 [3:54:10<1:54:31,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2980_20250107_175832.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  61%|██████████▍      | 2999/4886 [3:55:28<1:57:07,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3000_20250107_175949.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  62%|██████████▌      | 3019/4886 [3:56:44<2:02:48,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3020_20250107_180105.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  62%|██████████▌      | 3039/4886 [3:57:57<1:48:49,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3040_20250107_180219.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  62%|██████████▌      | 3048/4886 [3:58:35<1:59:07,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  63%|██████████▋      | 3059/4886 [3:59:20<1:52:11,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3060_20250107_180340.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  63%|██████████▋      | 3079/4886 [4:00:32<1:50:50,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3080_20250107_180453.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  63%|██████████▊      | 3099/4886 [4:01:45<1:55:31,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3100_20250107_180606.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  64%|██████████▊      | 3119/4886 [4:02:54<1:44:55,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3120_20250107_180715.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  64%|██████████▉      | 3139/4886 [4:04:07<1:56:36,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3140_20250107_180828.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  65%|██████████▉      | 3159/4886 [4:05:25<1:46:54,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3160_20250107_180947.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  65%|███████████      | 3179/4886 [4:06:41<1:47:07,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3180_20250107_181102.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  65%|███████████▏     | 3199/4886 [4:08:03<1:46:50,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3200_20250107_181224.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  66%|███████████▏     | 3219/4886 [4:09:17<1:36:29,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3220_20250107_181339.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  66%|███████████▎     | 3239/4886 [4:10:35<1:51:16,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3240_20250107_181456.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  67%|███████████▎     | 3259/4886 [4:11:53<1:40:24,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3260_20250107_181615.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  67%|███████████▍     | 3279/4886 [4:13:02<1:30:00,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3280_20250107_181723.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  68%|███████████▍     | 3299/4886 [4:14:19<1:32:05,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3300_20250107_181845.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  68%|███████████▌     | 3319/4886 [4:15:46<1:36:52,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3320_20250107_182008.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  68%|███████████▌     | 3339/4886 [4:17:00<1:46:11,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3340_20250107_182126.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  69%|███████████▋     | 3359/4886 [4:18:16<1:31:02,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3360_20250107_182239.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  69%|███████████▊     | 3379/4886 [4:19:29<1:30:32,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3380_20250107_182350.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  70%|███████████▊     | 3399/4886 [4:20:46<2:01:54,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3400_20250107_182508.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  70%|███████████▉     | 3419/4886 [4:21:57<1:33:42,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3420_20250107_182618.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  70%|███████████▉     | 3436/4886 [4:22:57<1:27:45,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  70%|███████████▉     | 3439/4886 [4:23:11<1:41:49,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3440_20250107_182732.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  71%|████████████     | 3459/4886 [4:24:36<1:33:36,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3460_20250107_182857.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  71%|████████████     | 3479/4886 [4:25:50<1:27:04,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3480_20250107_183011.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  72%|████████████▏    | 3499/4886 [4:27:02<1:25:11,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3500_20250107_183123.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  72%|████████████▏    | 3519/4886 [4:28:24<2:09:40,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3520_20250107_183245.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  72%|████████████▎    | 3539/4886 [4:29:41<1:22:18,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3540_20250107_183403.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  73%|████████████▍    | 3559/4886 [4:31:00<1:27:45,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3560_20250107_183522.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  73%|████████████▍    | 3579/4886 [4:32:09<1:16:07,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3580_20250107_183631.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  74%|████████████▌    | 3599/4886 [4:33:20<1:16:53,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3600_20250107_183741.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  74%|████████████▌    | 3619/4886 [4:34:33<1:17:42,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3620_20250107_183854.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  74%|████████████▋    | 3639/4886 [4:35:47<1:20:22,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3640_20250107_184009.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  75%|████████████▋    | 3659/4886 [4:37:05<1:12:36,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3660_20250107_184126.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  75%|████████████▋    | 3664/4886 [4:37:22<1:10:36,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  75%|████████████▊    | 3679/4886 [4:38:22<1:20:42,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3680_20250107_184243.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  76%|████████████▊    | 3699/4886 [4:39:39<1:06:32,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3700_20250107_184401.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  76%|████████████▉    | 3719/4886 [4:40:56<1:18:18,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3720_20250107_184518.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  77%|█████████████    | 3739/4886 [4:42:10<1:12:26,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3740_20250107_184634.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  77%|█████████████    | 3759/4886 [4:43:28<1:03:52,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3760_20250107_184750.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  77%|█████████████▏   | 3779/4886 [4:44:44<1:09:49,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3780_20250107_184905.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  78%|█████████████▏   | 3799/4886 [4:46:07<1:42:56,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3800_20250107_185029.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  78%|█████████████▎   | 3819/4886 [4:47:25<1:19:24,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3820_20250107_185146.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  79%|█████████████▎   | 3839/4886 [4:48:47<1:16:36,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3840_20250107_185309.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  79%|█████████████▍   | 3859/4886 [4:50:02<1:04:06,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3860_20250107_185424.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  79%|█████████████▍   | 3879/4886 [4:51:18<1:06:20,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3880_20250107_185539.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  80%|█████████████▌   | 3899/4886 [4:52:33<1:01:02,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3900_20250107_185654.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  80%|█████████████▋   | 3919/4886 [4:53:52<1:02:49,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3920_20250107_185818.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  81%|███████████████▎   | 3939/4886 [4:55:11<57:23,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3940_20250107_185933.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  81%|███████████████▍   | 3959/4886 [4:56:28<57:34,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3960_20250107_190049.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  81%|█████████████▊   | 3979/4886 [4:57:47<1:09:44,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3980_20250107_190210.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  82%|█████████████▉   | 3999/4886 [4:59:24<1:05:01,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4000_20250107_190346.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  82%|███████████████▋   | 4019/4886 [5:00:46<58:02,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4020_20250107_190508.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  83%|███████████████▋   | 4039/4886 [5:02:15<58:03,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4040_20250107_190637.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  83%|███████████████▊   | 4059/4886 [5:03:36<50:11,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4060_20250107_190758.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  83%|███████████████▊   | 4079/4886 [5:04:48<50:03,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4080_20250107_190909.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  84%|██████████████▎  | 4099/4886 [5:06:13<1:06:44,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4100_20250107_191034.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  84%|████████████████   | 4119/4886 [5:07:36<58:50,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4120_20250107_191158.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  85%|████████████████   | 4139/4886 [5:08:57<46:25,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4140_20250107_191318.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  85%|████████████████▏  | 4159/4886 [5:10:10<44:14,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4160_20250107_191432.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  86%|████████████████▎  | 4179/4886 [5:11:27<45:19,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4180_20250107_191548.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  86%|████████████████▎  | 4199/4886 [5:12:48<47:42,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4200_20250107_191709.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  86%|████████████████▍  | 4219/4886 [5:14:04<44:28,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4220_20250107_191825.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  87%|████████████████▍  | 4227/4886 [5:14:33<40:26,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  87%|████████████████▍  | 4239/4886 [5:15:23<45:24,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4240_20250107_191945.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  87%|████████████████▌  | 4259/4886 [5:16:45<45:16,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4260_20250107_192109.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  88%|████████████████▋  | 4279/4886 [5:18:02<42:11,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4280_20250107_192223.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  88%|████████████████▋  | 4299/4886 [5:19:36<41:05,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4300_20250107_192357.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  88%|████████████████▊  | 4319/4886 [5:20:55<35:08,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4320_20250107_192517.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  89%|████████████████▊  | 4339/4886 [5:22:36<47:27,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4340_20250107_192657.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  89%|████████████████▉  | 4359/4886 [5:24:06<38:06,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4360_20250107_192828.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  90%|█████████████████  | 4379/4886 [5:25:46<33:29,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4380_20250107_193007.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  90%|█████████████████  | 4399/4886 [5:27:07<34:10,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4400_20250107_193128.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  90%|█████████████████▏ | 4419/4886 [5:28:36<37:23,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4420_20250107_193258.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  91%|█████████████████▎ | 4439/4886 [5:30:09<28:38,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4440_20250107_193431.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  91%|█████████████████▎ | 4459/4886 [5:31:27<26:57,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4460_20250107_193549.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  92%|█████████████████▍ | 4479/4886 [5:33:04<32:49,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4480_20250107_193726.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  92%|█████████████████▍ | 4499/4886 [5:34:37<26:41,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4500_20250107_193859.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  92%|█████████████████▌ | 4519/4886 [5:36:00<27:03,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4520_20250107_194022.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  93%|█████████████████▋ | 4539/4886 [5:37:25<23:55,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4540_20250107_194146.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  93%|█████████████████▋ | 4559/4886 [5:38:44<19:29,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4560_20250107_194308.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  94%|█████████████████▊ | 4579/4886 [5:40:08<19:33,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4580_20250107_194429.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  94%|█████████████████▉ | 4599/4886 [5:41:53<36:55,  7.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4600_20250107_194615.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  95%|█████████████████▉ | 4619/4886 [5:43:26<18:27,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4620_20250107_194748.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  95%|██████████████████ | 4639/4886 [5:45:06<22:13,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4640_20250107_194928.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  95%|██████████████████ | 4659/4886 [5:46:37<17:52,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4660_20250107_195059.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  96%|██████████████████▏| 4679/4886 [5:48:01<13:59,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4680_20250107_195224.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  96%|██████████████████▎| 4699/4886 [5:49:28<12:54,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4700_20250107_195350.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  97%|██████████████████▎| 4719/4886 [5:50:48<11:16,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4720_20250107_195510.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  97%|██████████████████▍| 4739/4886 [5:52:10<09:36,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4740_20250107_195632.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  97%|██████████████████▌| 4759/4886 [5:53:41<10:45,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4760_20250107_195802.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  98%|██████████████████▌| 4779/4886 [5:55:09<09:08,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4780_20250107_195930.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  98%|██████████████████▋| 4799/4886 [5:56:33<05:29,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4800_20250107_200055.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  99%|██████████████████▋| 4819/4886 [5:58:20<07:04,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4820_20250107_200242.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  99%|██████████████████▊| 4839/4886 [5:59:56<03:38,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4840_20250107_200417.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  99%|██████████████████▉| 4859/4886 [6:01:42<02:15,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4860_20250107_200604.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers: 100%|██████████████████▉| 4879/4886 [6:03:11<00:30,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4880_20250107_200733.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers: 100%|███████████████████| 4886/4886 [6:03:41<00:00,  4.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_iterfinal_20250107_200800.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import openai\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Disable OpenAI logging\n",
    "logging.getLogger(\"openai\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "client = OpenAI(api_key='')  # Replace with your API key\n",
    "\n",
    "def count_tokens(text):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def truncate_text(text, max_tokens=14000):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    tokens = encoding.encode(text)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "        text = encoding.decode(tokens)\n",
    "    return text\n",
    "\n",
    "def create_prompt(paper_text):\n",
    "    return f\"\"\"Analyze this academic paper and extract two specific types of information:\n",
    "\n",
    "1. Benchmarks/Datasets used for evaluation:\n",
    "   - Include standard evaluation datasets (e.g., MNIST, ImageNet, SQuAD)\n",
    "   - Include custom datasets if they're used for evaluation\n",
    "   - Do NOT include training datasets unless they're also used for evaluation\n",
    "\n",
    "2. Base Language Models used in experiments:\n",
    "   - Include specific model architectures and variants (e.g., GPT-4, LLaMA-70B, PaLM-540B), which includes parameter size\n",
    "   - Do NOT include methods or techniques (e.g., don't include Chain-of-Thought, Self-Consistency, etc.)\n",
    "   - For custom models, specify the base model they use (e.g., if a paper introduces \"CustomBERT\", note it uses BERT as base)\n",
    "\n",
    "Format your response as a JSON object with this exact structure:\n",
    "{{\n",
    "    \"benchmarks\": [\"benchmark1\", \"benchmark2\"],\n",
    "    \"base_models\": [\"model1 (with size if specified)\", \"model2 (with size if specified)\"]\n",
    "}}\n",
    "\n",
    "Paper text:\n",
    "{paper_text}\"\"\"\n",
    "\n",
    "def analyze_paper_with_gpt(paper_text):\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            truncated_text = truncate_text(paper_text)\n",
    "            was_truncated = len(truncated_text) < len(paper_text)\n",
    "            \n",
    "            prompt = create_prompt(truncated_text)\n",
    "            token_count = count_tokens(prompt)\n",
    "            \n",
    "            if token_count > 15000:\n",
    "                print(\"Warning: Token count exceeds safe limit\")\n",
    "                return {\n",
    "                    \"benchmarks\": [],\n",
    "                    \"models\": [],\n",
    "                    \"error\": \"Token limit exceeded\"\n",
    "                }\n",
    "                \n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful graduate research assistant that analyzes academic papers and returns responses in JSON format only.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=1000,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "\n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            # Add error checking for response content\n",
    "            if not result.strip().startswith('{') or not result.strip().endswith('}'):\n",
    "                raise json.JSONDecodeError(\"Invalid JSON format\", result, 0)\n",
    "                \n",
    "            parsed_result = json.loads(result)\n",
    "            \n",
    "            if was_truncated:\n",
    "                parsed_result[\"note\"] = \"Analysis based on truncated paper text\"\n",
    "                \n",
    "            return parsed_result\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"\\nAttempt {attempt + 1}/{max_retries} - JSON parsing error: {str(e)}\")\n",
    "            print(f\"Raw response: {result}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return {\n",
    "                    \"benchmarks\": [],\n",
    "                    \"models\": [],\n",
    "                    \"error\": f\"Failed to parse GPT response as JSON after {max_retries} attempts: {str(e)}\"\n",
    "                }\n",
    "            time.sleep(2)  # Wait before retry\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nAttempt {attempt + 1}/{max_retries} - Error: {str(e)}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return {\n",
    "                    \"benchmarks\": [],\n",
    "                    \"models\": [],\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "            time.sleep(2)  # Wait before retry\n",
    "\n",
    "def save_progress(analysis_results, base_filename, iteration):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{base_filename}_iter{iteration}_{timestamp}.json\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(analysis_results, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"\\nProgress saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    # Load all papers from pdf_contents\n",
    "    with open('pdf_contents.json', 'r') as file:\n",
    "        pdf_contents = json.load(file)\n",
    "\n",
    "    print(f\"\\nFound {len(pdf_contents)} papers to analyze\")\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    analysis_results = {}\n",
    "    save_interval = 20  # Save every 20 papers\n",
    "    \n",
    "    # Convert items to list for tqdm\n",
    "    items = list(pdf_contents.items())\n",
    "    \n",
    "    for i, (paper_id, paper_data) in enumerate(tqdm(items, desc=\"Analyzing papers\")):\n",
    "        paper_text = paper_data.get('text', '')\n",
    "        \n",
    "        if not paper_text:\n",
    "            print(f\"\\nSkipping {paper_id} - no text content\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            result = analyze_paper_with_gpt(paper_text)\n",
    "            analysis_results[paper_id] = {\n",
    "                \"filename\": paper_data['filename'],\n",
    "                \"analysis\": result\n",
    "            }\n",
    "            \n",
    "            # Save progress periodically\n",
    "            if (i + 1) % save_interval == 0:\n",
    "                save_progress(analysis_results, \"papers_analysis\", f\"checkpoint_{i+1}\")\n",
    "                \n",
    "            time.sleep(2)  # Rate limiting\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError analyzing {paper_id}: {str(e)}\")\n",
    "            analysis_results[paper_id] = {\n",
    "                \"filename\": paper_data['filename'],\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    # Save final results\n",
    "    save_progress(analysis_results, \"papers_analysis\", \"final\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db34b881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Statistics ===\n",
      "Total unique models (after normalization): 4809\n",
      "Total model mentions: 16665\n",
      "\n",
      "Top 20 most frequently mentioned models:\n",
      "        model  frequency\n",
      "        gpt-4       1579\n",
      "gpt-3.5-turbo        743\n",
      "      gpt-3.5        530\n",
      "       gpt-4o        251\n",
      "        gpt-3        250\n",
      "         bert        198\n",
      "   mistral-7b        183\n",
      "   llama-2-7b        165\n",
      "     llama-7b        140\n",
      "        llama        140\n",
      "    llama2-7b        134\n",
      "       gpt-4v        120\n",
      "      llama-2        109\n",
      "      roberta        108\n",
      "  llama-2-13b        102\n",
      " gpt-3-(175b)         98\n",
      "  llama-2-70b         95\n",
      "        gpt-2         90\n",
      "   vicuna-13b         86\n",
      "           t5         78\n",
      "\n",
      "=== Benchmark Statistics ===\n",
      "Total unique benchmarks: 7635\n",
      "Total benchmark mentions: 15013\n",
      "\n",
      "Top 20 most frequently mentioned benchmarks:\n",
      "    benchmark  frequency\n",
      "        gsm8k        424\n",
      "         math        175\n",
      "         mmlu        171\n",
      "        svamp        139\n",
      "   strategyqa        126\n",
      "     hotpotqa        120\n",
      "    humaneval        120\n",
      "   truthfulqa         81\n",
      "commonsenseqa         78\n",
      "    hellaswag         71\n",
      "     triviaqa         64\n",
      "         mbpp         64\n",
      "         aqua         64\n",
      "   multiarith         61\n",
      "   winogrande         57\n",
      "        boolq         56\n",
      "         piqa         53\n",
      "   openbookqa         50\n",
      "        asdiv         46\n",
      "        squad         45\n",
      "\n",
      "Detailed frequency data has been saved to:\n",
      "- model_frequencies_normalized.csv\n",
      "- benchmark_frequencies_normalized.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from typing import Dict, Any\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def normalize_model_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes model names to handle different variations.\n",
    "    \n",
    "    Args:\n",
    "        name: Original model name\n",
    "        \n",
    "    Returns:\n",
    "        Normalized model name\n",
    "    \"\"\"\n",
    "    # Remove quotation marks and extra spaces\n",
    "    name = name.strip('\"').strip()\n",
    "    \n",
    "    # Convert to lowercase for initial processing\n",
    "    normalized = name.lower()\n",
    "    \n",
    "    # Remove extra spaces between parts\n",
    "    normalized = re.sub(r'\\s+', ' ', normalized)\n",
    "    \n",
    "    # Standardize common variations while preserving distinct models\n",
    "    replacements = {\n",
    "        'gpt-3.5-turbo': ['gpt-3.5 turbo', 'gpt3.5-turbo', 'gpt3.5 turbo', 'gpt-3.5-t', 'chatgpt', 'chat-gpt', 'chat gpt'],  # ChatGPT uses GPT-3.5-turbo\n",
    "        'gpt-3.5': ['gpt3.5', 'gpt 3.5'],  # Keep base GPT-3.5 separate\n",
    "        'gpt-4': ['gpt4', 'gpt 4', 'gpt-4-turbo'],  # Group GPT-4 variations\n",
    "        'llama-2': ['llama2', 'llama-2', 'llama 2'],  # Standardize LLaMA 2 naming\n",
    "        'llama': ['llama1', 'llama 1'],  # Keep original LLaMA separate\n",
    "        'roberta': ['roberta-base', 'roberta base'],\n",
    "        'bert': ['bert-base', 'bert base'],\n",
    "    }\n",
    "    \n",
    "    # Apply replacements\n",
    "    for standard, variants in replacements.items():\n",
    "        if normalized in variants or normalized == standard:\n",
    "            return standard\n",
    "            \n",
    "    # Standardize separators\n",
    "    normalized = re.sub(r'[-_\\s]+', '-', normalized)\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def analyze_paper_data(data: Dict[str, Any]) -> tuple[Counter, Counter]:\n",
    "    \"\"\"\n",
    "    Recursively analyzes paper data to count unique models and benchmarks.\n",
    "    \"\"\"\n",
    "    model_counter = Counter()\n",
    "    benchmark_counter = Counter()\n",
    "    \n",
    "    def process_item(item):\n",
    "        if isinstance(item, dict):\n",
    "            # Handle base_models section\n",
    "            if 'base_models' in item and isinstance(item['base_models'], list):\n",
    "                for model in item['base_models']:\n",
    "                    if isinstance(model, str):\n",
    "                        normalized_name = normalize_model_name(model)\n",
    "                        model_counter[normalized_name] += 1\n",
    "            \n",
    "            # Handle benchmarks section\n",
    "            if 'benchmarks' in item and isinstance(item['benchmarks'], list):\n",
    "                for benchmark in item['benchmarks']:\n",
    "                    if isinstance(benchmark, str):\n",
    "                        normalized_name = benchmark.strip('\"').strip().lower()\n",
    "                        normalized_name = re.sub(r'[-_\\s]+', '-', normalized_name)\n",
    "                        benchmark_counter[normalized_name] += 1\n",
    "            \n",
    "            # Recursively process all dictionary values\n",
    "            for value in item.values():\n",
    "                process_item(value)\n",
    "                \n",
    "        elif isinstance(item, list):\n",
    "            # Recursively process all list items\n",
    "            for value in item:\n",
    "                process_item(value)\n",
    "    \n",
    "    process_item(data)\n",
    "    return model_counter, benchmark_counter\n",
    "\n",
    "def create_frequency_dataframes(model_counter: Counter, benchmark_counter: Counter) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Creates pandas DataFrames with frequency information.\n",
    "    \"\"\"\n",
    "    # Create DataFrame for models\n",
    "    models_df = pd.DataFrame([\n",
    "        {\"model\": model, \"frequency\": count}\n",
    "        for model, count in model_counter.most_common()\n",
    "    ])\n",
    "    \n",
    "    # Create DataFrame for benchmarks\n",
    "    benchmarks_df = pd.DataFrame([\n",
    "        {\"benchmark\": benchmark, \"frequency\": count}\n",
    "        for benchmark, count in benchmark_counter.most_common()\n",
    "    ])\n",
    "    \n",
    "    return models_df, benchmarks_df\n",
    "\n",
    "def print_statistics(models_df: pd.DataFrame, benchmarks_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Prints detailed statistics about models and benchmarks.\n",
    "    \"\"\"\n",
    "    print(\"=== Model Statistics ===\")\n",
    "    print(f\"Total unique models (after normalization): {len(models_df)}\")\n",
    "    print(f\"Total model mentions: {models_df['frequency'].sum()}\")\n",
    "    print(\"\\nTop 20 most frequently mentioned models:\")\n",
    "    print(models_df.head(20).to_string(index=False))\n",
    "    \n",
    "    print(\"\\n=== Benchmark Statistics ===\")\n",
    "    print(f\"Total unique benchmarks: {len(benchmarks_df)}\")\n",
    "    print(f\"Total benchmark mentions: {benchmarks_df['frequency'].sum()}\")\n",
    "    print(\"\\nTop 20 most frequently mentioned benchmarks:\")\n",
    "    print(benchmarks_df.head(20).to_string(index=False))\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Load and process data\n",
    "        with open('papers_analysis_iterfinal_20250107_200800.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Get frequency counts\n",
    "        model_counter, benchmark_counter = analyze_paper_data(data)\n",
    "        \n",
    "        # Create DataFrames\n",
    "        models_df, benchmarks_df = create_frequency_dataframes(model_counter, benchmark_counter)\n",
    "        \n",
    "        # Save frequency data to CSV\n",
    "        models_df.to_csv('model_frequencies_normalized.csv', index=False)\n",
    "        benchmarks_df.to_csv('benchmark_frequencies_normalized.csv', index=False)\n",
    "        \n",
    "        # Print statistics\n",
    "        print_statistics(models_df, benchmarks_df)\n",
    "        \n",
    "        print(\"\\nDetailed frequency data has been saved to:\")\n",
    "        print(\"- model_frequencies_normalized.csv\")\n",
    "        print(\"- benchmark_frequencies_normalized.csv\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: papers_analysis_iterfinal_20250107_200800.json file not found\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: Invalid JSON format in the input file\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae79e46e-b724-4b60-bb92-7d47a521aa58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
