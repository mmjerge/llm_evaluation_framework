{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f37f9e6f-ed0e-4bd2-93d1-d01e20220cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total citations to fetch: 6318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching citations: 100%|██████████████████| 6318/6318 [00:25<00:00, 252.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total citations with arXiv IDs: 4895\n",
      "\n",
      "First 5 papers to be downloaded:\n",
      "1. Enhancing Table Recognition with Vision LLMs: A Benchmark and Neighbor-Guided Toolchain Reasoner\n",
      "2. Efficiently Serving LLM Reasoning Programs with Certaindex\n",
      "3. Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs\n",
      "4. Pushing the Envelope of Low-Bit LLM via Dynamic Error Compensation\n",
      "5. Toward Adaptive Reasoning in Large Language Models with Thought Rollback\n",
      "\n",
      "Starting downloads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing arXiv papers:   0%|                         | 0/4895 [00:00<?, ?it/s]/var/folders/0q/6k0gk6g5281gcsvh418ty0140000gn/T/ipykernel_2060/2173061590.py:10: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  paper = next(search.results())\n",
      "Processing arXiv papers:  10%|█▌             | 502/4895 [05:29<41:40,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error downloading http://arxiv.org/pdf/2410.07062v2: 404 Client Error: Not Found for url: http://arxiv.org/pdf/2410.07062v2\n",
      "\n",
      "Failed to download: TinyEmo: Scaling down Emotional Reasoning via Metric Projection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing arXiv papers:  37%|█████         | 1789/4895 [18:54<27:51,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error downloading http://arxiv.org/pdf/2405.02659v2: 404 Client Error: Not Found for url: http://arxiv.org/pdf/2405.02659v2\n",
      "\n",
      "Failed to download: R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing arXiv papers:  57%|███████▉      | 2777/4895 [31:35<34:25,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error downloading http://arxiv.org/pdf/2312.08926v2: 404 Client Error: Not Found for url: http://arxiv.org/pdf/2312.08926v2\n",
      "\n",
      "Failed to download: Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing arXiv papers:  63%|████████▊     | 3098/4895 [38:37<15:57,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error downloading http://arxiv.org/pdf/2310.18331v2: 404 Client Error: NOT FOUND for url: http://arxiv.org/pdf/2310.18331v2\n",
      "\n",
      "Failed to download: AllTogether: Investigating the Efficacy of Spliced Prompt for Web Navigation using Large Language Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing arXiv papers:  64%|█████████     | 3148/4895 [39:23<22:01,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error downloading http://arxiv.org/pdf/2310.10698v2: 404 Client Error: Not Found for url: http://arxiv.org/pdf/2310.10698v2\n",
      "\n",
      "Failed to download: Bridging Code Semantic and LLMs: Semantic Chain-of-Thought Prompting for Code Generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing arXiv papers:  70%|█████████▊    | 3438/4895 [44:14<24:42,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error downloading http://arxiv.org/pdf/2309.12481v2: 404 Client Error: NOT FOUND for url: http://arxiv.org/pdf/2309.12481v2\n",
      "\n",
      "Failed to download: HANS, are you clever? Clever Hans Effect Analysis of Neural Systems\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing arXiv papers:  71%|████████▌   | 3470/4895 [45:19<2:06:37,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error downloading http://arxiv.org/pdf/2309.09749v3: 404 Client Error: Not Found for url: http://arxiv.org/pdf/2309.09749v3\n",
      "\n",
      "Failed to download: Facilitating NSFW Text Detection in Open-Domain Dialogue Systems via Knowledge Distillation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing arXiv papers:  80%|███████████▏  | 3922/4895 [54:26<14:02,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error downloading http://arxiv.org/pdf/2306.08997v2: 404 Client Error: NOT FOUND for url: http://arxiv.org/pdf/2306.08997v2\n",
      "\n",
      "Failed to download: Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing arXiv papers:  95%|███████████▍| 4667/4895 [1:10:46<05:45,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error downloading http://arxiv.org/pdf/2306.07622v2: 404 Client Error: Not Found for url: http://arxiv.org/pdf/2306.07622v2\n",
      "\n",
      "Failed to download: Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing arXiv papers: 100%|████████████| 4895/4895 [1:14:17<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import arxiv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def get_arxiv_pdf(arxiv_id):\n",
    "    try:\n",
    "        search = arxiv.Search(id_list=[arxiv_id])\n",
    "        paper = next(search.results())\n",
    "        return paper.pdf_url\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError getting arXiv PDF URL for {arxiv_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def retrieve_url(url, filepath):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(filepath, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError downloading {url}: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_all_citations(paper_id):\n",
    "    citations = []\n",
    "    offset = 0\n",
    "    limit = 1000\n",
    "    total_citations = 0\n",
    "    \n",
    "    url = f\"http://api.semanticscholar.org/graph/v1/paper/{paper_id}\"\n",
    "    params = {\"fields\": \"citationCount\"}\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            total_citations = response.json()['citationCount']\n",
    "            print(f\"Total citations to fetch: {total_citations}\")\n",
    "        else:\n",
    "            print(f\"Failed to get citation count: {response.status_code}\")\n",
    "            return citations\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting citation count: {e}\")\n",
    "        return citations\n",
    "    \n",
    "    with tqdm(total=total_citations, desc=\"Fetching citations\") as pbar:\n",
    "        while True:\n",
    "            url = f\"http://api.semanticscholar.org/graph/v1/paper/{paper_id}/citations\"\n",
    "            params = {\n",
    "                \"fields\": \"citingPaper.title,citingPaper.externalIds,citingPaper.url\",\n",
    "                \"limit\": limit,\n",
    "                \"offset\": offset\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(url, params=params)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    batch = data.get('data', [])\n",
    "                    if not batch:\n",
    "                        break\n",
    "                    \n",
    "                    citations.extend(batch)\n",
    "                    pbar.update(len(batch))\n",
    "                    \n",
    "                    offset += limit\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    print(f\"Error fetching citations: {response.status_code}\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Error in citation request: {e}\")\n",
    "                break\n",
    "                \n",
    "    return citations\n",
    "\n",
    "paperId = \"1b6e810ce0afd0dd093f789d2b2742d047e316d5\"\n",
    "citations = get_all_citations(paperId)\n",
    "\n",
    "arxiv_citations = []\n",
    "for citation in citations:\n",
    "    if 'citingPaper' in citation:\n",
    "        paper = citation['citingPaper']\n",
    "        external_ids = paper.get('externalIds', {})\n",
    "        if any(key.lower() == 'arxiv' for key in external_ids.keys()):\n",
    "            arxiv_citations.append(paper)\n",
    "\n",
    "print(f\"\\nTotal citations with arXiv IDs: {len(arxiv_citations)}\")\n",
    "print(\"\\nFirst 5 papers to be downloaded:\")\n",
    "for i, paper in enumerate(arxiv_citations[:5]):\n",
    "    print(f\"{i+1}. {paper.get('title')}\")\n",
    "\n",
    "print(\"\\nStarting downloads...\")\n",
    "if not os.path.exists(\"pdfs\"):\n",
    "    os.makedirs(\"pdfs\")\n",
    "\n",
    "for paper in tqdm(arxiv_citations, desc=\"Processing arXiv papers\"):\n",
    "    title = paper['title']\n",
    "    arxiv_key = next(key for key in paper['externalIds'].keys() if key.lower() == 'arxiv')\n",
    "    arxiv_id = paper['externalIds'][arxiv_key]\n",
    "    \n",
    "    safe_title = \"\".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).strip()\n",
    "    pdf_path = f\"pdfs/{safe_title}.pdf\"\n",
    "    \n",
    "    if os.path.exists(pdf_path):\n",
    "        continue\n",
    "        \n",
    "    pdf_url = get_arxiv_pdf(arxiv_id)\n",
    "    if pdf_url:\n",
    "        if retrieve_url(pdf_url, pdf_path):\n",
    "            continue  \n",
    "        else:\n",
    "            print(f\"\\nFailed to download: {title}\")\n",
    "    \n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d26637aa-a5e2-47b6-b2b1-adbc75791594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|████████████████████| 4886/4886 [1:00:14<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved data to pdf_contents.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"PyPDF2\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "warnings.filterwarnings('ignore', message='.*FloatObject.*invalid.*')\n",
    "warnings.filterwarnings('ignore', message='.*unknown widths.*')\n",
    "warnings.filterwarnings('ignore', category=Warning, module='PyPDF2')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text of invalid Unicode characters\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Replace surrogate characters with a replacement character\n",
    "    return text.encode('utf-16', 'surrogatepass').decode('utf-16', 'replace')\n",
    "\n",
    "directory = \"pdfs\"\n",
    "pdf_files = list(Path(directory).glob('*.pdf'))\n",
    "pdf_text_dict = {}\n",
    "\n",
    "# Create progress bar\n",
    "for file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "    try:\n",
    "        reader = PdfReader(file)\n",
    "        meta = reader.metadata\n",
    "        \n",
    "        # Initialize dictionary entry with metadata, cleaning each field\n",
    "        pdf_text_dict[file.stem] = {\n",
    "            \"filename\": file.name,\n",
    "            \"author\": clean_text(meta.author) if meta.author else \"\",\n",
    "            \"creator\": clean_text(meta.creator) if meta.creator else \"\",\n",
    "            \"subject\": clean_text(meta.subject) if meta.subject else \"\",\n",
    "            \"title\": clean_text(meta.title) if meta.title else \"\",\n",
    "            \"text\": \"\"\n",
    "        }\n",
    "        \n",
    "        # Extract text from all pages\n",
    "        text_content = []\n",
    "        for page in reader.pages:\n",
    "            try:\n",
    "                extracted_text = page.extract_text()\n",
    "                if extracted_text:\n",
    "                    text_content.append(clean_text(extracted_text))\n",
    "            except Exception as page_error:\n",
    "                print(f\"\\nError extracting text from page in {file}: {str(page_error)}\")\n",
    "                continue\n",
    "            \n",
    "        # Join all pages' text with newlines\n",
    "        pdf_text_dict[file.stem][\"text\"] = \"\\n\".join(text_content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing {file}: {str(e)}\")\n",
    "        pdf_text_dict[file.stem] = {\n",
    "            \"filename\": file.name,\n",
    "            \"error\": str(e),\n",
    "            \"text\": \"\"\n",
    "        }\n",
    "\n",
    "# Save to JSON file\n",
    "output_path = \"pdf_contents.json\"\n",
    "try:\n",
    "    # First try with standard encoding\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(pdf_text_dict, f, ensure_ascii=False, indent=4)\n",
    "    except UnicodeEncodeError:\n",
    "        # If that fails, fall back to ensuring ASCII with escaping\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(pdf_text_dict, f, ensure_ascii=True, indent=4)\n",
    "    print(f\"\\nSuccessfully saved data to {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving JSON file: {str(e)}\")\n",
    "    # Last resort: try to save with problematic characters removed\n",
    "    try:\n",
    "        cleaned_dict = {k: {\n",
    "            key: str(value).encode('ascii', 'ignore').decode('ascii') \n",
    "            for key, value in v.items()\n",
    "        } for k, v in pdf_text_dict.items()}\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(cleaned_dict, f, ensure_ascii=True, indent=4)\n",
    "        print(f\"Saved file with ASCII-only characters to {output_path}\")\n",
    "    except Exception as final_e:\n",
    "        print(f\"Final attempt to save failed: {str(final_e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa11ee5a-3ffd-48e7-9ca3-aeb545689407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   3%|▎         | 48/1886 [04:20<2:44:35,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   4%|▍         | 72/1886 [07:24<3:53:16,  7.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - JSON parsing error: Invalid JSON format: line 1 column 1 (char 0)\n",
      "Raw response: {\n",
      "    \"benchmarks\": [\n",
      "        \"AQA-Bench\",\n",
      "        \"GuessNum\",\n",
      "        \"DFS\",\n",
      "        \"BFS\",\n",
      "        \"Coin\",\n",
      "        \"CaveDFS\",\n",
      "        \"CaveBFS\"\n",
      "    ],\n",
      "    \"models\": [\n",
      "        \"GPT-4\",\n",
      "        \"Gemini\",\n",
      "        \"GPT-3.5-Turbo\",\n",
      "        \"GPT-4-Turbo\",\n",
      "        \"Gemini-Pro\",\n",
      "        \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   9%|▊         | 161/1886 [20:59<2:14:33,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - JSON parsing error: Invalid JSON format: line 1 column 1 (char 0)\n",
      "Raw response: {\n",
      "    \"benchmarks\": [\n",
      "        \"ACE04\",\n",
      "        \"ACE05\",\n",
      "        \"CoNLL03\",\n",
      "        \"OntoNotes 5.0\",\n",
      "        \"GENIA\",\n",
      "        \"NYT\",\n",
      "        \"ADE\",\n",
      "        \"CoNLL04\",\n",
      "        \"SciERC\",\n",
      "        \"TACRED\",\n",
      "        \"Re-TACRED\",\n",
      "        \"TACREV\",\n",
      "        \"SemEval\"\n",
      "    ],\n",
      "    \"models\": [\n",
      "        \"GPT-4\",\n",
      "        \"ChatGPT\",\n",
      "        \"LLaMA\",\n",
      "        \"Flan-T5\",\n",
      "        \"CodeX\",\n",
      "        \"GPT-NER\",\n",
      "        \"Cp-NER\",\n",
      "        \"LLMaAA\",\n",
      "        \"PromptNER\",\n",
      "        \"UniNER\",\n",
      "        \"NAG-NER\",\n",
      "        \"GNER\",\n",
      "        \"NuNER\",\n",
      "        \"MetaNER\",\n",
      "        \"LinkNER\",\n",
      "        \"SLCoLM\",\n",
      "        \"ProgGen\",\n",
      "        \"C-ICL\",\n",
      "        \"VerifiNER\",\n",
      "        \"ConsistNER\",\n",
      "        \"GLiNER\",\n",
      "        \"LTNER\",\n",
      "        \"ToNER\",\n",
      "        \"RT\",\n",
      "        \"VANER\",\n",
      "        \"RiVEG\",\n",
      "        \"LLM-DA\",\n",
      "        \"REBEL\",\n",
      "        \"QA4RE\",\n",
      "        \"GPT-RE\",\n",
      "        \"STAR\",\n",
      "        \"AugURE\",\n",
      "        \"REPAL\",\n",
      "        \"RAG4RE\",\n",
      "        \"BART-Gen\",\n",
      "        \"Text2Event\",\n",
      "        \"ClarET\",\n",
      "        \"X-GEAR\",\n",
      "        \"PAIE\",\n",
      "        \"GTEE-DYNPREF\",\n",
      "        \"Code4Struct\",\n",
      "        \"PGAD\",\n",
      "        \"QGA-EE\",\n",
      "        \"SPEAE\",\n",
      "        \"AMPERE\",\n",
      "        \"KeyEE\",\n",
      "        \"ULTRA\",\n",
      "        \"DEEPSTRUCT\",\n",
      "        \"GenIE\",\n",
      "        \"UIE\",\n",
      "        \"LasUIE\",\n",
      "        \"ChatIE\",\n",
      "        \"InstructUIE\",\n",
      "        \"GIELLM\",\n",
      "        \"Set\",\n",
      "        \"CollabKG\",\n",
      "        \"TechGPT-2\",\n",
      "        \"YAYI-UIE\",\n",
      "        \"ChatUIE\",\n",
      "        \"IEPile\",\n",
      "        \"Guo et al.\",\n",
      "        \"CODEIE\",\n",
      "        \"CodeKGC\",\n",
      "        \"GoLLIE\",\n",
      "        \"Code4UIE\",\n",
      "        \"KnowCoder\",\n",
      "        \"TANL\",\n",
      "        \"TEMPGEN\",\n",
      "        \"Cui et al.\",\n",
      "        \"Yan et al.\",\n",
      "        \"Xia et al.\",\n",
      "        \"EnTDA\",\n",
      "        \"YAYI-UIE\",\n",
      "        \"KnowCoder\",\n",
      "        \"USM\",\n",
      "        \"RexUIE\",\n",
      "        \"Mirror\",\n",
      "        \"Li et al.\",\n",
      "        \"Xu et al.\",\n",
      "        \"Otto et al.\",\n",
      "        \"Shi et al.\",\n",
      "        \"Li et al.\",\n",
      "        \"Ni et al.\",\n",
      "        \"Zaratiana et al.\",\n",
      "        \"MetaIE\",\n",
      "        \"Atuhurra et al.\",\n",
      "        \"CHisIEC\",\n",
      "        \"Veyseh et al.\",\n",
      "        \"DAFS\",\n",
      "        \"Cai et al.\",\n",
      "        \"Ni et al.\",\n",
      "        \"BART-Gen\",\n",
      "        \"DEGREE\",\n",
      "        \"DemoSG\",\n",
      "        \"Kwak et al.\",\n",
      "        \"EventRL\",\n",
      "        \"Huang et al.\",\n",
      "        \"Ding et al.\",\n",
      "        \"Sun et al.\",\n",
      "        \"Zhou et al.\",\n",
      "        \"Hu et al.\",\n",
      "        \"Shao et al.\",\n",
      "        \"Evans et al.\",\n",
      "        \"Gonzalez et al.\",\n",
      "        \"Nune et al.\",\n",
      "        \"Oliveira et al.\",\n",
      "        \"Kwak et al.\",\n",
      "        \"Gutierrez et al.\",\n",
      "        \"GPT-3 +R\",\n",
      "        \"Labrak et al.\",\n",
      "        \"Xie et al.\",\n",
      "        \"Gao et al.\",\n",
      "        \"InstructIE\",\n",
      "        \"Han et al.\",\n",
      "        \"Katz et al.\",\n",
      "        \"XNLP\",\n",
      "        \"Foppiano et al.\",\n",
      "        \"PolyIE\",\n",
      "        \"Li et al.\",\n",
      "        \"Qi et al.\",\n",
      "        \"SynthIE\",\n",
      "        \"Chen et al.\",\n",
      "        \"Amalvy et al.\",\n",
      "        \"PGIM\",\n",
      "        \"VerifiNER\",\n",
      "        \"EnTDA\",\n",
      "        \"UniversalNER\",\n",
      "        \"GNER\",\n",
      "        \"Chen et al.\",\n",
      "        \"Wadhwa et al.\",\n",
      "        \"Yuan et al.\",\n",
      "        \"Bian et al.\",\n",
      "        \"Xie et al.\",\n",
      "        \"ProgGen\",\n",
      "        \"QA4RE\",\n",
      "        \"SUMASK\",\n",
      "        \"GPT-RE\",\n",
      "        \"Xu et al.\",\n",
      "        \"REBEL\",\n",
      "        \"Li et al.\",\n",
      "        \"Code4Struct\",\n",
      "        \"Code4UIE\",\n",
      "        \"TANL\",\n",
      "        \"Text2Event\",\n",
      "        \"BART-Gen\",\n",
      "        \"GTEE-DYNPREF\",\n",
      "        \"DEEPSTRUCT\",\n",
      "        \"PAIE\",\n",
      "        \"PGAD\",\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   9%|▉         | 176/1886 [34:42<5:37:17, 11.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 162\u001b[0m\n\u001b[1;32m    159\u001b[0m     save_progress(analysis_results, output_base, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 162\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[5], line 139\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     result \u001b[38;5;241m=\u001b[39m analyze_paper_with_gpt(paper_text)\n\u001b[1;32m    140\u001b[0m     analysis_results[paper_id] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m: paper_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalysis\u001b[39m\u001b[38;5;124m\"\u001b[39m: result\n\u001b[1;32m    143\u001b[0m     }\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Save progress every save_interval papers\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 66\u001b[0m, in \u001b[0;36manalyze_paper_with_gpt\u001b[0;34m(paper_text)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: Token count exceeds safe limit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenchmarks\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken limit exceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m     }\n\u001b[0;32m---> 66\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     67\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     68\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     69\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant that analyzes academic papers and returns responses in JSON format only.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     70\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}\n\u001b[1;32m     71\u001b[0m     ],\n\u001b[1;32m     72\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     73\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     74\u001b[0m     response_format\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson_object\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     77\u001b[0m result \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Add error checking for response content\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py:581\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    580\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    583\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    584\u001b[0m             {\n\u001b[1;32m    585\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    586\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    587\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    588\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    589\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    590\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    591\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    592\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    593\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    595\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    596\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    597\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    598\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    599\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    600\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    601\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    602\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    603\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    604\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    605\u001b[0m             },\n\u001b[1;32m    606\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    607\u001b[0m         ),\n\u001b[1;32m    608\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    609\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    610\u001b[0m         ),\n\u001b[1;32m    611\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    612\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    613\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    614\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1233\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1221\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1228\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1229\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1230\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1231\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1232\u001b[0m     )\n\u001b[0;32m-> 1233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:922\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    915\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    920\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    921\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 922\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    923\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    924\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    925\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    926\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    927\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    928\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:951\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    948\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_auth\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 951\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m    952\u001b[0m         request,\n\u001b[1;32m    953\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[1;32m    954\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    955\u001b[0m     )\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    957\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m    915\u001b[0m     request,\n\u001b[1;32m    916\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m    917\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    918\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    919\u001b[0m )\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m    943\u001b[0m         request,\n\u001b[1;32m    944\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    945\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(\n\u001b[1;32m    197\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv(max_bytes)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/ssl.py:1296\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(buflen)\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/ssl.py:1169\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import openai\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "from datetime import datetime\n",
    "\n",
    "# Add your OpenAI API token here\n",
    "\n",
    "def count_tokens(text):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def truncate_text(text, max_tokens=14000):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    tokens = encoding.encode(text)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "        text = encoding.decode(tokens)\n",
    "    return text\n",
    "\n",
    "def create_prompt(paper_text):\n",
    "    return f\"\"\"Analyze this academic paper and extract all benchmarks/datasets and models mentioned in experiments, evaluations, or comparisons.\n",
    "\n",
    "Rules for extraction:\n",
    "1. For benchmarks/datasets:\n",
    "   - Include standard evaluation datasets (e.g., MNIST, ImageNet, SQuAD)\n",
    "   - Include custom datasets if they're used for evaluation\n",
    "   - Do NOT include training datasets unless they're also used for evaluation\n",
    "\n",
    "2. For models:\n",
    "   - Include baseline models used for comparison\n",
    "   - Include proposed/novel models being evaluated\n",
    "   - Include model variants tested in ablation studies\n",
    "   - Do NOT include referenced models that weren't actually tested\n",
    "\n",
    "Format your response as a JSON object with this exact structure:\n",
    "{{\n",
    "    \"benchmarks\": [\"benchmark1\", \"benchmark2\"],\n",
    "    \"models\": [\"model1\", \"model2\"]\n",
    "}}\n",
    "\n",
    "Paper text:\n",
    "{paper_text}\"\"\"\n",
    "\n",
    "def analyze_paper_with_gpt(paper_text):\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            truncated_text = truncate_text(paper_text)\n",
    "            was_truncated = len(truncated_text) < len(paper_text)\n",
    "            \n",
    "            prompt = create_prompt(truncated_text)\n",
    "            token_count = count_tokens(prompt)\n",
    "            \n",
    "            if token_count > 15000:\n",
    "                print(\"Warning: Token count exceeds safe limit\")\n",
    "                return {\n",
    "                    \"benchmarks\": [],\n",
    "                    \"models\": [],\n",
    "                    \"error\": \"Token limit exceeded\"\n",
    "                }\n",
    "                \n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that analyzes academic papers and returns responses in JSON format only.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=1000,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "\n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            # Add error checking for response content\n",
    "            if not result.strip().startswith('{') or not result.strip().endswith('}'):\n",
    "                raise json.JSONDecodeError(\"Invalid JSON format\", result, 0)\n",
    "                \n",
    "            parsed_result = json.loads(result)\n",
    "            \n",
    "            if was_truncated:\n",
    "                parsed_result[\"note\"] = \"Analysis based on truncated paper text\"\n",
    "                \n",
    "            return parsed_result\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"\\nAttempt {attempt + 1}/{max_retries} - JSON parsing error: {str(e)}\")\n",
    "            print(f\"Raw response: {result}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return {\n",
    "                    \"benchmarks\": [],\n",
    "                    \"models\": [],\n",
    "                    \"error\": f\"Failed to parse GPT response as JSON after {max_retries} attempts: {str(e)}\"\n",
    "                }\n",
    "            time.sleep(2)  # Wait before retry\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nAttempt {attempt + 1}/{max_retries} - Error: {str(e)}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return {\n",
    "                    \"benchmarks\": [],\n",
    "                    \"models\": [],\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "            time.sleep(2)  # Wait before retry\n",
    "\n",
    "def save_progress(analysis_results, base_filename, iteration):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{base_filename}_iter{iteration}_{timestamp}.json\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(analysis_results, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"\\nProgress saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    input_file = \"pdf_contents.json\"\n",
    "    output_base = \"paper_analysis\"\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        pdf_contents = json.load(f)\n",
    "\n",
    "    analysis_results = {}\n",
    "    \n",
    "    # Convert items to list for tqdm\n",
    "    items = list(pdf_contents.items())\n",
    "    save_interval = 500  # Save every 500 papers\n",
    "    \n",
    "    for i, (paper_id, paper_data) in enumerate(tqdm(items[3000:], desc=\"Analyzing papers\")):\n",
    "        paper_text = paper_data.get('text', '')\n",
    "        \n",
    "        if not paper_text:\n",
    "            print(f\"\\nSkipping {paper_id} - no text content\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            result = analyze_paper_with_gpt(paper_text)\n",
    "            analysis_results[paper_id] = {\n",
    "                \"filename\": paper_data['filename'],\n",
    "                \"analysis\": result\n",
    "            }\n",
    "            \n",
    "            # Save progress every save_interval papers\n",
    "            if (i + 1) % save_interval == 0:\n",
    "                save_progress(analysis_results, output_base, f\"checkpoint_{i+1}\")\n",
    "                \n",
    "            time.sleep(2)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError analyzing {paper_id}: {str(e)}\")\n",
    "            analysis_results[paper_id] = {\n",
    "                \"filename\": paper_data['filename'],\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    # Save final results\n",
    "    save_progress(analysis_results, output_base, \"final\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a33ce81-6ecf-4cdd-87a6-d9bdf1e3a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "json_files = glob.glob('paper_analysis_itercheckpoint_*.json')\n",
    "combined_data = {}\n",
    "\n",
    "for file in json_files:\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "        combined_data.update(data)\n",
    "\n",
    "with open('combined_analysis.json', 'w') as f:\n",
    "    json.dump(combined_data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61924570-ed8e-4229-ad3c-21a99f876304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Properties', 'Models Used', 'Models Used Recently', 'Models Used Paste', 'Benchmarks Used', 'Benchmarks by Use'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "excel_file = \"ensemble_works-new.xlsx\"\n",
    "sheet_names = pd.read_excel(excel_file, sheet_name=None).keys()\n",
    "print(sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8dd6b62-f120-4f61-8f7d-09154493e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.read_excel(excel_file, sheet_name='Models Used')\n",
    "model_df = model_df.iloc[:109, :-2]\n",
    "# model_df.head()\n",
    "# model_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c55b503e-13d4-430b-ae47-e8adf6b61d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_df = pd.read_excel(excel_file, sheet_name='Benchmarks Used')\n",
    "benchmark_df = benchmarks_df.iloc[:109, :-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f40145e2-010a-4384-a08f-758e48b66243",
   "metadata": {},
   "outputs": [],
   "source": [
    "annoted_papers = {}\n",
    "\n",
    "for (index_one, model_row), (index_two, benchmark_row) in zip(model_df.iterrows(), benchmark_df.iterrows()):\n",
    "    paper_title = model_row['Paper Titles']\n",
    "    annoted_papers[paper_title] = {\n",
    "        \"filename\": f\"{paper_title}.pdf\",\n",
    "        \"analysis\": {\n",
    "            \"models\": [\n",
    "                model for model, value in model_row[2:].items()\n",
    "                if value == 1.0\n",
    "            ],\n",
    "            \"benchmarks\": [\n",
    "                benchmark for benchmark, value in benchmark_row[2:].items()\n",
    "                if value == 1.0\n",
    "            ],\n",
    "            \"note\": \"Analysis based on human review.\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "with open('annotated_papers.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(annoted_papers, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f79232c0-8d4f-494a-9f27-de93afcd4cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Match found:\n",
      "File 1: ReAct: Synergizing Reasoning and Acting in Language Models\n",
      "File 2: ReAct Synergizing Reasoning and Acting in Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n",
      "File 2: Tree of Thoughts Deliberate Problem Solving with Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Boosted Prompt Ensembles for Large Language Models\n",
      "File 2: Boosted Prompt Ensembles for Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Ask Me Anything: A simple strategy for prompting language models\n",
      "File 2: Ask Me Anything A simple strategy for prompting language models\n",
      "\n",
      "Match found:\n",
      "File 1: Graph of Thoughts: Solving Elaborate Problems with Large Language Models\n",
      "File 2: Graph of Thoughts Solving Elaborate Problems with Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: More Agents Is All You Need\n",
      "File 2: More Agents Is All You Need\n",
      "\n",
      "Match found:\n",
      "File 1: ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs\n",
      "File 2: ReConcile Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs\n",
      "\n",
      "Match found:\n",
      "File 1: Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models\n",
      "File 2: Boosting of Thoughts Trial-and-Error Problem Solving with Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems\n",
      "File 2: Fill in the Blank Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems\n",
      "\n",
      "Match found:\n",
      "File 1: InferFix: End-to-End Program Repair with LLMs\n",
      "File 2: InferFix End-to-End Program Repair with LLMs\n",
      "\n",
      "Match found:\n",
      "File 1: Revisit Input Perturbation Problems for LLMs: A Unified Robustness Evaluation Framework for Noisy Slot Filling Task\n",
      "File 2: Revisit Input Perturbation Problems for LLMs A Unified Robustness Evaluation Framework for Noisy Slot Filling Task\n",
      "\n",
      "Match found:\n",
      "File 1: FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation\n",
      "File 2: FreshLLMs Refreshing Large Language Models with Search Engine Augmentation\n",
      "\n",
      "Match found:\n",
      "File 1: DiversiGATE: A Comprehensive Framework for Reliable Large Language Models\n",
      "File 2: DiversiGATE A Comprehensive Framework for Reliable Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Large Language Models are Zero-Shot Reasoners\n",
      "File 2: Large Language Models are Zero-Shot Reasoners\n",
      "\n",
      "Match found:\n",
      "File 1: ART: Automatic multi-step reasoning and tool-use for large language models\n",
      "File 2: ART Automatic multi-step reasoning and tool-use for large language models\n",
      "\n",
      "Match found:\n",
      "File 1: MathPrompter: Mathematical Reasoning using Large Language Models\n",
      "File 2: MathPrompter Mathematical Reasoning using Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Prompting GPT-3 To Be Reliable\n",
      "File 2: Prompting GPT-3 To Be Reliable\n",
      "\n",
      "Match found:\n",
      "File 1: Beyond Self-Consistency: Ensemble Reasoning Boosts Consistency and Accuracy of LLMs in Cancer Staging\n",
      "File 2: Beyond Self-Consistency Ensemble Reasoning Boosts Consistency and Accuracy of LLMs in Cancer Staging\n",
      "\n",
      "Match found:\n",
      "File 1: Chain-of-Verification Reduces Hallucination in Large Language Models\n",
      "File 2: Chain-of-Verification Reduces Hallucination in Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: LM vs LM: Detecting Factual Errors via Cross Examination\n",
      "File 2: LM vs LM Detecting Factual Errors via Cross Examination\n",
      "\n",
      "Match found:\n",
      "File 1: REFINER: Reasoning Feedback on Intermediate Representations\n",
      "File 2: REFINER Reasoning Feedback on Intermediate Representations\n",
      "\n",
      "Match found:\n",
      "File 1: Reliable Natural Language Understanding with Large Language Models and Answer Set Programming\n",
      "File 2: Reliable Natural Language Understanding with Large Language Models and Answer Set Programming\n",
      "\n",
      "Match found:\n",
      "File 1: Complexity-based Prompting for Multi-step Reasoning\n",
      "File 2: Complexity-Based Prompting for Multi-Step Reasoning\n",
      "\n",
      "Match found:\n",
      "File 1: Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling\n",
      "File 2: Reprompting Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling\n",
      "\n",
      "Match found:\n",
      "File 1: Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions\n",
      "File 2: Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions\n",
      "\n",
      "Match found:\n",
      "File 1: Rationale-Augmented Ensembles in Language Models\n",
      "File 2: Rationale-Augmented Ensembles in Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Automatic Chain of Thought Prompting in Large Language Models\n",
      "File 2: Automatic Chain of Thought Prompting in Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Inference-Time Intervention: Eliciting Truthful Answers from a Language Model\n",
      "File 2: Inference-Time Intervention Eliciting Truthful Answers from a Language Model\n",
      "\n",
      "Match found:\n",
      "File 1: Large Language Models Can Self-Improve\n",
      "File 2: Large Language Models Can Self-Improve\n",
      "\n",
      "Match found:\n",
      "File 1: Self-Refine: Iterative Refinement with Self-Feedback\n",
      "File 2: Self-Refine Iterative Refinement with Self-Feedback\n",
      "\n",
      "Match found:\n",
      "File 1: Reflexion: Language Agents with Verbal Reinforcement Learning\n",
      "File 2: Reflexion language agents with verbal reinforcement learning\n",
      "\n",
      "Match found:\n",
      "File 1: PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine\n",
      "File 2: PREFER Prompt Ensemble Learning via Feedback-Reflect-Refine\n",
      "\n",
      "Match found:\n",
      "File 1: N-CRITICS: Self-Refinement of Large Language Models with Ensemble of Critics\n",
      "File 2: N-Critics Self-Refinement of Large Language Models with Ensemble of Critics\n",
      "\n",
      "Match found:\n",
      "File 1: Improving Factuality and Reasoning in Language Models through Multiagent Debate\n",
      "File 2: Improving Factuality and Reasoning in Language Models through Multiagent Debate\n",
      "\n",
      "Match found:\n",
      "File 1: Self-Consistency Improves Chain of Thought Reasoning in Language Models\n",
      "File 2: Self-Consistency Improves Chain of Thought Reasoning in Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Universal Self-Consistency for Large Language Model Generation\n",
      "File 2: Universal Self-Consistency for Large Language Model Generation\n",
      "\n",
      "Match found:\n",
      "File 1: Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models\n",
      "File 2: Found in the Middle Permutation Self-Consistency Improves Listwise Ranking in Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate\n",
      "File 2: Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate\n",
      "\n",
      "Match found:\n",
      "File 1: Boosting Language Models Reasoning with Chain-of-Knowledge Prompting\n",
      "File 2: Boosting Language Models Reasoning with Chain-of-Knowledge Prompting\n",
      "\n",
      "Match found:\n",
      "File 1: Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models\n",
      "File 2: Chain-of-Note Enhancing Robustness in Retrieval-Augmented Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Active Prompting with Chain-of-Thought for Large Language Models\n",
      "File 2: Active Prompting with Chain-of-Thought for Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves\n",
      "File 2: Rephrase and Respond Let Large Language Models Ask Better Questions for Themselves\n",
      "\n",
      "Match found:\n",
      "File 1: Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models\n",
      "File 2: Take a Step Back Evoking Reasoning via Abstraction in Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models\n",
      "File 2: Synthetic Prompting Generating Chain-of-Thought Demonstrations for Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Towards Expert-Level Medical Question Answering with Large Language Models\n",
      "File 2: Towards Expert-Level Medical Question Answering with Large Language Models\n",
      "\n",
      "Match found:\n",
      "File 1: Embodied LLM Agents Learn to Cooperate in Organized Teams\n",
      "File 2: Embodied LLM Agents Learn to Cooperate in Organized Teams\n",
      "\n",
      "Match found:\n",
      "File 1: The Consensus Game: Language Model Generation via Equilibrium Search\n",
      "File 2: The Consensus Game Language Model Generation via Equilibrium Search\n",
      "\n",
      "Match found:\n",
      "File 1: RELIC: Investigating Large Language Model Responses using Self-Consistency\n",
      "File 2: RELIC Investigating Large Language Model Responses using Self-Consistency\n",
      "\n",
      "Match found:\n",
      "File 1: Better Zero-Shot Reasoning with Self-Adaptive Prompting\n",
      "File 2: Better Zero-Shot Reasoning with Self-Adaptive Prompting\n",
      "\n",
      "Match found:\n",
      "File 1: Universal Self-Adaptive Prompting\n",
      "File 2: Universal Self-adaptive Prompting\n",
      "\n",
      "Total papers in first file: 109\n",
      "Total papers in second file: 4886\n",
      "Number of matching papers: 50\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def clean_title(title):\n",
    "    # Remove common punctuation and convert to lowercase\n",
    "    return title.lower().replace(\":\", \"\").replace(\"-\", \"\").replace(\"  \", \" \").strip()\n",
    "\n",
    "with open('annotated_papers.json', 'r') as file1, open('pdf_contents.json', 'r') as file2:\n",
    "    data1 = json.load(file1)\n",
    "    data2 = json.load(file2)\n",
    "    \n",
    "    matches = 0\n",
    "    \n",
    "    # For each paper in first file\n",
    "    for title1, paper1 in data1.items():\n",
    "        title1_clean = clean_title(title1)\n",
    "        \n",
    "        # Check if this title exists in any title from second file\n",
    "        for title2 in data2.keys():\n",
    "            title2_clean = clean_title(title2)\n",
    "            \n",
    "            # Check if titles are effectively the same\n",
    "            if title1_clean == title2_clean or \\\n",
    "               title1_clean in title2_clean and len(title1_clean) > 20:  # Length check to avoid short title false matches\n",
    "                matches += 1\n",
    "                print(f\"\\nMatch found:\")\n",
    "                print(f\"File 1: {title1}\")\n",
    "                print(f\"File 2: {title2}\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\nTotal papers in first file: {len(data1)}\")\n",
    "    print(f\"Total papers in second file: {len(data2)}\")\n",
    "    print(f\"Number of matching papers: {matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e43616a0-dd55-441f-bd89-8108bce8e47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found: ReAct: Synergizing Reasoning and Acting in Language Models\n",
      "Match found: Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n",
      "Match found: Boosted Prompt Ensembles for Large Language Models\n",
      "Match found: Ask Me Anything: A simple strategy for prompting language models\n",
      "Match found: Graph of Thoughts: Solving Elaborate Problems with Large Language Models\n",
      "Match found: More Agents Is All You Need\n",
      "Match found: ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs\n",
      "Match found: Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models\n",
      "Match found: Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems\n",
      "Match found: InferFix: End-to-End Program Repair with LLMs\n",
      "Match found: Revisit Input Perturbation Problems for LLMs: A Unified Robustness Evaluation Framework for Noisy Slot Filling Task\n",
      "Match found: FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation\n",
      "Match found: DiversiGATE: A Comprehensive Framework for Reliable Large Language Models\n",
      "Match found: Large Language Models are Zero-Shot Reasoners\n",
      "Match found: ART: Automatic multi-step reasoning and tool-use for large language models\n",
      "Match found: MathPrompter: Mathematical Reasoning using Large Language Models\n",
      "Match found: Prompting GPT-3 To Be Reliable\n",
      "Match found: Beyond Self-Consistency: Ensemble Reasoning Boosts Consistency and Accuracy of LLMs in Cancer Staging\n",
      "Match found: Chain-of-Verification Reduces Hallucination in Large Language Models\n",
      "Match found: LM vs LM: Detecting Factual Errors via Cross Examination\n",
      "Match found: REFINER: Reasoning Feedback on Intermediate Representations\n",
      "Match found: Reliable Natural Language Understanding with Large Language Models and Answer Set Programming\n",
      "Match found: Complexity-based Prompting for Multi-step Reasoning\n",
      "Match found: Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling\n",
      "Match found: Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions\n",
      "Match found: Rationale-Augmented Ensembles in Language Models\n",
      "Match found: Automatic Chain of Thought Prompting in Large Language Models\n",
      "Match found: Inference-Time Intervention: Eliciting Truthful Answers from a Language Model\n",
      "Match found: Large Language Models Can Self-Improve\n",
      "Match found: Self-Refine: Iterative Refinement with Self-Feedback\n",
      "Match found: Reflexion: Language Agents with Verbal Reinforcement Learning\n",
      "Match found: PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine\n",
      "Match found: N-CRITICS: Self-Refinement of Large Language Models with Ensemble of Critics\n",
      "Match found: Improving Factuality and Reasoning in Language Models through Multiagent Debate\n",
      "Match found: Self-Consistency Improves Chain of Thought Reasoning in Language Models\n",
      "Match found: Universal Self-Consistency for Large Language Model Generation\n",
      "Match found: Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models\n",
      "Match found: Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate\n",
      "Match found: Boosting Language Models Reasoning with Chain-of-Knowledge Prompting\n",
      "Match found: Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models\n",
      "Match found: Active Prompting with Chain-of-Thought for Large Language Models\n",
      "Match found: Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves\n",
      "Match found: Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models\n",
      "Match found: Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models\n",
      "Match found: Towards Expert-Level Medical Question Answering with Large Language Models\n",
      "Match found: Embodied LLM Agents Learn to Cooperate in Organized Teams\n",
      "Match found: The Consensus Game: Language Model Generation via Equilibrium Search\n",
      "Match found: RELIC: Investigating Large Language Model Responses using Self-Consistency\n",
      "Match found: Better Zero-Shot Reasoning with Self-Adaptive Prompting\n",
      "Match found: Universal Self-Adaptive Prompting\n",
      "\n",
      "Found 50 matching papers to analyze\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing matched papers:  18%|███▏              | 9/50 [01:05<07:29, 10.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to matched_papers_analysis_itercheckpoint_10_20250103_132347.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing matched papers:  38%|██████▍          | 19/50 [01:47<02:18,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to matched_papers_analysis_itercheckpoint_20_20250103_132430.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing matched papers:  58%|█████████▊       | 29/50 [02:32<01:31,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to matched_papers_analysis_itercheckpoint_30_20250103_132516.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing matched papers:  78%|█████████████▎   | 39/50 [03:16<00:47,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to matched_papers_analysis_itercheckpoint_40_20250103_132559.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing matched papers:  98%|████████████████▋| 49/50 [04:13<00:05,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to matched_papers_analysis_itercheckpoint_50_20250103_132658.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing matched papers: 100%|█████████████████| 50/50 [04:20<00:00,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to matched_papers_analysis_iterfinal_20250103_132700.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import openai\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "from datetime import datetime\n",
    "\n",
    "# Replace with your API key\n",
    "\n",
    "def count_tokens(text):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def truncate_text(text, max_tokens=14000):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    tokens = encoding.encode(text)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "        text = encoding.decode(tokens)\n",
    "    return text\n",
    "\n",
    "def create_prompt(paper_text):\n",
    "    return f\"\"\"Analyze this academic paper and extract two specific types of information:\n",
    "\n",
    "1. Benchmarks/Datasets used for evaluation:\n",
    "   - Include standard evaluation datasets (e.g., MNIST, ImageNet, SQuAD)\n",
    "   - Include custom datasets if they're used for evaluation\n",
    "   - Do NOT include training datasets unless they're also used for evaluation\n",
    "\n",
    "2. Base Language Models used in experiments:\n",
    "   - Include specific model architectures and variants (e.g., GPT-4, LLaMA-70B, PaLM-540B), which includes parameter size\n",
    "   - Do NOT include methods or techniques (e.g., don't include Chain-of-Thought, Self-Consistency, etc.)\n",
    "   - For custom models, specify the base model they use (e.g., if a paper introduces \"CustomBERT\", note it uses BERT as base)\n",
    "\n",
    "Format your response as a JSON object with this exact structure:\n",
    "{{\n",
    "    \"benchmarks\": [\"benchmark1\", \"benchmark2\"],\n",
    "    \"base_models\": [\"model1 (with size if specified)\", \"model2 (with size if specified)\"]\n",
    "}}\n",
    "\n",
    "Paper text:\n",
    "{paper_text}\"\"\"\n",
    "\n",
    "def analyze_paper_with_gpt(paper_text):\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            truncated_text = truncate_text(paper_text)\n",
    "            was_truncated = len(truncated_text) < len(paper_text)\n",
    "            \n",
    "            prompt = create_prompt(truncated_text)\n",
    "            token_count = count_tokens(prompt)\n",
    "            \n",
    "            if token_count > 15000:\n",
    "                print(\"Warning: Token count exceeds safe limit\")\n",
    "                return {\n",
    "                    \"benchmarks\": [],\n",
    "                    \"models\": [],\n",
    "                    \"error\": \"Token limit exceeded\"\n",
    "                }\n",
    "                \n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful graduate research assistant that analyzes academic papers and returns responses in JSON format only.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=1000,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "\n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            # Add error checking for response content\n",
    "            if not result.strip().startswith('{') or not result.strip().endswith('}'):\n",
    "                raise json.JSONDecodeError(\"Invalid JSON format\", result, 0)\n",
    "                \n",
    "            parsed_result = json.loads(result)\n",
    "            \n",
    "            if was_truncated:\n",
    "                parsed_result[\"note\"] = \"Analysis based on truncated paper text\"\n",
    "                \n",
    "            return parsed_result\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"\\nAttempt {attempt + 1}/{max_retries} - JSON parsing error: {str(e)}\")\n",
    "            print(f\"Raw response: {result}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return {\n",
    "                    \"benchmarks\": [],\n",
    "                    \"models\": [],\n",
    "                    \"error\": f\"Failed to parse GPT response as JSON after {max_retries} attempts: {str(e)}\"\n",
    "                }\n",
    "            time.sleep(2)  # Wait before retry\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nAttempt {attempt + 1}/{max_retries} - Error: {str(e)}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return {\n",
    "                    \"benchmarks\": [],\n",
    "                    \"models\": [],\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "            time.sleep(2)  # Wait before retry\n",
    "\n",
    "def save_progress(analysis_results, base_filename, iteration):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{base_filename}_iter{iteration}_{timestamp}.json\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(analysis_results, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"\\nProgress saved to {filename}\")\n",
    "\n",
    "def clean_title(title):\n",
    "    return title.lower().replace(\":\", \"\").replace(\"-\", \"\").replace(\"  \", \" \").strip()\n",
    "\n",
    "def main():\n",
    "    # First, load and find matching papers\n",
    "    with open('annotated_papers.json', 'r') as file1, open('pdf_contents.json', 'r') as file2:\n",
    "        data1 = json.load(file1)\n",
    "        pdf_contents = json.load(file2)\n",
    "        \n",
    "        matched_papers = {}\n",
    "        \n",
    "        # Find matching papers\n",
    "        for title1, paper1 in data1.items():\n",
    "            title1_clean = clean_title(title1)\n",
    "            \n",
    "            for paper_id, paper_data in pdf_contents.items():\n",
    "                title2 = paper_data['filename']\n",
    "                title2_clean = clean_title(title2)\n",
    "                \n",
    "                if title1_clean == title2_clean or \\\n",
    "                   (title1_clean in title2_clean and len(title1_clean) > 20):\n",
    "                    matched_papers[paper_id] = paper_data\n",
    "                    print(f\"Match found: {title1}\")\n",
    "                    break\n",
    "\n",
    "    print(f\"\\nFound {len(matched_papers)} matching papers to analyze\")\n",
    "    \n",
    "    # Now process only the matched papers\n",
    "    analysis_results = {}\n",
    "    save_interval = 10  # Reduced save interval since we have fewer papers\n",
    "    \n",
    "    # Convert items to list for tqdm\n",
    "    items = list(matched_papers.items())\n",
    "    \n",
    "    for i, (paper_id, paper_data) in enumerate(tqdm(items, desc=\"Analyzing matched papers\")):\n",
    "        paper_text = paper_data.get('text', '')\n",
    "        \n",
    "        if not paper_text:\n",
    "            print(f\"\\nSkipping {paper_id} - no text content\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            result = analyze_paper_with_gpt(paper_text)\n",
    "            analysis_results[paper_id] = {\n",
    "                \"filename\": paper_data['filename'],\n",
    "                \"analysis\": result\n",
    "            }\n",
    "            \n",
    "            # Save progress more frequently since we're processing fewer papers\n",
    "            if (i + 1) % save_interval == 0:\n",
    "                save_progress(analysis_results, \"matched_papers_analysis\", f\"checkpoint_{i+1}\")\n",
    "                \n",
    "            time.sleep(2)  # Rate limiting\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError analyzing {paper_id}: {str(e)}\")\n",
    "            analysis_results[paper_id] = {\n",
    "                \"filename\": paper_data['filename'],\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    # Save final results\n",
    "    save_progress(analysis_results, \"matched_papers_analysis\", \"final\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a4e4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 4886 papers to analyze\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   0%|                     | 19/4886 [01:22<6:03:38,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_20_20250107_140550.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   1%|                     | 25/4886 [01:55<6:22:50,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   1%|▏                    | 39/4886 [03:06<7:45:07,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_40_20250107_140727.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   1%|▎                    | 59/4886 [04:42<6:00:59,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_60_20250107_140904.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   2%|▎                    | 79/4886 [06:48<9:12:03,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_80_20250107_141110.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   2%|▍                    | 99/4886 [08:16<5:38:43,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_100_20250107_141237.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   2%|▍                   | 119/4886 [10:10<5:45:55,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_120_20250107_141432.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   3%|▌                   | 139/4886 [11:40<5:31:01,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_140_20250107_141602.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   3%|▋                   | 159/4886 [13:22<5:53:46,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_160_20250107_141745.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   4%|▋                   | 179/4886 [15:27<8:34:39,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_180_20250107_141949.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   4%|▊                   | 199/4886 [17:10<6:00:34,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_200_20250107_142133.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   4%|▉                   | 219/4886 [19:05<8:33:10,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_220_20250107_142327.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   5%|▉                   | 239/4886 [20:40<6:26:20,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_240_20250107_142502.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   5%|█                   | 259/4886 [22:21<6:42:11,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_260_20250107_142643.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   6%|█▏                  | 279/4886 [23:54<7:07:05,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_280_20250107_142817.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   6%|█▏                  | 299/4886 [25:30<7:27:45,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_300_20250107_142952.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   7%|█▎                  | 319/4886 [27:11<6:49:04,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_320_20250107_143133.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   7%|█▍                  | 339/4886 [29:11<6:34:46,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_340_20250107_143336.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   7%|█▍                  | 359/4886 [30:48<5:15:36,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_360_20250107_143509.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   8%|█▌                  | 379/4886 [32:25<6:58:32,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_380_20250107_143647.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   8%|█▋                  | 399/4886 [33:47<5:31:18,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_400_20250107_143810.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   9%|█▋                  | 419/4886 [35:25<5:47:17,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_420_20250107_143948.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   9%|█▊                  | 439/4886 [37:17<9:08:40,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_440_20250107_144139.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:   9%|█▉                  | 459/4886 [38:45<5:21:46,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_460_20250107_144309.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  10%|█▉                  | 479/4886 [40:41<7:09:51,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_480_20250107_144503.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  10%|██                  | 499/4886 [42:25<6:53:40,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_500_20250107_144648.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  11%|██                  | 519/4886 [43:59<5:28:40,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_520_20250107_144821.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  11%|██▏                 | 539/4886 [45:29<5:08:45,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_540_20250107_144951.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  11%|██▎                 | 559/4886 [47:04<5:54:32,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_560_20250107_145126.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  12%|██▎                 | 579/4886 [48:49<4:49:02,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_580_20250107_145314.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  12%|██▍                 | 599/4886 [50:25<4:43:43,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_600_20250107_145447.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  13%|██▌                 | 619/4886 [52:07<6:38:30,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_620_20250107_145629.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  13%|██▌                 | 639/4886 [53:42<5:55:24,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_640_20250107_145803.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  13%|██▋                 | 659/4886 [55:16<5:17:01,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_660_20250107_145938.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  14%|██▊                 | 679/4886 [56:55<4:50:18,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_680_20250107_150117.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  14%|██▊                 | 693/4886 [57:55<5:09:01,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  14%|██▊                 | 699/4886 [58:30<7:42:10,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_700_20250107_150302.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  15%|██▋               | 719/4886 [1:00:22<6:23:34,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_720_20250107_150447.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Analyzing papers:  15%|██▋               | 720/4886 [1:00:29<7:09:10,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  15%|██▋               | 739/4886 [1:02:06<5:52:56,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_740_20250107_150627.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  16%|██▊               | 759/4886 [1:03:45<5:26:31,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_760_20250107_150808.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  16%|██▊               | 779/4886 [1:05:32<5:46:35,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_780_20250107_150953.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  16%|██▉               | 799/4886 [1:07:19<5:50:48,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_800_20250107_151141.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  17%|███               | 819/4886 [1:09:16<5:39:12,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_820_20250107_151338.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  17%|███               | 827/4886 [1:09:53<5:39:50,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endofprompt|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endofprompt|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endofprompt|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endofprompt|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endofprompt|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endofprompt|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endofprompt|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endofprompt|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endofprompt|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  17%|███               | 839/4886 [1:11:20<7:19:12,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_840_20250107_151541.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  18%|███▏              | 859/4886 [1:13:02<4:43:06,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_860_20250107_151725.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  18%|███▏              | 879/4886 [1:14:57<5:25:30,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_880_20250107_151920.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  18%|███▎              | 899/4886 [1:16:40<6:02:19,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_900_20250107_152103.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  19%|███▍              | 919/4886 [1:18:20<4:18:20,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_920_20250107_152242.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  19%|███▍              | 939/4886 [1:19:46<4:34:12,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_940_20250107_152408.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  20%|███▌              | 959/4886 [1:21:17<5:31:24,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_960_20250107_152539.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  20%|███▌              | 979/4886 [1:22:50<4:20:58,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_980_20250107_152711.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  20%|███▋              | 999/4886 [1:24:22<4:31:51,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1000_20250107_152844.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  21%|███▌             | 1019/4886 [1:26:20<8:15:56,  7.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1020_20250107_153043.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  21%|███▌             | 1039/4886 [1:28:01<5:29:46,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1040_20250107_153227.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  22%|███▋             | 1059/4886 [1:29:36<4:49:08,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1060_20250107_153357.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  22%|███▊             | 1079/4886 [1:31:20<6:45:03,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1080_20250107_153542.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  22%|███▊             | 1099/4886 [1:33:10<5:07:55,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1100_20250107_153733.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  23%|███▉             | 1119/4886 [1:34:49<5:37:13,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1120_20250107_153916.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  23%|███▉             | 1139/4886 [1:36:25<4:17:48,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1140_20250107_154047.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  24%|████             | 1159/4886 [1:37:53<4:57:22,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1160_20250107_154215.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  24%|████             | 1179/4886 [1:39:28<4:45:59,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1180_20250107_154350.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  25%|████▏            | 1199/4886 [1:40:56<4:20:20,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1200_20250107_154518.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  25%|████▏            | 1219/4886 [1:42:28<4:39:55,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1220_20250107_154652.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  25%|████▎            | 1239/4886 [1:44:21<6:02:16,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1240_20250107_154843.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  26%|████▍            | 1259/4886 [1:45:48<4:15:06,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1260_20250107_155009.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  26%|████▍            | 1279/4886 [1:47:12<4:30:26,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1280_20250107_155140.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  27%|████▌            | 1299/4886 [1:48:49<4:13:25,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1300_20250107_155311.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  27%|████▌            | 1319/4886 [1:50:30<5:37:57,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1320_20250107_155451.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  27%|████▋            | 1335/4886 [1:51:40<4:40:03,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: 'NoneType' object has no attribute 'strip'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  27%|████▋            | 1339/4886 [1:51:59<4:22:09,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1340_20250107_155621.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  28%|████▋            | 1359/4886 [1:53:24<4:22:50,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1360_20250107_155746.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  28%|████▊            | 1379/4886 [1:54:57<5:22:31,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1380_20250107_155919.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  29%|████▊            | 1399/4886 [1:56:30<5:00:15,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1400_20250107_160052.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  29%|████▉            | 1419/4886 [1:58:05<4:39:47,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1420_20250107_160226.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  29%|█████            | 1439/4886 [1:59:43<3:53:48,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1440_20250107_160405.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  30%|█████            | 1459/4886 [2:01:06<3:36:42,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1460_20250107_160530.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  30%|█████▏           | 1479/4886 [2:02:28<3:56:47,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1480_20250107_160652.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  31%|█████▏           | 1499/4886 [2:03:57<4:06:37,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1500_20250107_160819.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  31%|█████▎           | 1519/4886 [2:05:20<3:44:39,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1520_20250107_160943.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  31%|█████▎           | 1539/4886 [2:06:48<3:56:02,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1540_20250107_161113.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  32%|█████▍           | 1559/4886 [2:08:23<3:50:13,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1560_20250107_161244.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  32%|█████▍           | 1579/4886 [2:09:47<3:31:48,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1580_20250107_161409.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  33%|█████▌           | 1599/4886 [2:11:13<3:34:24,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1600_20250107_161535.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  33%|█████▋           | 1619/4886 [2:12:58<4:57:25,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1620_20250107_161719.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  34%|█████▋           | 1639/4886 [2:14:33<4:57:05,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1640_20250107_161855.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  34%|█████▊           | 1659/4886 [2:16:14<6:47:34,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1660_20250107_162036.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  34%|█████▊           | 1679/4886 [2:17:44<3:58:26,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1680_20250107_162206.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  35%|█████▉           | 1699/4886 [2:19:11<3:56:04,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1700_20250107_162332.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  35%|█████▉           | 1719/4886 [2:20:36<3:24:39,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1720_20250107_162459.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  36%|██████           | 1739/4886 [2:22:05<3:49:57,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1740_20250107_162627.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  36%|██████           | 1759/4886 [2:23:25<3:43:32,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1760_20250107_162746.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  36%|██████▏          | 1779/4886 [2:24:53<3:49:18,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1780_20250107_162914.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  37%|██████▎          | 1799/4886 [2:26:14<3:21:07,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1800_20250107_163036.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  37%|██████▎          | 1819/4886 [2:27:45<3:09:53,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1820_20250107_163208.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  38%|██████▍          | 1839/4886 [2:29:16<3:46:47,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1840_20250107_163339.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  38%|██████▍          | 1859/4886 [2:30:36<3:09:24,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1860_20250107_163457.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  38%|██████▌          | 1879/4886 [2:32:05<4:10:41,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1880_20250107_163626.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  39%|██████▌          | 1899/4886 [2:33:38<4:35:16,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1900_20250107_163759.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  39%|██████▋          | 1919/4886 [2:35:10<3:20:43,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1920_20250107_163933.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  40%|██████▋          | 1939/4886 [2:36:40<3:35:37,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1940_20250107_164106.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  40%|██████▊          | 1959/4886 [2:38:20<4:19:11,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1960_20250107_164248.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  41%|██████▉          | 1979/4886 [2:39:54<4:08:42,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_1980_20250107_164416.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  41%|██████▉          | 1999/4886 [2:41:20<3:06:10,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2000_20250107_164542.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  41%|███████          | 2019/4886 [2:42:47<4:00:53,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2020_20250107_164708.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  42%|███████          | 2039/4886 [2:44:18<3:54:11,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2040_20250107_164839.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  42%|███████▏         | 2059/4886 [2:45:54<3:33:36,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2060_20250107_165015.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  43%|███████▏         | 2079/4886 [2:47:31<3:18:12,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2080_20250107_165153.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  43%|███████▎         | 2099/4886 [2:49:05<3:24:42,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2100_20250107_165326.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  43%|███████▎         | 2119/4886 [2:50:29<3:00:08,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2120_20250107_165453.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  44%|███████▍         | 2139/4886 [2:54:12<3:48:32,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2140_20250107_165834.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  44%|███████▌         | 2159/4886 [2:55:36<3:13:52,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2160_20250107_165958.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  45%|███████▌         | 2179/4886 [2:57:11<3:06:10,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2180_20250107_170134.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  45%|███████▋         | 2199/4886 [2:58:49<3:42:19,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2200_20250107_170314.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  45%|███████▋         | 2219/4886 [3:00:15<2:50:28,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2220_20250107_170437.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  46%|███████▊         | 2239/4886 [3:01:39<3:24:10,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2240_20250107_170601.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  46%|███████▊         | 2259/4886 [3:03:04<3:20:19,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2260_20250107_170728.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  47%|███████▉         | 2279/4886 [3:04:34<2:51:20,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2280_20250107_170857.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  47%|███████▉         | 2299/4886 [3:05:50<2:33:11,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2300_20250107_171012.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  47%|████████         | 2319/4886 [3:07:16<3:17:05,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2320_20250107_171139.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  48%|████████▏        | 2339/4886 [3:08:38<3:06:33,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2340_20250107_171259.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  48%|████████▏        | 2356/4886 [3:09:49<2:41:29,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  48%|████████▏        | 2359/4886 [3:10:04<3:07:17,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2360_20250107_171426.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  49%|████████▎        | 2379/4886 [3:11:32<2:55:24,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2380_20250107_171553.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  49%|████████▎        | 2399/4886 [3:12:59<3:05:40,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2400_20250107_171720.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  50%|████████▍        | 2419/4886 [3:14:25<2:48:54,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2420_20250107_171847.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  50%|████████▍        | 2439/4886 [3:15:49<2:50:03,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2440_20250107_172010.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  50%|████████▌        | 2459/4886 [3:17:21<3:37:32,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2460_20250107_172143.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  51%|████████▋        | 2479/4886 [3:18:48<2:41:12,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2480_20250107_172310.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  51%|████████▋        | 2499/4886 [3:20:20<2:57:32,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2500_20250107_172442.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  52%|████████▊        | 2519/4886 [3:21:52<2:40:01,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2520_20250107_172613.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  52%|████████▊        | 2539/4886 [3:23:27<2:53:47,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2540_20250107_172750.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  52%|████████▉        | 2559/4886 [3:24:54<2:58:11,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2560_20250107_172915.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  53%|████████▉        | 2579/4886 [3:26:24<2:49:48,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2580_20250107_173045.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  53%|█████████        | 2599/4886 [3:27:46<2:25:44,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2600_20250107_173209.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  54%|█████████        | 2619/4886 [3:29:20<2:57:10,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2620_20250107_173342.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  54%|█████████▏       | 2639/4886 [3:30:41<2:35:35,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2640_20250107_173504.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  54%|█████████▎       | 2659/4886 [3:32:34<6:10:49,  9.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2660_20250107_173656.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  55%|█████████▎       | 2679/4886 [3:34:07<2:48:27,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2680_20250107_173830.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  55%|█████████▍       | 2699/4886 [3:35:32<2:26:04,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2700_20250107_173953.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  56%|█████████▍       | 2719/4886 [3:37:07<4:58:53,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2720_20250107_174134.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  56%|█████████▌       | 2739/4886 [3:38:49<2:40:50,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2740_20250107_174310.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  56%|█████████▌       | 2759/4886 [3:40:35<3:37:31,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2760_20250107_174458.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  57%|█████████▋       | 2779/4886 [3:41:50<2:04:30,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2780_20250107_174612.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  57%|█████████▋       | 2799/4886 [3:43:03<2:10:40,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2800_20250107_174725.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  58%|█████████▊       | 2819/4886 [3:44:13<1:54:42,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2820_20250107_174835.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  58%|█████████▉       | 2839/4886 [3:45:30<2:03:01,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2840_20250107_174951.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  59%|█████████▉       | 2859/4886 [3:46:47<2:27:24,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2860_20250107_175109.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  59%|██████████       | 2879/4886 [3:47:59<2:10:19,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2880_20250107_175220.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  59%|██████████       | 2899/4886 [3:49:12<1:50:05,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2900_20250107_175333.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  60%|██████████▏      | 2919/4886 [3:50:27<2:15:00,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2920_20250107_175448.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  60%|██████████▏      | 2925/4886 [3:50:50<2:05:10,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  60%|██████████▏      | 2939/4886 [3:51:42<1:49:05,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2940_20250107_175603.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  61%|██████████▎      | 2959/4886 [3:52:56<2:00:06,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2960_20250107_175717.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  61%|██████████▎      | 2979/4886 [3:54:10<1:54:31,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_2980_20250107_175832.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  61%|██████████▍      | 2999/4886 [3:55:28<1:57:07,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3000_20250107_175949.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  62%|██████████▌      | 3019/4886 [3:56:44<2:02:48,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3020_20250107_180105.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  62%|██████████▌      | 3039/4886 [3:57:57<1:48:49,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3040_20250107_180219.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  62%|██████████▌      | 3048/4886 [3:58:35<1:59:07,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  63%|██████████▋      | 3059/4886 [3:59:20<1:52:11,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3060_20250107_180340.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  63%|██████████▋      | 3079/4886 [4:00:32<1:50:50,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3080_20250107_180453.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  63%|██████████▊      | 3099/4886 [4:01:45<1:55:31,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3100_20250107_180606.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  64%|██████████▊      | 3119/4886 [4:02:54<1:44:55,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3120_20250107_180715.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  64%|██████████▉      | 3139/4886 [4:04:07<1:56:36,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3140_20250107_180828.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  65%|██████████▉      | 3159/4886 [4:05:25<1:46:54,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3160_20250107_180947.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  65%|███████████      | 3179/4886 [4:06:41<1:47:07,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3180_20250107_181102.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  65%|███████████▏     | 3199/4886 [4:08:03<1:46:50,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3200_20250107_181224.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  66%|███████████▏     | 3219/4886 [4:09:17<1:36:29,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3220_20250107_181339.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  66%|███████████▎     | 3239/4886 [4:10:35<1:51:16,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3240_20250107_181456.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  67%|███████████▎     | 3259/4886 [4:11:53<1:40:24,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3260_20250107_181615.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  67%|███████████▍     | 3279/4886 [4:13:02<1:30:00,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3280_20250107_181723.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  68%|███████████▍     | 3299/4886 [4:14:19<1:32:05,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3300_20250107_181845.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  68%|███████████▌     | 3319/4886 [4:15:46<1:36:52,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3320_20250107_182008.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  68%|███████████▌     | 3339/4886 [4:17:00<1:46:11,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3340_20250107_182126.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  69%|███████████▋     | 3359/4886 [4:18:16<1:31:02,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3360_20250107_182239.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  69%|███████████▊     | 3379/4886 [4:19:29<1:30:32,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3380_20250107_182350.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  70%|███████████▊     | 3399/4886 [4:20:46<2:01:54,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3400_20250107_182508.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  70%|███████████▉     | 3419/4886 [4:21:57<1:33:42,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3420_20250107_182618.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  70%|███████████▉     | 3436/4886 [4:22:57<1:27:45,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  70%|███████████▉     | 3439/4886 [4:23:11<1:41:49,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3440_20250107_182732.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  71%|████████████     | 3459/4886 [4:24:36<1:33:36,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3460_20250107_182857.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  71%|████████████     | 3479/4886 [4:25:50<1:27:04,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3480_20250107_183011.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  72%|████████████▏    | 3499/4886 [4:27:02<1:25:11,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3500_20250107_183123.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  72%|████████████▏    | 3519/4886 [4:28:24<2:09:40,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3520_20250107_183245.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  72%|████████████▎    | 3539/4886 [4:29:41<1:22:18,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3540_20250107_183403.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  73%|████████████▍    | 3559/4886 [4:31:00<1:27:45,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3560_20250107_183522.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  73%|████████████▍    | 3579/4886 [4:32:09<1:16:07,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3580_20250107_183631.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  74%|████████████▌    | 3599/4886 [4:33:20<1:16:53,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3600_20250107_183741.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  74%|████████████▌    | 3619/4886 [4:34:33<1:17:42,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3620_20250107_183854.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  74%|████████████▋    | 3639/4886 [4:35:47<1:20:22,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3640_20250107_184009.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  75%|████████████▋    | 3659/4886 [4:37:05<1:12:36,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3660_20250107_184126.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  75%|████████████▋    | 3664/4886 [4:37:22<1:10:36,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  75%|████████████▊    | 3679/4886 [4:38:22<1:20:42,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3680_20250107_184243.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  76%|████████████▊    | 3699/4886 [4:39:39<1:06:32,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3700_20250107_184401.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  76%|████████████▉    | 3719/4886 [4:40:56<1:18:18,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3720_20250107_184518.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  77%|█████████████    | 3739/4886 [4:42:10<1:12:26,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3740_20250107_184634.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  77%|█████████████    | 3759/4886 [4:43:28<1:03:52,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3760_20250107_184750.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  77%|█████████████▏   | 3779/4886 [4:44:44<1:09:49,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3780_20250107_184905.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  78%|█████████████▏   | 3799/4886 [4:46:07<1:42:56,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3800_20250107_185029.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  78%|█████████████▎   | 3819/4886 [4:47:25<1:19:24,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3820_20250107_185146.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  79%|█████████████▎   | 3839/4886 [4:48:47<1:16:36,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3840_20250107_185309.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  79%|█████████████▍   | 3859/4886 [4:50:02<1:04:06,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3860_20250107_185424.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  79%|█████████████▍   | 3879/4886 [4:51:18<1:06:20,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3880_20250107_185539.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  80%|█████████████▌   | 3899/4886 [4:52:33<1:01:02,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3900_20250107_185654.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  80%|█████████████▋   | 3919/4886 [4:53:52<1:02:49,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3920_20250107_185818.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  81%|███████████████▎   | 3939/4886 [4:55:11<57:23,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3940_20250107_185933.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  81%|███████████████▍   | 3959/4886 [4:56:28<57:34,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3960_20250107_190049.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  81%|█████████████▊   | 3979/4886 [4:57:47<1:09:44,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_3980_20250107_190210.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  82%|█████████████▉   | 3999/4886 [4:59:24<1:05:01,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4000_20250107_190346.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  82%|███████████████▋   | 4019/4886 [5:00:46<58:02,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4020_20250107_190508.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  83%|███████████████▋   | 4039/4886 [5:02:15<58:03,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4040_20250107_190637.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  83%|███████████████▊   | 4059/4886 [5:03:36<50:11,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4060_20250107_190758.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  83%|███████████████▊   | 4079/4886 [5:04:48<50:03,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4080_20250107_190909.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  84%|██████████████▎  | 4099/4886 [5:06:13<1:06:44,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4100_20250107_191034.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  84%|████████████████   | 4119/4886 [5:07:36<58:50,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4120_20250107_191158.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  85%|████████████████   | 4139/4886 [5:08:57<46:25,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4140_20250107_191318.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  85%|████████████████▏  | 4159/4886 [5:10:10<44:14,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4160_20250107_191432.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  86%|████████████████▎  | 4179/4886 [5:11:27<45:19,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4180_20250107_191548.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  86%|████████████████▎  | 4199/4886 [5:12:48<47:42,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4200_20250107_191709.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  86%|████████████████▍  | 4219/4886 [5:14:04<44:28,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4220_20250107_191825.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  87%|████████████████▍  | 4227/4886 [5:14:33<40:26,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempt 1/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 2/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "\n",
      "Attempt 3/3 - Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  87%|████████████████▍  | 4239/4886 [5:15:23<45:24,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4240_20250107_191945.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  87%|████████████████▌  | 4259/4886 [5:16:45<45:16,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4260_20250107_192109.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  88%|████████████████▋  | 4279/4886 [5:18:02<42:11,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4280_20250107_192223.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  88%|████████████████▋  | 4299/4886 [5:19:36<41:05,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4300_20250107_192357.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  88%|████████████████▊  | 4319/4886 [5:20:55<35:08,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4320_20250107_192517.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  89%|████████████████▊  | 4339/4886 [5:22:36<47:27,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4340_20250107_192657.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  89%|████████████████▉  | 4359/4886 [5:24:06<38:06,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4360_20250107_192828.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  90%|█████████████████  | 4379/4886 [5:25:46<33:29,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4380_20250107_193007.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  90%|█████████████████  | 4399/4886 [5:27:07<34:10,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4400_20250107_193128.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  90%|█████████████████▏ | 4419/4886 [5:28:36<37:23,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4420_20250107_193258.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  91%|█████████████████▎ | 4439/4886 [5:30:09<28:38,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4440_20250107_193431.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  91%|█████████████████▎ | 4459/4886 [5:31:27<26:57,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4460_20250107_193549.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  92%|█████████████████▍ | 4479/4886 [5:33:04<32:49,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4480_20250107_193726.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  92%|█████████████████▍ | 4499/4886 [5:34:37<26:41,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4500_20250107_193859.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  92%|█████████████████▌ | 4519/4886 [5:36:00<27:03,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4520_20250107_194022.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  93%|█████████████████▋ | 4539/4886 [5:37:25<23:55,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4540_20250107_194146.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  93%|█████████████████▋ | 4559/4886 [5:38:44<19:29,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4560_20250107_194308.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  94%|█████████████████▊ | 4579/4886 [5:40:08<19:33,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4580_20250107_194429.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  94%|█████████████████▉ | 4599/4886 [5:41:53<36:55,  7.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4600_20250107_194615.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  95%|█████████████████▉ | 4619/4886 [5:43:26<18:27,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4620_20250107_194748.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  95%|██████████████████ | 4639/4886 [5:45:06<22:13,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4640_20250107_194928.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  95%|██████████████████ | 4659/4886 [5:46:37<17:52,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4660_20250107_195059.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  96%|██████████████████▏| 4679/4886 [5:48:01<13:59,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4680_20250107_195224.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  96%|██████████████████▎| 4699/4886 [5:49:28<12:54,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4700_20250107_195350.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  97%|██████████████████▎| 4719/4886 [5:50:48<11:16,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4720_20250107_195510.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  97%|██████████████████▍| 4739/4886 [5:52:10<09:36,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4740_20250107_195632.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  97%|██████████████████▌| 4759/4886 [5:53:41<10:45,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4760_20250107_195802.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  98%|██████████████████▌| 4779/4886 [5:55:09<09:08,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4780_20250107_195930.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  98%|██████████████████▋| 4799/4886 [5:56:33<05:29,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4800_20250107_200055.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  99%|██████████████████▋| 4819/4886 [5:58:20<07:04,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4820_20250107_200242.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  99%|██████████████████▊| 4839/4886 [5:59:56<03:38,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4840_20250107_200417.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers:  99%|██████████████████▉| 4859/4886 [6:01:42<02:15,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4860_20250107_200604.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers: 100%|██████████████████▉| 4879/4886 [6:03:11<00:30,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_itercheckpoint_4880_20250107_200733.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing papers: 100%|███████████████████| 4886/4886 [6:03:41<00:00,  4.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress saved to papers_analysis_iterfinal_20250107_200800.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import openai\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Disable OpenAI logging\n",
    "logging.getLogger(\"openai\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "# Replace with your API key\n",
    "\n",
    "def count_tokens(text):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def truncate_text(text, max_tokens=14000):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    tokens = encoding.encode(text)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "        text = encoding.decode(tokens)\n",
    "    return text\n",
    "\n",
    "def create_prompt(paper_text):\n",
    "    return f\"\"\"Analyze this academic paper and extract two specific types of information:\n",
    "\n",
    "1. Benchmarks/Datasets used for evaluation:\n",
    "   - Include standard evaluation datasets (e.g., MNIST, ImageNet, SQuAD)\n",
    "   - Include custom datasets if they're used for evaluation\n",
    "   - Do NOT include training datasets unless they're also used for evaluation\n",
    "\n",
    "2. Base Language Models used in experiments:\n",
    "   - Include specific model architectures and variants (e.g., GPT-4, LLaMA-70B, PaLM-540B), which includes parameter size\n",
    "   - Do NOT include methods or techniques (e.g., don't include Chain-of-Thought, Self-Consistency, etc.)\n",
    "   - For custom models, specify the base model they use (e.g., if a paper introduces \"CustomBERT\", note it uses BERT as base)\n",
    "\n",
    "Format your response as a JSON object with this exact structure:\n",
    "{{\n",
    "    \"benchmarks\": [\"benchmark1\", \"benchmark2\"],\n",
    "    \"base_models\": [\"model1 (with size if specified)\", \"model2 (with size if specified)\"]\n",
    "}}\n",
    "\n",
    "Paper text:\n",
    "{paper_text}\"\"\"\n",
    "\n",
    "def analyze_paper_with_gpt(paper_text):\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            truncated_text = truncate_text(paper_text)\n",
    "            was_truncated = len(truncated_text) < len(paper_text)\n",
    "            \n",
    "            prompt = create_prompt(truncated_text)\n",
    "            token_count = count_tokens(prompt)\n",
    "            \n",
    "            if token_count > 15000:\n",
    "                print(\"Warning: Token count exceeds safe limit\")\n",
    "                return {\n",
    "                    \"benchmarks\": [],\n",
    "                    \"models\": [],\n",
    "                    \"error\": \"Token limit exceeded\"\n",
    "                }\n",
    "                \n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful graduate research assistant that analyzes academic papers and returns responses in JSON format only.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=1000,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "\n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            # Add error checking for response content\n",
    "            if not result.strip().startswith('{') or not result.strip().endswith('}'):\n",
    "                raise json.JSONDecodeError(\"Invalid JSON format\", result, 0)\n",
    "                \n",
    "            parsed_result = json.loads(result)\n",
    "            \n",
    "            if was_truncated:\n",
    "                parsed_result[\"note\"] = \"Analysis based on truncated paper text\"\n",
    "                \n",
    "            return parsed_result\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"\\nAttempt {attempt + 1}/{max_retries} - JSON parsing error: {str(e)}\")\n",
    "            print(f\"Raw response: {result}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return {\n",
    "                    \"benchmarks\": [],\n",
    "                    \"models\": [],\n",
    "                    \"error\": f\"Failed to parse GPT response as JSON after {max_retries} attempts: {str(e)}\"\n",
    "                }\n",
    "            time.sleep(2)  # Wait before retry\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nAttempt {attempt + 1}/{max_retries} - Error: {str(e)}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return {\n",
    "                    \"benchmarks\": [],\n",
    "                    \"models\": [],\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "            time.sleep(2)  # Wait before retry\n",
    "\n",
    "def save_progress(analysis_results, base_filename, iteration):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{base_filename}_iter{iteration}_{timestamp}.json\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(analysis_results, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"\\nProgress saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    # Load all papers from pdf_contents\n",
    "    with open('pdf_contents.json', 'r') as file:\n",
    "        pdf_contents = json.load(file)\n",
    "\n",
    "    print(f\"\\nFound {len(pdf_contents)} papers to analyze\")\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    analysis_results = {}\n",
    "    save_interval = 20  # Save every 20 papers\n",
    "    \n",
    "    # Convert items to list for tqdm\n",
    "    items = list(pdf_contents.items())\n",
    "    \n",
    "    for i, (paper_id, paper_data) in enumerate(tqdm(items, desc=\"Analyzing papers\")):\n",
    "        paper_text = paper_data.get('text', '')\n",
    "        \n",
    "        if not paper_text:\n",
    "            print(f\"\\nSkipping {paper_id} - no text content\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            result = analyze_paper_with_gpt(paper_text)\n",
    "            analysis_results[paper_id] = {\n",
    "                \"filename\": paper_data['filename'],\n",
    "                \"analysis\": result\n",
    "            }\n",
    "            \n",
    "            # Save progress periodically\n",
    "            if (i + 1) % save_interval == 0:\n",
    "                save_progress(analysis_results, \"papers_analysis\", f\"checkpoint_{i+1}\")\n",
    "                \n",
    "            time.sleep(2)  # Rate limiting\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError analyzing {paper_id}: {str(e)}\")\n",
    "            analysis_results[paper_id] = {\n",
    "                \"filename\": paper_data['filename'],\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    # Save final results\n",
    "    save_progress(analysis_results, \"papers_analysis\", \"final\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db34b881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Statistics ===\n",
      "Total unique models (after normalization): 4809\n",
      "Total model mentions: 16665\n",
      "\n",
      "Top 20 most frequently mentioned models:\n",
      "        model  frequency\n",
      "        gpt-4       1579\n",
      "gpt-3.5-turbo        743\n",
      "      gpt-3.5        530\n",
      "       gpt-4o        251\n",
      "        gpt-3        250\n",
      "         bert        198\n",
      "   mistral-7b        183\n",
      "   llama-2-7b        165\n",
      "     llama-7b        140\n",
      "        llama        140\n",
      "    llama2-7b        134\n",
      "       gpt-4v        120\n",
      "      llama-2        109\n",
      "      roberta        108\n",
      "  llama-2-13b        102\n",
      " gpt-3-(175b)         98\n",
      "  llama-2-70b         95\n",
      "        gpt-2         90\n",
      "   vicuna-13b         86\n",
      "           t5         78\n",
      "\n",
      "=== Benchmark Statistics ===\n",
      "Total unique benchmarks: 7635\n",
      "Total benchmark mentions: 15013\n",
      "\n",
      "Top 20 most frequently mentioned benchmarks:\n",
      "    benchmark  frequency\n",
      "        gsm8k        424\n",
      "         math        175\n",
      "         mmlu        171\n",
      "        svamp        139\n",
      "   strategyqa        126\n",
      "     hotpotqa        120\n",
      "    humaneval        120\n",
      "   truthfulqa         81\n",
      "commonsenseqa         78\n",
      "    hellaswag         71\n",
      "     triviaqa         64\n",
      "         mbpp         64\n",
      "         aqua         64\n",
      "   multiarith         61\n",
      "   winogrande         57\n",
      "        boolq         56\n",
      "         piqa         53\n",
      "   openbookqa         50\n",
      "        asdiv         46\n",
      "        squad         45\n",
      "\n",
      "Detailed frequency data has been saved to:\n",
      "- model_frequencies_normalized.csv\n",
      "- benchmark_frequencies_normalized.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from typing import Dict, Any\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def normalize_model_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes model names to handle different variations.\n",
    "    \n",
    "    Args:\n",
    "        name: Original model name\n",
    "        \n",
    "    Returns:\n",
    "        Normalized model name\n",
    "    \"\"\"\n",
    "    # Remove quotation marks and extra spaces\n",
    "    name = name.strip('\"').strip()\n",
    "    \n",
    "    # Convert to lowercase for initial processing\n",
    "    normalized = name.lower()\n",
    "    \n",
    "    # Remove extra spaces between parts\n",
    "    normalized = re.sub(r'\\s+', ' ', normalized)\n",
    "    \n",
    "    # Standardize common variations while preserving distinct models\n",
    "    replacements = {\n",
    "        'gpt-3.5-turbo': ['gpt-3.5 turbo', 'gpt3.5-turbo', 'gpt3.5 turbo', 'gpt-3.5-t', 'chatgpt', 'chat-gpt', 'chat gpt'],  # ChatGPT uses GPT-3.5-turbo\n",
    "        'gpt-3.5': ['gpt3.5', 'gpt 3.5'],  # Keep base GPT-3.5 separate\n",
    "        'gpt-4': ['gpt4', 'gpt 4', 'gpt-4-turbo'],  # Group GPT-4 variations\n",
    "        'llama-2': ['llama2', 'llama-2', 'llama 2'],  # Standardize LLaMA 2 naming\n",
    "        'llama': ['llama1', 'llama 1'],  # Keep original LLaMA separate\n",
    "        'roberta': ['roberta-base', 'roberta base'],\n",
    "        'bert': ['bert-base', 'bert base'],\n",
    "    }\n",
    "    \n",
    "    # Apply replacements\n",
    "    for standard, variants in replacements.items():\n",
    "        if normalized in variants or normalized == standard:\n",
    "            return standard\n",
    "            \n",
    "    # Standardize separators\n",
    "    normalized = re.sub(r'[-_\\s]+', '-', normalized)\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def analyze_paper_data(data: Dict[str, Any]) -> tuple[Counter, Counter]:\n",
    "    \"\"\"\n",
    "    Recursively analyzes paper data to count unique models and benchmarks.\n",
    "    \"\"\"\n",
    "    model_counter = Counter()\n",
    "    benchmark_counter = Counter()\n",
    "    \n",
    "    def process_item(item):\n",
    "        if isinstance(item, dict):\n",
    "            # Handle base_models section\n",
    "            if 'base_models' in item and isinstance(item['base_models'], list):\n",
    "                for model in item['base_models']:\n",
    "                    if isinstance(model, str):\n",
    "                        normalized_name = normalize_model_name(model)\n",
    "                        model_counter[normalized_name] += 1\n",
    "            \n",
    "            # Handle benchmarks section\n",
    "            if 'benchmarks' in item and isinstance(item['benchmarks'], list):\n",
    "                for benchmark in item['benchmarks']:\n",
    "                    if isinstance(benchmark, str):\n",
    "                        normalized_name = benchmark.strip('\"').strip().lower()\n",
    "                        normalized_name = re.sub(r'[-_\\s]+', '-', normalized_name)\n",
    "                        benchmark_counter[normalized_name] += 1\n",
    "            \n",
    "            # Recursively process all dictionary values\n",
    "            for value in item.values():\n",
    "                process_item(value)\n",
    "                \n",
    "        elif isinstance(item, list):\n",
    "            # Recursively process all list items\n",
    "            for value in item:\n",
    "                process_item(value)\n",
    "    \n",
    "    process_item(data)\n",
    "    return model_counter, benchmark_counter\n",
    "\n",
    "def create_frequency_dataframes(model_counter: Counter, benchmark_counter: Counter) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Creates pandas DataFrames with frequency information.\n",
    "    \"\"\"\n",
    "    # Create DataFrame for models\n",
    "    models_df = pd.DataFrame([\n",
    "        {\"model\": model, \"frequency\": count}\n",
    "        for model, count in model_counter.most_common()\n",
    "    ])\n",
    "    \n",
    "    # Create DataFrame for benchmarks\n",
    "    benchmarks_df = pd.DataFrame([\n",
    "        {\"benchmark\": benchmark, \"frequency\": count}\n",
    "        for benchmark, count in benchmark_counter.most_common()\n",
    "    ])\n",
    "    \n",
    "    return models_df, benchmarks_df\n",
    "\n",
    "def print_statistics(models_df: pd.DataFrame, benchmarks_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Prints detailed statistics about models and benchmarks.\n",
    "    \"\"\"\n",
    "    print(\"=== Model Statistics ===\")\n",
    "    print(f\"Total unique models (after normalization): {len(models_df)}\")\n",
    "    print(f\"Total model mentions: {models_df['frequency'].sum()}\")\n",
    "    print(\"\\nTop 20 most frequently mentioned models:\")\n",
    "    print(models_df.head(20).to_string(index=False))\n",
    "    \n",
    "    print(\"\\n=== Benchmark Statistics ===\")\n",
    "    print(f\"Total unique benchmarks: {len(benchmarks_df)}\")\n",
    "    print(f\"Total benchmark mentions: {benchmarks_df['frequency'].sum()}\")\n",
    "    print(\"\\nTop 20 most frequently mentioned benchmarks:\")\n",
    "    print(benchmarks_df.head(20).to_string(index=False))\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Load and process data\n",
    "        with open('papers_analysis_iterfinal_20250107_200800.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Get frequency counts\n",
    "        model_counter, benchmark_counter = analyze_paper_data(data)\n",
    "        \n",
    "        # Create DataFrames\n",
    "        models_df, benchmarks_df = create_frequency_dataframes(model_counter, benchmark_counter)\n",
    "        \n",
    "        # Save frequency data to CSV\n",
    "        models_df.to_csv('model_frequencies_normalized.csv', index=False)\n",
    "        benchmarks_df.to_csv('benchmark_frequencies_normalized.csv', index=False)\n",
    "        \n",
    "        # Print statistics\n",
    "        print_statistics(models_df, benchmarks_df)\n",
    "        \n",
    "        print(\"\\nDetailed frequency data has been saved to:\")\n",
    "        print(\"- model_frequencies_normalized.csv\")\n",
    "        print(\"- benchmark_frequencies_normalized.csv\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: papers_analysis_iterfinal_20250107_200800.json file not found\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: Invalid JSON format in the input file\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae79e46e-b724-4b60-bb92-7d47a521aa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching paper dates: 100%|████████████| 4886/4886 [6:25:52<00:00,  4.74s/paper]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done! Checking results...\n",
      "\n",
      "Could not find dates for 330 papers:\n",
      "- Energy-Based Diffusion Language Models for Text Generation\n",
      "- Advancing GenAI Assisted Programming-A Comparative Study on Prompt Efficiency and Code Quality Between GPT-4 and GLM-4\n",
      "- Meaningful Learning Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance\n",
      "- Autonomous Tree-search Ability of Large Language Models\n",
      "- Retrieval-Augmented Generation for AI-Generated Content A Survey\n",
      "- Self-Consistency Preference Optimization\n",
      "- Language Agents Meet Causality - Bridging LLMs and Causal World Models\n",
      "- Ambiguity-Aware In-Context Learning with Large Language Models\n",
      "- AVA Towards Autonomous Visualization Agents through Visual PerceptionDriven DecisionMaking\n",
      "- Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models\n",
      "- Self-Supervised Multimodal Learning A Survey\n",
      "- Non-myopic Generation of Language Models for Reasoning and Planning\n",
      "- Towards More Effective Table-to-Text Generation Assessing In-Context Learning and Self-Evaluation with Open-Source Models\n",
      "- Scaling Data-Constrained Language Models\n",
      "- Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models A Case Study on ChatGPT\n",
      "- Chain-of-Scrutiny Detecting Backdoor Attacks for Large Language Models\n",
      "- Aligning LLMs to Be Robust Against Prompt Injection\n",
      "- Anchored Alignment for Self-Explanations Enhancement\n",
      "- Repository-Level Prompt Generation for Large Language Models of Code\n",
      "- Its not like Jarvis but its pretty close - Examining ChatGPTs Usage among Undergraduate Students in Computer Science\n",
      "- A Survey on Vision-Language-Action Models for Embodied AI\n",
      "- Self-Correcting LLM-Controlled Diffusion Models\n",
      "- AutoManual Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning\n",
      "- Video In-context Learning\n",
      "- UltraFeedback Boosting Language Models with High-quality Feedback\n",
      "- Determinants of LLM-assisted Decision-Making\n",
      "- RAH RecSysAssistantHuman A Human-Centered Recommendation Framework With LLM Agents\n",
      "- Trade-off Between Efficiency and Consistency for Removal-based Explanations\n",
      "- Next-Generation Database Interfaces A Survey of LLM-based Text-to-SQL\n",
      "- A Mechanism for Sample-Efficient In-Context Learning for Sparse Retrieval Tasks\n",
      "- Probabilistic Adaptation of Text-to-Video Models\n",
      "- Large Model Agents State-of-the-Art Cooperation Paradigms Security and Privacy and Future Trends\n",
      "- Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning\n",
      "- Data Poisoning for In-context Learning\n",
      "- Achieving97 on GSM8K Deeply Understanding the Problems Makes LLMs Better Solvers for Math Word Problems\n",
      "- GPT-Neo for commonsense reasoning-a theoretical and practical lens\n",
      "- LLM Multi-Agent Systems Challenges and Open Problems\n",
      "- Dissecting Chain-of-Thought A Study on Compositional In-Context Learning of MLPs\n",
      "- Hierarchical Video-Moment Retrieval and Step-Captioning\n",
      "- On the use of Large Language Models in Model-Driven Engineering\n",
      "- Multi-stage Large Language Model Correction for Speech Recognition\n",
      "- Structure-informed Language Models Are Protein Designers\n",
      "- A qualitative assessment of using ChatGPT as large language model for scientific workflow development\n",
      "- Tele-FLM Technical Report\n",
      "- Reasoning Circuits Few-shot Multi-hop Question Generation with Structured Rationales\n",
      "- Parameter-Efficient Tuning Helps Language Model Alignment\n",
      "- Towards Open-Ended Visual Recognition with Large Language Model\n",
      "- Are Human-generated Demonstrations Necessary for In-context Learning\n",
      "- Test-Driven Development and LLM-based Code Generation\n",
      "- Large Language ModelsLLMs on Tabular Data Prediction Generation and Understanding - A Survey\n",
      "- Managing AI Risks in an Era of Rapid Progress\n",
      "- Innovations in Neural Data-to-text Generation\n",
      "- LLM-augmented Preference Learning from Natural Language\n",
      "- Internet-augmented language models through few-shot prompting for open-domain question answering\n",
      "- Progressive-Hint Prompting Improves Reasoning in Large Language Models\n",
      "- Large Language Models are Diverse Role-Players for Summarization Evaluation\n",
      "- Mixture-of-Experts Meets Instruction Tuning A Winning Combination for Large Language Models\n",
      "- Agent-Oriented Planning in Multi-Agent Systems\n",
      "- Fact-checking based fake news detection a review\n",
      "- Intuition-aware Mixture-of-Rank-1-Experts for Parameter Efficient Finetuning\n",
      "- A Comprehensive Study of GPT-4Vs Multimodal Capabilities in Medical Imaging\n",
      "- Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation\n",
      "- Black-box Uncertainty Quantification Method for LLM-as-a-Judge\n",
      "- BMW Agents - A Framework For Task Automation Through Multi-Agent Collaboration\n",
      "- Evaluating Large Language Models in Process Mining Capabilities Benchmarks Evaluation Strategies and Future Challenges\n",
      "- Short Film Dataset SFD A Benchmark for Story-Level Video Understanding\n",
      "- Context Matter Data-Efficient Augmentation of Large Language Models for Scientific Applications\n",
      "- Chain-of-Thought Predictive Control\n",
      "- Self-planning Code Generation with Large Language Models\n",
      "- Instance-adaptive Zero-shot Chain-of-Thought Prompting\n",
      "- On the Expressive Power of a Variant of the Looped Transformer\n",
      "- MetaMath Integrating Natural Language and Code for Enhanced Mathematical Reasoning in Large Language Models\n",
      "- Correctness Comparison of ChatGPT4 Gemini Claude3 and Copilot for Spatial Tasks\n",
      "- Professional Agents - Evolving Large Language Models into Autonomous Experts with Human-Level Competencies\n",
      "- Reasoning in Transformers - Mitigating Spurious Correlations and Reasoning Shortcuts\n",
      "- Deepfake definitions performance metrics and standards datasets and a meta-review\n",
      "- Verify-and-Edit A Knowledge-Enhanced Chain-of-Thought Framework\n",
      "- DrICL Demonstration-Retrieved In-context Learning\n",
      "- LLM-Assisted Visual Analytics Opportunities and Challenges\n",
      "- SDS - See it Do it Sorted Quadruped Skill Synthesis from Single Video Demonstration\n",
      "- From Images to Textual Prompts Zero-shot Visual Question Answering with Frozen Large Language Models\n",
      "- Intern VL Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks\n",
      "- Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\n",
      "- Thinking Like an Expert Multimodal Hypergraph-of-Thought HoT Reasoning to boost Foundation Modals\n",
      "- Making Language Models Better Reasoners with Step-Aware Verifier\n",
      "- Enhancing Few-shot Text-to-SQL Capabilities of Large Language Models A Study on Prompt Design Strategies\n",
      "- LIFT Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks\n",
      "- Language Models as Zero-Shot Trajectory Generators\n",
      "- InfiMM-WebMath-40B Advancing Multimodal Pre-Training for Enhanced Mathematical Reasoning\n",
      "- Meta-prompting Optimized Retrieval-augmented Generation\n",
      "- Universal Self-adaptive Prompting\n",
      "- How to Train Data-Efficient LLMs\n",
      "- Retrieval-Augmented Chain-of-Thought in Semi-structured Domains\n",
      "- How to Determine the Preferred Image Distribution of a Black-Box Vision-Language Model\n",
      "- Human-in-the-Loop through Chain-of-Thought\n",
      "- Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them\n",
      "- Factor-Conditioned Speaking-Style Captioning\n",
      "- Paralinguistics-Aware Speech-Empowered Large Language Models for Natural Conversation\n",
      "- Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks\n",
      "- Breaking Language Barriers with a LEAP Learning Strategies for Polyglot LLMs\n",
      "- Conformer LLMs - Convolution Augmented Large Language Models\n",
      "- Test-Time Fairness and Robustness in Large Language Models\n",
      "- AgentDojo A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents\n",
      "- Least-to-Most Prompting Enables Complex Reasoning in Large Language Models\n",
      "- Large Language Models are Better Reasoners with Self-Verification\n",
      "- A Meta-Evaluation of Faithfulness Metrics for Long-Form Hospital-Course Summarization\n",
      "- THaLLE Text Hyperlocally Augmented Large Language Extension - Technical Report\n",
      "- Evaluating Instruction-Tuned Large Language Models on Code Comprehension and Generation\n",
      "- Leveraging Large Language Models for Automated Web-Form-Test Generation An Empirical Study\n",
      "- Grace Language Models Meet Code Edits\n",
      "- InternGPT Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language\n",
      "- Few-shot In-context Learning on Knowledge Base Question Answering\n",
      "- Tool-Augmented Reward Modeling\n",
      "- Why think step-by-step Reasoning emerges from the locality of experience\n",
      "- PAL Program-aided Language Models\n",
      "- Can AI Understand Human Personality - Comparing Human Experts and AI Systems at Predicting Personality Correlations\n",
      "- An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought\n",
      "- LLM-Enhanced Data Management\n",
      "- Towards Multimodal In-Context Learning for Vision  Language Models\n",
      "- Annotation-Efficient Preference Optimization for Language Model Alignment\n",
      "- Evaluating Human-Language Model Interaction\n",
      "- A Survey on ChatGPT AIGenerated Contents Challenges and Solutions\n",
      "- T-cell receptor binding prediction A machine learning revolution\n",
      "- Self-critiquing models for assisting human evaluators\n",
      "- Multi-agent Planning using Visual Language Models\n",
      "- Multimodal Chain-of-Thought Reasoning in Language Models\n",
      "- Large Language Models as Commonsense Knowledge for Large-Scale Task Planning\n",
      "- Planning-Driven Programming A Large Language Model Programming Workflow\n",
      "- Long-Horizon Planning for Multi-Agent Robots in Partially Observable Environments\n",
      "- Self-supervised Interpretable Concept-based Models for Text Classification\n",
      "- Token-Efficient Leverage Learning in Large Language Models\n",
      "- Large Language Models for Constrained-Based Causal Discovery\n",
      "- Learning Performance-Improving Code Edits\n",
      "- Understanding In-Context Learning from Repetitions\n",
      "- Jailbreaking Text-to-Image Models with LLM-Based Agents\n",
      "- Dont Trust Verify - Grounding LLM Quantitative Reasoning with Autoformalization\n",
      "- An In-Context Learning Agent for Formal Theorem-Proving\n",
      "- Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training\n",
      "- Robustness of Demonstration-based Learning Under Limited Data Scenario\n",
      "- SayPlan Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning\n",
      "- FacTool Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios\n",
      "- Self-contradictory Hallucinations of Large Language Models Evaluation Detection and Mitigation\n",
      "- Retrieval-Augmented Generation for Large Language Models A Survey\n",
      "- Self-Supervised Open-Ended Classification with Small Visual Language Models\n",
      "- Universal Self-Consistency for Large Language Model Generation\n",
      "- Chain of Explanation New Prompting Method to Generate Quality Natural Language Explanation for Implicit Hate Speech\n",
      "- Language Model Decoding as LikelihoodUtility Alignment\n",
      "- Decoding In-Context Learning Neuroscience-inspired Analysis of Representations in Large Language Models\n",
      "- A Survey on In-context Learning\n",
      "- Do Large Language Models Know What They Dont Know\n",
      "- How AI Processing Delays Foster Creativity Exploring Research Question Co-Creation with an LLM-based Agent\n",
      "- A Survey on Compositional Learning of AI Models Theoretical and Experimetnal Practices\n",
      "- Do LLMs Have Political Correctness Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems\n",
      "- Integrating Planning into Single-Turn Long-Form Text Generation\n",
      "- Brain-to-Text Decoding with Context-Aware Neural Representations and Large Language Models\n",
      "- A Multi-Source Retrieval Question Answering Framework Based on RAG\n",
      "- Self-consistent Reasoning For Solving Math Word Problems\n",
      "- Battle of the Large Language Models Dolly vs LLaMA vs Vicuna vs Guanaco vs Bard vs ChatGPT - A Text-to-SQL Parsing Comparison\n",
      "- Element-aware Summarization with Large Language Models Expert-aligned Evaluation and Chain-of-Thought Method\n",
      "- Re-Scoring Using Image-Language Similarity for Few-Shot Object Detection\n",
      "- Optimizing Prompts for Text-to-Image Generation\n",
      "- Skywork-Math Data Scaling Laws for Mathematical Reasoning in Large Language Models - The Story Goes On\n",
      "- Chain-of-Spot Interactive Reasoning Improves Large Vision-Language Models\n",
      "- The Expressive Power of Low-Rank Adaptation\n",
      "- Self-collaboration Code Generation via ChatGPT\n",
      "- Vision-Language Interpreter for Robot Task Planning\n",
      "- Calibrating LLM-Based Evaluator\n",
      "- Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning\n",
      "- On the Planning Abilities of Large Language Models - A Critical Investigation\n",
      "- LLM3 Large Language Model-based Task and Motion Planning with Motion Failure Reasoning\n",
      "- Learning by Self-Explaining\n",
      "- Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models\n",
      "- Large Language Models Are Zero-Shot Time Series Forecasters\n",
      "- Large Language Models for Human-like Autonomous Driving A Survey\n",
      "- Multi-Level Explanations for Generative Language Models\n",
      "- Insert-expansions for Tool-enabled Conversational Agents\n",
      "- Leveraging Environment Interaction for Automated PDDL Generation and Planning with Large Language Models\n",
      "- Gender-specific Machine Translation with Large Language Models\n",
      "- In-Context Editing Learning Knowledge from Self-Induced Distributions\n",
      "- Tuning-Free Accountable Intervention for LLM Deployment - A Metacognitive Approach\n",
      "- Document-Level In-Context Few-Shot Relation Extraction via Pre-Trained Language Models\n",
      "- Scaling Instruction-Finetuned Language Models\n",
      "- Tulip Agent - Enabling LLM-Based Agents to Solve Tasks Using Large Tool Libraries\n",
      "- Comparative Analysis of GPT-4 and Human Graders in Evaluating Human Tutors Giving Praise to Students\n",
      "- Diffusion Self-Distillation for Zero-Shot Customized Image Generation\n",
      "- The student becomes the master Outperforming GPT3 on Scientific Factual Error Correction\n",
      "- General Purpose Artificial Intelligence Systems GPAIS Properties Definition Taxonomy Open Challenges and Implications\n",
      "- Gesture-Informed Robot Assistance via Foundation Models\n",
      "- Visual Chain-of-Thought Diffusion Models\n",
      "- Self-Prompting Large Language Models for Zero-Shot Open-Domain QA\n",
      "- Evaluation of Retrieval-Augmented Generation A Survey\n",
      "- General-Purpose In-Context Learning by Meta-Learning Transformers\n",
      "- Evaluating Language-Model Agents on Realistic Autonomous Tasks\n",
      "- Exploiting Large Language Models Capabilities for Question Answer-Driven Knowledge Graph Completion Across Static and Temporal Domains\n",
      "- Recitation-Augmented Language Models\n",
      "- Distilled Language Models are economically efficient for the enterprise mostly\n",
      "- Chain-of-Note Enhancing Robustness in Retrieval-Augmented Language Models\n",
      "- Large Language Models as Agents in Two-Player Games\n",
      "- On Domain-Specific Post-Training for Multimodal Large Language Models\n",
      "- Ask Again Then Fail Large Language Models Vacillations in Judgement\n",
      "- Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V\n",
      "- Satisfiability-Aided Language Models Using Declarative Prompting\n",
      "- Training Chain-of-Thought via Latent-Variable Inference\n",
      "- Can MLLMs Perform Text-to-Image In-Context Learning\n",
      "- DreamBench A Human-Aligned Benchmark for Personalized Image Generation\n",
      "- Evaluating the Instruction-following Abilities of Language Models using Knowledge Tasks\n",
      "- Few-Shot Self-Rationalization with Natural Language Prompts\n",
      "- Improving accuracy of GPT-34 results on biomedical data using a retrieval-augmented language model\n",
      "- Language-Image Models with 3D Understanding\n",
      "- When Geoscience Meets Foundation Models Toward a general geoscience artificial intelligence system\n",
      "- Active Prompting with Chain-of-Thought for Large Language Models\n",
      "- Implicit Sentiment Analysis Based on Chain-of-Thought Prompting\n",
      "- Inductive-bias Learning Generating Code Models with Large Language Model\n",
      "- Towards Top-Down Reasoning An Explainable Multi-Agent Approach for Visual Question Answering\n",
      "- Improving Cross-Task Generalization with Step-by-Step Instructions\n",
      "- GPT-4Vision is a Human-Aligned Evaluator for Text-to-3D Generation\n",
      "- ChatGPT for GTFS From Words to Information\n",
      "- Receiver-Centric Generative Semantic Communications\n",
      "- Large Language Models Can Self-Improve At Web Agent Tasks\n",
      "- Max-Margin Token Selection in Attention Mechanism\n",
      "- Towards Vision-Language Geo-Foundation Model A Survey\n",
      "- Initialization is Critical to Whether Transformers Fit Composite Functions by Inference or Memorizing\n",
      "- Measuring Faithfulness in Chain-of-Thought Reasoning\n",
      "- ChatGPT Is Here to Help Not to Replace Anybody - An Evaluation of Students Opinions On Integrating ChatGPT In CS Courses\n",
      "- GPT-Fabric Folding and Smoothing Fabric by Leveraging Pre-Trained Foundation Models\n",
      "- Beyond Accuracy Evaluating the Reasoning Behavior of Large Language Models - A Survey\n",
      "- Chain-of-Verification Reduces Hallucination in Large Language Models\n",
      "- Exploring the Potential of Large Language Models LLMsin Learning on Graphs\n",
      "- MARIO MAth Reasoning with code Interpreter Output - A Reproducible Pipeline\n",
      "- Theoretical limitations of multi-layer Transformer\n",
      "- Exploration of LLM Multi-Agent Application Implementation Based on LangGraphCrewAI\n",
      "- Teach LLMs to Personalize - An Approach inspired by Writing Education\n",
      "- IMProv Inpainting-based Multimodal Prompting for Computer Vision Tasks\n",
      "- Chain-of-Restoration Multi-Task Image Restoration Models are Zero-Shot Step-by-Step Universal Image Restorers\n",
      "- Non-Autoregressive Sentence Ordering\n",
      "- Global-Local Collaborative Inference with LLM for Lidar-Based Open-Vocabulary Detection\n",
      "- Requirements Satisfiability with In-Context Learning\n",
      "- The pitfalls of next-token prediction\n",
      "- Learning In-context Learning for Named Entity Recognition\n",
      "- Construction of Hyper-Relational Knowledge Graphs Using Pre-Trained Large Language Models\n",
      "- Prompting with Pseudo-Code Instructions\n",
      "- Retrieval-Enhanced Named Entity Recognition\n",
      "- Self-Guiding Exploration for Combinatorial Problems\n",
      "- Are Large Language Models Aligned with Peoples Social Intuitions for HumanRobot Interactions\n",
      "- Trustworthiness in Retrieval-Augmented Generation Systems A Survey\n",
      "- A Survey on Human-AI Teaming with Large Pre-Trained Models\n",
      "- MaterioMiner - An ontology-based text mining dataset for extraction of process-structure-property entities\n",
      "- Context-Enhanced Language Models for Generating Multi-paper Citations\n",
      "- SCOTT Self-Consistent Chain-of-Thought Distillation\n",
      "- From Natural Language to SQL Review of LLM-based Text-to-SQL Systems\n",
      "- Exploring Human-Like Translation Strategy with Large Language Models\n",
      "- A Philosophical Introduction to Language Models - Part I Continuity With Classic Debates\n",
      "- Concept - An Evaluation Protocol on Conversational Recommender Systems with System-centric and User-centric Factors\n",
      "- Investigate-Consolidate-Exploit A General Strategy for Inter-Task Agent Self-Evolution\n",
      "- Language Models are General-Purpose Interfaces\n",
      "- Graph-Based Captioning Enhancing Visual Descriptions by Interconnecting Region Captions\n",
      "- Mitigating Entity-Level Hallucination in Large Language Models\n",
      "- Benchmarking the Text-to-SQL Capability of Large Language Models A Comprehensive Evaluation\n",
      "- Self-Infilling Code Generation\n",
      "- Temporal Data Meets LLM - Explainable Financial Time Series Forecasting\n",
      "- Biomedical knowledge graph-optimized prompt generation for large language models\n",
      "- Uncovering mesa-optimization algorithms in Transformers\n",
      "- Vulnerability Handling of AI-Generated Code - Existing Solutions and Open Challenges\n",
      "- In-Context Fine-Tuning for Time-Series Foundation Models\n",
      "- Large Language Models are Versatile Decomposers Decomposing Evidence and Questions for Table-based Reasoning\n",
      "- Design of Chain-of-Thought in Math Problem Solving\n",
      "- T-SciQ Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering\n",
      "- Self-Consistency Improves Chain of Thought Reasoning in Language Models\n",
      "- Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large Language Models\n",
      "- Task-oriented Robotic Manipulation with Vision Language Models\n",
      "- DataGpt-SQL-7B An Open-Source Language Model for Text-to-SQL\n",
      "- Many-Shot In-Context Learning\n",
      "- From Instructions to Intrinsic Human Values - A Survey of Alignment Goals for Big Models\n",
      "- Of Models and Tin Men - a behavioural economics study of principal-agent problems in AI alignment using large-language models\n",
      "- C2Ideas Supporting Creative Interior Color Design Ideation with a Large Language Model\n",
      "- Towards Knowledge-driven Autonomous Driving\n",
      "- Concept-aware Training Improves In-context Learning Ability of Language Models\n",
      "- A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection\n",
      "- Demystifying Higher-Order Graph Neural Networks\n",
      "- From Sands to Mansions Enabling Automatic Full-Life-Cycle Cyberattack Construction with LLM\n",
      "- Decision-Oriented Dialogue for Human-AI Collaboration\n",
      "- utoL Autonomous Evaluation of LLMs for Truth Maintenance and Reasoning Tasks\n",
      "- 15-Pints Technical Report Pretraining in Days Not Months - Your Language Model Thrives on Quality Data\n",
      "- A Survey of Knowledge Graph Reasoning on Graph Types Static Dynamic and Multi-Modal\n",
      "- Keeping Users Engaged During Repeated Interviews by a Virtual Agent Using Large Language Models to Reliably Diversify Questions\n",
      "- H2O Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models\n",
      "- Query Rewriting for Retrieval-Augmented Large Language Models\n",
      "- The Two-Hop Curse LLMs trained on A-B B-C fail to learn A--C\n",
      "- On the Effectiveness of Large Language Models in Domain-Specific Code Generation\n",
      "- sigma-GPTs A New Approach to Autoregressive Models\n",
      "- Feedback-guided Data Synthesis for Imbalanced Classification\n",
      "- Test smells in LLM-Generated Unit Tests\n",
      "- TELM Test and Evaluation of Language Models\n",
      "- HumanEval on Latest GPT Models - 2024\n",
      "- Multi-Conditional Ranking with Large Language Models\n",
      "- Self-Reflection in LLM Agents Effects on Problem-Solving Performance\n",
      "- Demonstrate-Search-Predict Composing retrieval and language models for knowledge-intensive NLP\n",
      "- A Theory of Intelligences Concepts Models Implications\n",
      "- Beyond Text Improving LLMs Decision Making for Robot Navigation via Vocal Cues\n",
      "- Large Language Models Are Semi-Parametric Reinforcement Learning Agents\n",
      "- AHAM Adapt Help Ask Model - Harvesting LLMs for literature mining\n",
      "- Pronunciation Assessment with Multi-modal Large Language Models\n",
      "- A Survey of Language-Based Communication in Robotics\n",
      "- Better Zero-Shot Reasoning with Self-Adaptive Prompting\n",
      "- LLM-Empowered State Representation for Reinforcement Learning\n",
      "- Co-Writing Screenplays and Theatre Scripts with Language Models Evaluation by Industry Professionals\n",
      "- Fact-Checking Complex Claims with Program-Guided Reasoning\n",
      "- Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency\n",
      "- Dynamic Self-Distillation via Previous Mini-batches for Fine-tuning Small Language Models\n",
      "- AIMS-EREA - A framework for AI-accelerated Innovation of Materials for Sustainability - for Environmental Remediation and Energy Applications\n",
      "- Language Models as Black-Box Optimizers for Vision-Language Models\n",
      "- Model Criticism for Long-Form Text Generation\n",
      "- Large Language Models Are Human-Level Prompt Engineers\n",
      "- LiveXiv - A Multi-Modal Live Benchmark Based on Arxiv Papers Content\n",
      "- Rationale-Augmented Ensembles in Language Models\n",
      "- Rethinking Machine Ethics - Can LLMs Perform Moral Reasoning through the Lens of Moral Theories\n",
      "- Boosting Theory-of-Mind Performance in Large Language Models via Prompting\n",
      "- Are Large Language Models Table-based Fact-Checkers\n",
      "- Mixture-of-Agents Enhances Large Language Model Capabilities\n",
      "- GPT-in-the-Loop Adaptive Decision-Making for Multiagent Systems\n",
      "- Can large language models explore in-context\n",
      "- Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation\n",
      "- A Survey of the Evolution of Language Model-Based Dialogue Systems\n",
      "- Generative Calibration for In-context Learning\n",
      "- Towards Uncertainty-Aware Language Agent\n",
      "- Cognitively Inspired Energy-Based World Models\n",
      "- Unmasking Context Injection on Interactive Large Language Models\n",
      "- Fairness-Aware Graph Neural Networks A Survey\n",
      "- Code-Style In-Context Learning for Knowledge-Based Question Answering\n",
      "- Evaluating Open-QA Evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import arxiv\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_title(title):\n",
    "    \"\"\"Clean and normalize the title for better matching\"\"\"\n",
    "    # Remove special characters and extra whitespace\n",
    "    cleaned = re.sub(r'[^\\w\\s-]', '', title)\n",
    "    cleaned = ' '.join(cleaned.split())\n",
    "    return cleaned\n",
    "\n",
    "def fetch_arxiv_date(title):\n",
    "    \"\"\"\n",
    "    Fetch publication date from arXiv given a paper title.\n",
    "    Try multiple search strategies.\n",
    "    \"\"\"\n",
    "    client = arxiv.Client()\n",
    "    \n",
    "    # Strategy 1: Exact title match\n",
    "    search = arxiv.Search(\n",
    "        query=f'ti:\"{title}\"',\n",
    "        max_results=1\n",
    "    )\n",
    "    result = next(client.results(search), None)\n",
    "    if result:\n",
    "        return result.published.strftime('%Y-%m-%d'), result.entry_id\n",
    "\n",
    "    time.sleep(1)  # Rate limiting between attempts\n",
    "    \n",
    "    # Strategy 2: Search without quotes and ti: prefix\n",
    "    cleaned_title = clean_title(title)\n",
    "    search = arxiv.Search(\n",
    "        query=cleaned_title,\n",
    "        max_results=5  # Get more results to find potential matches\n",
    "    )\n",
    "    \n",
    "    # Check each result for title similarity\n",
    "    results = list(client.results(search))\n",
    "    for result in results:\n",
    "        result_title = clean_title(result.title)\n",
    "        # Check if titles are very similar\n",
    "        if (cleaned_title.lower() in result_title.lower() or \n",
    "            result_title.lower() in cleaned_title.lower()):\n",
    "            return result.published.strftime('%Y-%m-%d'), result.entry_id\n",
    "    \n",
    "    time.sleep(1)  # Rate limiting between attempts\n",
    "    \n",
    "    # Strategy 3: Try with first few words of title\n",
    "    first_words = ' '.join(cleaned_title.split()[:4])  # First 4 words\n",
    "    search = arxiv.Search(\n",
    "        query=f'ti:\"{first_words}\"',\n",
    "        max_results=5\n",
    "    )\n",
    "    \n",
    "    results = list(client.results(search))\n",
    "    for result in results:\n",
    "        result_title = clean_title(result.title)\n",
    "        if cleaned_title.lower() in result_title.lower():\n",
    "            return result.published.strftime('%Y-%m-%d'), result.entry_id\n",
    "            \n",
    "    return None, None\n",
    "\n",
    "# Load your JSON data\n",
    "with open('papers_analysis_iterfinal_20250107_200800.json', 'r') as f:\n",
    "    papers_data = json.load(f)\n",
    "\n",
    "# Add dates to the data\n",
    "for title in tqdm(papers_data.keys(), desc=\"Fetching paper dates\", unit=\"paper\"):\n",
    "    paper_info = papers_data[title]\n",
    "    date, arxiv_id = fetch_arxiv_date(title)\n",
    "    if date:\n",
    "        paper_info['publication_date'] = date\n",
    "        paper_info['arxiv_id'] = arxiv_id\n",
    "        # tqdm.write(f\"✓ Found date for: {title} ({date})\")\n",
    "    else:\n",
    "        continue\n",
    "        # tqdm.write(f\"✗ Could not find date for: {title}\")\n",
    "    # Respect rate limits\n",
    "    time.sleep(2)\n",
    "\n",
    "# Save the updated data\n",
    "with open('papers_data_with_dates.json', 'w') as f:\n",
    "    json.dump(papers_data, f, indent=4)\n",
    "\n",
    "print(\"\\nDone! Checking results...\")\n",
    "# Print summary of papers without dates\n",
    "missing_dates = [title for title, info in papers_data.items() \n",
    "                if 'publication_date' not in info]\n",
    "if missing_dates:\n",
    "    print(f\"\\nCould not find dates for {len(missing_dates)} papers:\")\n",
    "    for title in missing_dates:\n",
    "        print(f\"- {title}\")\n",
    "else:\n",
    "    print(\"\\nFound dates for all papers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3712815-a0aa-4010-b451-014be6774d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paper Statistics:\n",
      "Total papers analyzed: 4865\n",
      "\n",
      "Model statistics:\n",
      "Total model mentions: 16629\n",
      "Papers that mention models: 4760 (97.8%)\n",
      "Average models per paper (across all papers): 3.42\n",
      "Average models when present: 3.49\n",
      "\n",
      "Benchmark statistics:\n",
      "Total benchmark mentions: 14903\n",
      "Papers that mention benchmarks: 4464 (91.8%)\n",
      "Average benchmarks per paper (across all papers): 3.06\n",
      "Average benchmarks when present: 3.34\n",
      "\n",
      "Top 20 Benchmarks and their frequencies:\n",
      "GSM8K: 425\n",
      "MATH: 174\n",
      "MMLU: 171\n",
      "SVAMP: 139\n",
      "StrategyQA: 126\n",
      "HotpotQA: 120\n",
      "HumanEval: 119\n",
      "TruthfulQA: 81\n",
      "CommonsenseQA: 77\n",
      "HellaSwag: 70\n",
      "AQUA: 64\n",
      "TriviaQA: 63\n",
      "MBPP: 63\n",
      "MultiArith: 61\n",
      "Winogrande: 56\n",
      "BoolQ: 55\n",
      "PiQA: 52\n",
      "openbookqa: 49\n",
      "ASDiv: 46\n",
      "SQuAD: 44\n",
      "\n",
      "Top 20 Models and their frequencies:\n",
      "GPT-4: 1789\n",
      "ChatGPT (gpt-3.5-turbo): 1739\n",
      "GPT-3 (DaVinci-002): 705\n",
      "PaLM-2: 415\n",
      "GPT4o: 373\n",
      "LLaMa 2 (7B parameters): 372\n",
      "BERT: 266\n",
      "Llama-3-8B: 264\n",
      "Mistral 7B: 243\n",
      "LLaMA-1: 230\n",
      "LLaMA-7B: 201\n",
      "LLAMA-2 (13b): 198\n",
      "GPT-4V: 195\n",
      "Llama3 (70B): 175\n",
      "LLaMA 2 70B: 172\n",
      "T5: 171\n",
      "RoBERTa: 159\n",
      "LLaMA-2: 116\n",
      "GPT-2: 110\n",
      "Llama 2 7B Chat (7 billion): 108\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.dates as mdates\n",
    "from scipy import stats\n",
    "\n",
    "def normalize_model_name(name: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Normalizes model names while preserving important distinctions between model sizes\n",
    "    and architectural variants. Returns tuple of (normalized_name, original_name)\n",
    "    \"\"\"\n",
    "    original = name.strip('\"').strip()\n",
    "    normalized = original.lower()\n",
    "    normalized = re.sub(r'\\s+', ' ', normalized)\n",
    "    \n",
    "    # Extract parameter count/size information\n",
    "    size_pattern = r'[\\(\\[\\s]?(\\d+[bm])[\\)\\]\\s]?'\n",
    "    size_match = re.search(size_pattern, normalized, re.IGNORECASE)\n",
    "    model_size = size_match.group(1).lower() if size_match else None\n",
    "    \n",
    "    # First check for GPT-4 variants\n",
    "    gpt4_patterns = [\n",
    "        'gpt4', 'gpt 4', 'gpt-4', 'gpt-4-turbo', 'gpt4-turbo',\n",
    "        'chatgpt-4', 'chat-gpt-4', 'chatgpt-4.0', 'chat gpt 4', \n",
    "        'chatgpt 4', 'gpt-4 turbo', 'gpt 4 turbo'\n",
    "    ]\n",
    "    \n",
    "    if any(re.match(f'^{re.escape(pattern)}', normalized) for pattern in gpt4_patterns):\n",
    "        if any(v in normalized for v in ['vision', '-v', 'v']):\n",
    "            return 'gpt-4v', original\n",
    "        elif any(o in normalized for o in ['4o', '4-o', '4 o']):\n",
    "            return 'gpt-4o', original\n",
    "        return 'gpt-4', 'GPT-4'\n",
    "    \n",
    "    # Model family mappings with their variants\n",
    "    base_models = {\n",
    "        'gpt-3.5': [\n",
    "            'gpt-3.5-turbo', 'gpt3.5-turbo', 'gpt3.5 turbo', \n",
    "            'gpt-3.5-t', 'chatgpt', 'chat-gpt', 'chat gpt', \n",
    "            'chatgpt-3.5', 'chatgpt-3.5-turbo', 'chat-gpt-3.5',\n",
    "            'chat gpt 3.5', 'chatgpt 3.5', 'gpt3.5'\n",
    "        ],\n",
    "        'gpt-3': [\n",
    "            'gpt-3', 'gpt3', 'gpt 3', 'gpt-3-davinci',\n",
    "            'davinci', 'text-davinci', 'davinci-002',\n",
    "            'chatgpt-3', 'chat-gpt-3', 'gpt-3-(175b)',\n",
    "            'gpt-3 (175b)', 'gpt-3 (davinci)'\n",
    "        ],\n",
    "        'llama-2': [\n",
    "            'llama2', 'llama 2', 'llama-2', 'llama2-', \n",
    "            'llama-2-', 'llama 2 ', 'llama2 ', 'llama-2 '\n",
    "        ],\n",
    "        'llama': [\n",
    "            'llama1', 'llama 1', 'llama-1', 'llama-', \n",
    "            'llama ', 'llamab', 'llama-b'\n",
    "        ],\n",
    "        'mistral': [\n",
    "            'mistral', 'mistral-', 'mistral '\n",
    "        ],\n",
    "        'palm': ['palm-', 'palm2', 'palm 2', 'palm-2'],\n",
    "        'bert': ['bert-base', 'bert base', 'bert-large', 'bert large'],\n",
    "        'roberta': ['roberta-base', 'roberta base', 'roberta-large', 'roberta large'],\n",
    "        't5': ['t5-', 't5 ', 't5base', 't5-base', 't5 base']\n",
    "    }\n",
    "    \n",
    "    # Check for other model matches\n",
    "    for family, variants in base_models.items():\n",
    "        patterns = [f'^{re.escape(v)}' for v in variants + [family]]\n",
    "        if any(re.match(pattern, normalized) for pattern in patterns):\n",
    "            if family in ['llama', 'llama-2', 'mistral']:\n",
    "                base = family\n",
    "                \n",
    "                if any(v in normalized for v in ['instruct', 'ins']):\n",
    "                    base = f\"{base}-instruct\"\n",
    "                elif 'chat' in normalized:\n",
    "                    base = f\"{base}-chat\"\n",
    "                    \n",
    "                if family == 'mistral' and model_size:\n",
    "                    return f\"{base}-{model_size}\", original\n",
    "                elif model_size:\n",
    "                    return f\"{base}-{model_size}\", original\n",
    "                    \n",
    "                return base, original\n",
    "            \n",
    "            if model_size and family in ['bert', 'roberta', 't5']:\n",
    "                return f\"{family}-{model_size}\", original\n",
    "            \n",
    "            return family, original\n",
    "    \n",
    "    # Remove parenthetical details and extra whitespace\n",
    "    normalized = re.sub(r'\\s*\\([^)]*\\)\\s*', '', normalized)\n",
    "    normalized = re.sub(r'\\s*\\[[^\\]]*\\]\\s*', '', normalized)\n",
    "    normalized = re.sub(r'[-_\\s]+', '-', normalized)\n",
    "    \n",
    "    if model_size and not normalized.endswith(model_size):\n",
    "        normalized = f\"{normalized}-{model_size}\"\n",
    "        \n",
    "    return normalized, original\n",
    "\n",
    "def plot_ridgeline(ax, df, names, counter, colors, vertical_spacing, original_names=None):\n",
    "    \"\"\"Plot the ridgeline distribution\"\"\"\n",
    "    # Set fixed date range\n",
    "    date_min = datetime(2022, 1, 1)\n",
    "    date_max = datetime(2024, 12, 31)\n",
    "    \n",
    "    months = pd.date_range(start=date_min, end=date_max, freq='M')\n",
    "    month_width = (months[1] - months[0])\n",
    "    \n",
    "    monthly_totals = df.groupby(df['date'].dt.to_period('M'))['name'].value_counts()\n",
    "    max_freq = 0\n",
    "    \n",
    "    for idx, name in enumerate(reversed(names)):\n",
    "        if name in monthly_totals.unstack():\n",
    "            monthly_counts = monthly_totals.unstack()[name].fillna(0)\n",
    "            monthly_sums = monthly_totals.groupby('date').sum()\n",
    "            monthly_freq = monthly_counts / monthly_sums\n",
    "            max_freq = max(max_freq, monthly_freq.max())\n",
    "            \n",
    "            plot_dates = pd.to_datetime([period.to_timestamp() for period in monthly_freq.index])\n",
    "            baseline = idx * vertical_spacing\n",
    "            \n",
    "            # Plot filled bars\n",
    "            ax.bar(plot_dates, \n",
    "                  monthly_freq,\n",
    "                  width=month_width,\n",
    "                  bottom=baseline,\n",
    "                  color=colors[min(idx, len(colors)-1)],\n",
    "                  alpha=0.6,\n",
    "                  align='edge',\n",
    "                  zorder=idx)\n",
    "            \n",
    "            # Add border\n",
    "            ax.bar(plot_dates,\n",
    "                  monthly_freq,\n",
    "                  width=month_width,\n",
    "                  bottom=baseline,\n",
    "                  color='none',\n",
    "                  edgecolor=colors[min(idx, len(colors)-1)],\n",
    "                  linewidth=1.0,\n",
    "                  alpha=0.9,\n",
    "                  align='edge',\n",
    "                  zorder=idx)\n",
    "            \n",
    "            # Add peak value label\n",
    "            max_month_idx = monthly_freq.argmax()\n",
    "            max_month_date = monthly_freq.index[max_month_idx].to_timestamp()\n",
    "            ax.text(max_month_date, \n",
    "                   baseline + monthly_freq[max_month_idx], \n",
    "                   f'{monthly_freq[max_month_idx]:.2f}',\n",
    "                   ha='left', va='bottom', fontsize=8)\n",
    "            \n",
    "            # Add horizontal line\n",
    "            ax.axhline(y=baseline,\n",
    "                      color='gray',\n",
    "                      linewidth=0.2,\n",
    "                      alpha=0.3,\n",
    "                      zorder=0)\n",
    "            \n",
    "            # Add labels on the right side\n",
    "            display_name = original_names.get(name, name) if original_names else name\n",
    "            ax.annotate(f\"{display_name} ({counter[name]})\",\n",
    "                       xy=(date_max, baseline),\n",
    "                       xytext=(5, 0),\n",
    "                       textcoords=\"offset points\",\n",
    "                       va='center',\n",
    "                       fontsize=9,\n",
    "                       fontfamily='serif',\n",
    "                       fontstyle='normal',\n",
    "                       bbox=dict(facecolor='white',\n",
    "                               edgecolor='none',\n",
    "                               alpha=0.9,\n",
    "                               pad=1))\n",
    "\n",
    "def create_combined_plot(df, names, counter, title, filename, color_h, original_names=None):\n",
    "    \"\"\"Create a combined plot with paper counts and distribution\"\"\"\n",
    "    plt.style.use('default')\n",
    "    plt.rcParams.update({\n",
    "        'font.family': 'serif',\n",
    "        'font.size': 10,\n",
    "        'axes.labelsize': 11,\n",
    "        'axes.titlesize': 12,\n",
    "        'figure.titlesize': 14\n",
    "    })\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create top subplot for paper counts (25% of height)\n",
    "    ax1 = plt.subplot2grid((4, 1), (0, 0), rowspan=1)\n",
    "    \n",
    "    # Get total mentions per month\n",
    "    monthly_mentions = df.groupby(df['date'].dt.to_period('M')).size()\n",
    "    \n",
    "    start_date = datetime(2022, 1, 1)\n",
    "    end_date = datetime(2024, 12, 31)\n",
    "    \n",
    "    # First, filter papers_df to the date range we want\n",
    "    papers_df_filtered = papers_df[\n",
    "        (papers_df.index >= start_date) & \n",
    "        (papers_df.index <= end_date)\n",
    "    ]\n",
    "    \n",
    "    # Create date range and align data\n",
    "    monthly_dates = pd.date_range(start=start_date, end=end_date, freq='M')\n",
    "    \n",
    "    # Only include months where we actually have papers\n",
    "    valid_months = papers_df_filtered.index.to_period('M')\n",
    "    monthly_mentions = monthly_mentions[monthly_mentions.index.isin(valid_months)]\n",
    "    \n",
    "    # Resample papers data\n",
    "    monthly_papers = papers_df_filtered.resample('M')['total_papers'].last()\n",
    "    \n",
    "    # Ensure both series align with actual paper months\n",
    "    monthly_papers = monthly_papers[monthly_papers > 0]\n",
    "    monthly_mentions = monthly_mentions[monthly_mentions.index.isin(monthly_papers.index.to_period('M'))]\n",
    "    \n",
    "    # Create date range for plotting\n",
    "    start_date = datetime(2022, 1, 1)\n",
    "    end_date = datetime(2024, 12, 31)\n",
    "    monthly_dates = pd.date_range(start=start_date, end=end_date, freq='MS')  # MS = Month Start\n",
    "    \n",
    "    # Filter and align the data\n",
    "    papers_df_filtered = papers_df[\n",
    "        (papers_df.index >= start_date) & \n",
    "        (papers_df.index <= end_date)\n",
    "    ]\n",
    "    \n",
    "    # Resample to month start frequency\n",
    "    monthly_papers = papers_df_filtered.resample('MS')['total_papers'].last()\n",
    "    monthly_mentions = df.groupby(df['date'].dt.to_period('M')).size()\n",
    "    \n",
    "    # Ensure both series align with actual paper months\n",
    "    monthly_papers = monthly_papers[monthly_papers > 0]\n",
    "    monthly_mentions = monthly_mentions[monthly_mentions.index.isin(monthly_papers.index.to_period('M'))]\n",
    "    \n",
    "    # Convert period index to timestamp for plotting\n",
    "    plot_dates = pd.date_range(start=monthly_papers.index.min(), \n",
    "                             end=monthly_papers.index.max(), \n",
    "                             freq='MS')\n",
    "    \n",
    "    # Plot paper counts - align to start of each month\n",
    "    bars = ax1.bar(plot_dates,\n",
    "                   monthly_papers.values,\n",
    "                   width=30,\n",
    "                   color='gray',\n",
    "                   alpha=0.5,\n",
    "                   align='edge')  # Align to start of month\n",
    "\n",
    "    # Customize top subplot\n",
    "    ax1.set_xlim(start_date, end_date)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    \n",
    "    # Add value labels for paper counts\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if height > 0:\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2, height,\n",
    "                    f'{int(height)}',\n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Create bottom subplot for distribution (75% of height)\n",
    "    ax2 = plt.subplot2grid((4, 1), (1, 0), rowspan=3)\n",
    "    \n",
    "    n_shades = len(names)\n",
    "    vertical_spacing = 0.35\n",
    "    colors = sns.husl_palette(n_shades, h=color_h, s=0.85, l=0.6)\n",
    "    \n",
    "    plot_ridgeline(ax2, df, names, counter, colors, vertical_spacing, original_names)\n",
    "    \n",
    "    ax2.set_xlim(start_date, end_date)\n",
    "    ax2.yaxis.set_visible(False)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax2.spines['left'].set_visible(False)\n",
    "    \n",
    "    # Format x-axis with year ticks\n",
    "    for ax in [ax1, ax2]:\n",
    "        # Set major ticks only at start of year\n",
    "        ax.xaxis.set_major_locator(mdates.YearLocator(month=1, day=1))\n",
    "        \n",
    "        # Remove year labels from ticks\n",
    "        ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "        \n",
    "        # Handle year labels differently for each subplot\n",
    "        if ax == ax1:  # Top subplot\n",
    "            y_pos = ax.get_ylim()[0] - 0.1\n",
    "        else:  # Bottom subplot (ax2)\n",
    "            y_pos = -0.02  # Reduced negative value to move labels up\n",
    "            \n",
    "        # Manually add year labels at the middle of each year\n",
    "        years = range(2022, 2025)\n",
    "        for year in years:\n",
    "            mid_year_date = datetime(year, 7, 1)\n",
    "            ax.text(mdates.date2num(mid_year_date), y_pos,\n",
    "                   str(year),\n",
    "                   ha='center',\n",
    "                   va='top',\n",
    "                   transform=ax.get_xaxis_transform())\n",
    "        \n",
    "        # Add minor ticks for all months\n",
    "        ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "        \n",
    "        # Customize tick lengths\n",
    "        ax.tick_params(axis='x', which='major', length=8)  # Longer ticks for years\n",
    "        ax.tick_params(axis='x', which='minor', length=4)  # Shorter ticks for months\n",
    "        \n",
    "        # Adjust label positions to center them\n",
    "        ax.tick_params(axis='x', which='major', pad=10)  # Add padding for year labels\n",
    "        \n",
    "        # Add subtle grid\n",
    "        ax.grid(True, alpha=0.1, linestyle='-', axis='x')\n",
    "        ax.grid(True, alpha=0.05, linestyle='-', axis='x', which='minor')\n",
    "    \n",
    "    # Only show x-label on bottom plot\n",
    "    ax1.set_xlabel('')\n",
    "    ax2.set_xlabel('')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.subplots_adjust(right=0.85, hspace=0.3)\n",
    "    \n",
    "    plt.savefig(filename,\n",
    "                dpi=300,\n",
    "                bbox_inches='tight',\n",
    "                facecolor='white')\n",
    "    plt.close()\n",
    "\n",
    "# Load and process data\n",
    "with open('papers_data_with_dates.json', 'r') as f:\n",
    "    papers_data = json.load(f)\n",
    "\n",
    "# Calculate total papers per month\n",
    "all_papers_by_month = {}\n",
    "for title, paper_info in papers_data.items():\n",
    "    if 'publication_date' in paper_info:\n",
    "        date = paper_info['publication_date']\n",
    "        date_obj = datetime.strptime(date, '%Y-%m-%d')\n",
    "        month_key = date_obj.strftime('%Y-%m')\n",
    "        all_papers_by_month[month_key] = all_papers_by_month.get(month_key, 0) + 1\n",
    "\n",
    "# Convert to DataFrame for plotting\n",
    "papers_df = pd.DataFrame(\n",
    "    [(datetime.strptime(k, '%Y-%m'), v) for k, v in all_papers_by_month.items()],\n",
    "    columns=['date', 'total_papers']\n",
    ")\n",
    "papers_df.set_index('date', inplace=True)\n",
    "\n",
    "# Get frequency counts\n",
    "model_counter = Counter()\n",
    "benchmark_counter = Counter()\n",
    "model_original_names = {}\n",
    "benchmark_original_names = {}\n",
    "benchmark_records = []\n",
    "model_records = []\n",
    "\n",
    "# Process the data\n",
    "for title, paper_info in papers_data.items():\n",
    "    if 'publication_date' in paper_info and 'analysis' in paper_info:\n",
    "        date = paper_info['publication_date']\n",
    "        date_obj = datetime.strptime(date, '%Y-%m-%d')\n",
    "        \n",
    "        # Only process data from 2022 onwards\n",
    "        if date_obj.year >= 2022:\n",
    "            # Process benchmarks\n",
    "            if 'benchmarks' in paper_info['analysis']:\n",
    "                for benchmark in paper_info['analysis']['benchmarks']:\n",
    "                    normalized_benchmark = benchmark.strip('\"').strip().lower()\n",
    "                    normalized_benchmark = re.sub(r'[-_\\s]+', '-', normalized_benchmark)\n",
    "                    benchmark_counter[normalized_benchmark] += 1\n",
    "                    benchmark_original_names[normalized_benchmark] = benchmark.strip('\"').strip()\n",
    "                    benchmark_records.append({\n",
    "                        'date': date_obj,\n",
    "                        'name': normalized_benchmark,\n",
    "                        'type': 'Benchmark',\n",
    "                        'paper': title\n",
    "                    })\n",
    "            \n",
    "            # Process models\n",
    "            if 'base_models' in paper_info['analysis']:\n",
    "                for model in paper_info['analysis']['base_models']:\n",
    "                    normalized_model, original_name = normalize_model_name(model)\n",
    "                    model_counter[normalized_model] += 1\n",
    "                    if normalized_model not in model_original_names:\n",
    "                        model_original_names[normalized_model] = original_name\n",
    "                    model_records.append({\n",
    "                        'date': date_obj,\n",
    "                        'name': normalized_model,\n",
    "                        'type': 'Model',\n",
    "                        'paper': title\n",
    "                    })\n",
    "\n",
    "# Get top 20 benchmarks and models\n",
    "top_20_benchmarks = [b[0] for b in benchmark_counter.most_common(20)]\n",
    "top_20_models = [m[0] for m in model_counter.most_common(20)]\n",
    "\n",
    "# Create DataFrames\n",
    "df_benchmarks = pd.DataFrame(benchmark_records)\n",
    "df_models = pd.DataFrame(model_records)\n",
    "\n",
    "# Filter for top 20\n",
    "df_benchmarks = df_benchmarks[df_benchmarks['name'].isin(top_20_benchmarks)]\n",
    "df_models = df_models[df_models['name'].isin(top_20_models)]\n",
    "\n",
    "# Update DataFrames to use datetime\n",
    "df_benchmarks['date'] = pd.to_datetime(df_benchmarks['date'])\n",
    "df_models['date'] = pd.to_datetime(df_models['date'])\n",
    "\n",
    "# Calculate statistics\n",
    "total_papers = len(set(title for title, paper_info in papers_data.items() \n",
    "                      if 'publication_date' in paper_info \n",
    "                      and datetime.strptime(paper_info['publication_date'], '%Y-%m-%d').year >= 2022))\n",
    "\n",
    "total_model_mentions = 0\n",
    "total_benchmark_mentions = 0\n",
    "papers_with_models = 0\n",
    "papers_with_benchmarks = 0\n",
    "\n",
    "for title, paper_info in papers_data.items():\n",
    "    if 'publication_date' in paper_info and 'analysis' in paper_info:\n",
    "        date = datetime.strptime(paper_info['publication_date'], '%Y-%m-%d')\n",
    "        if date.year >= 2022:\n",
    "            # Count model mentions\n",
    "            if 'base_models' in paper_info['analysis'] and paper_info['analysis']['base_models']:\n",
    "                papers_with_models += 1\n",
    "                total_model_mentions += len(paper_info['analysis']['base_models'])\n",
    "            \n",
    "            # Count benchmark mentions\n",
    "            if 'benchmarks' in paper_info['analysis'] and paper_info['analysis']['benchmarks']:\n",
    "                papers_with_benchmarks += 1\n",
    "                total_benchmark_mentions += len(paper_info['analysis']['benchmarks'])\n",
    "\n",
    "# Calculate averages\n",
    "avg_models_per_paper = total_model_mentions / total_papers\n",
    "avg_benchmarks_per_paper = total_benchmark_mentions / total_papers\n",
    "avg_models_when_present = total_model_mentions / papers_with_models if papers_with_models > 0 else 0\n",
    "avg_benchmarks_when_present = total_benchmark_mentions / papers_with_benchmarks if papers_with_benchmarks > 0 else 0\n",
    "\n",
    "print(\"\\nPaper Statistics:\")\n",
    "print(f\"Total papers analyzed: {total_papers}\")\n",
    "print(f\"\\nModel statistics:\")\n",
    "print(f\"Total model mentions: {total_model_mentions}\")\n",
    "print(f\"Papers that mention models: {papers_with_models} ({papers_with_models/total_papers*100:.1f}%)\")\n",
    "print(f\"Average models per paper (across all papers): {avg_models_per_paper:.2f}\")\n",
    "print(f\"Average models when present: {avg_models_when_present:.2f}\")\n",
    "\n",
    "print(f\"\\nBenchmark statistics:\")\n",
    "print(f\"Total benchmark mentions: {total_benchmark_mentions}\")\n",
    "print(f\"Papers that mention benchmarks: {papers_with_benchmarks} ({papers_with_benchmarks/total_papers*100:.1f}%)\")\n",
    "print(f\"Average benchmarks per paper (across all papers): {avg_benchmarks_per_paper:.2f}\")\n",
    "print(f\"Average benchmarks when present: {avg_benchmarks_when_present:.2f}\")\n",
    "\n",
    "# Create combined plots\n",
    "create_combined_plot(df_benchmarks, top_20_benchmarks, benchmark_counter,\n",
    "                    'Distribution of Benchmark Usage Over Time',\n",
    "                    'benchmark_distribution.pdf', 0.5,\n",
    "                    benchmark_original_names)\n",
    "\n",
    "create_combined_plot(df_models, top_20_models, model_counter,\n",
    "                    'Distribution of Model Usage Over Time',\n",
    "                    'model_distribution.pdf', 0.9,\n",
    "                    model_original_names)\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nTop 20 Benchmarks and their frequencies:\")\n",
    "for benchmark, count in benchmark_counter.most_common(20):\n",
    "    original_name = benchmark_original_names.get(benchmark, benchmark)\n",
    "    print(f\"{original_name}: {count}\")\n",
    "\n",
    "print(\"\\nTop 20 Models and their frequencies:\")\n",
    "for model, count in model_counter.most_common(20):\n",
    "    original_name = model_original_names.get(model, model)\n",
    "    print(f\"{original_name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac22032b-b55d-43b8-855a-c396b5b5e05f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
